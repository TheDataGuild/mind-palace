<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Techniques for combining fast local decoders with global decoders under circuit-level noise</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2022-09-28">28 Sep 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Christopher</forename><surname>Chamberland</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">AWS Center for Quantum Computing</orgName>
								<address>
									<postCode>91125</postCode>
									<settlement>Pasadena</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">IQIM</orgName>
								<orgName type="institution" key="instit2">California Institute of Technology</orgName>
								<address>
									<postCode>91125</postCode>
									<settlement>Pasadena</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Luis</forename><surname>Goncalves</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">AWS Center for Quantum Computing</orgName>
								<address>
									<postCode>91125</postCode>
									<settlement>Pasadena</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Prasahnt</forename><surname>Sivarajah</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">AWS Center for Quantum Computing</orgName>
								<address>
									<postCode>91125</postCode>
									<settlement>Pasadena</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Eric</forename><surname>Peterson</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">AWS Center for Quantum Computing</orgName>
								<address>
									<postCode>91125</postCode>
									<settlement>Pasadena</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Sebastian</forename><surname>Grimberg</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">AWS Center for Quantum Computing</orgName>
								<address>
									<postCode>91125</postCode>
									<settlement>Pasadena</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Techniques for combining fast local decoders with global decoders under circuit-level noise</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-09-28">28 Sep 2022</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2208.01178v2[quant-ph]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-10-09T23:23+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Although sparser syndromes results in faster implementations of global decoders such as MWPM and UF, we leave the problem of optimizing such implementations using distributed resources for future work.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Implementing algorithms on a fault-tolerant quantum computer will require fast decoding throughput and latency times to prevent an exponential increase in buffer times between the applications of gates. In this work we begin by quantifying these requirements. We then introduce the construction of local neural network (NN) decoders using three-dimensional convolutions. These local decoders are adapted to circuit-level noise and can be applied to surface code volumes of arbitrary size. Their application removes errors arising from a certain number of faults, which serves to substantially reduce the syndrome density. Remaining errors can then be corrected by a global decoder, such as Blossom or Union Find, with their implementation significantly accelerated due to the reduced syndrome density. However, in the circuit-level setting, the corrections applied by the local decoder introduce many vertical pairs of highlighted vertices. To obtain a low syndrome density in the presence of vertical pairs, we consider a strategy of performing a syndrome collapse which removes many vertical pairs and reduces the size of the decoding graph used by the global decoder. We also consider a strategy of performing a vertical cleanup, which consists of removing all local vertical pairs prior to implementing the global decoder. Lastly, we estimate the cost of implementing our local decoders on Field Programmable Gate Arrays (FPGAs).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Quantum computers have the potential to implement certain families of algorithms with significant speedups relative to classical computers <ref type="bibr" target="#b0">[1]</ref><ref type="bibr" target="#b1">[2]</ref><ref type="bibr" target="#b2">[3]</ref>. However, one of the main challenges in building a quantum computer is in mitigating the effects of noise, which can introduce errors during a computation corrupting the results. Since the successful implementation of quantum algorithms require qubits, gates and measurements to fail with very low probabilities, additional methods are required for detecting and correcting errors when they occur. Universal fault-tolerant quantum computers are one such strategy, where the low desired failure rates come at the cost of substantial extra qubit and gate overhead requirements <ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref><ref type="bibr" target="#b5">[6]</ref><ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b8">[9]</ref><ref type="bibr" target="#b9">[10]</ref><ref type="bibr" target="#b10">[11]</ref><ref type="bibr" target="#b11">[12]</ref><ref type="bibr" target="#b12">[13]</ref><ref type="bibr" target="#b13">[14]</ref><ref type="bibr" target="#b14">[15]</ref><ref type="bibr" target="#b15">[16]</ref>.</p><p>The idea behind stabilizer based error correction is to encode logical qubits using a set of physical data qubits. The qubits are encoded in a state which is a +1 eigenstate of all operators in a stabilizer group, which is an Abelian group of Pauli operators <ref type="bibr" target="#b16">[17]</ref>. Measuring operators in the stabilizer group, known as a syndrom measurement, provides information on the possible errors afflicting the data qubits. The results of the syndrome measurements are then fed to a classical decoding algorithm whose goal is to determine the most likely errors afflicting the data qubits. In recent decades, a lot of effort has been made towards improving the performance of error correcting codes and fault-tolerant quantum computing architectures in order to reduce the large overhead requirements arising from error correction. An equally important problem is in devising classical decoding algorithms which operate on the very fast time scales required to avoid exponential backlogs during the implementation of a quantum algorithm <ref type="bibr" target="#b17">[18]</ref>.</p><p>Several decoders have been proposed with the potential of meeting the speed requirements imposed by quantum algorithms. Cellular automata and renormalization group decoders are based on simple local update rules and have the potential of achieving fast runtimes when using distributed hardware resources <ref type="bibr" target="#b18">[19]</ref><ref type="bibr" target="#b19">[20]</ref><ref type="bibr" target="#b20">[21]</ref><ref type="bibr" target="#b21">[22]</ref><ref type="bibr" target="#b22">[23]</ref><ref type="bibr" target="#b23">[24]</ref><ref type="bibr" target="#b24">[25]</ref>. However, such decoders have yet to demonstrate the low logical failure rates imposed by algorithms in the circuit-level noise setting. Linear-time decoders such as Union Find (UF) <ref type="bibr" target="#b25">[26]</ref> and a hierarchical implementation of UF with local update rules <ref type="bibr" target="#b26">[27]</ref> have been proposed. Even with the favorable decoding complexity, further work is needed to show how fast such decoders can be implemented using distributed classical resources in the circuit-level noise regime at the high physical error rates observed for quantum hardware. Lastly, many NN decoders have been introduced, with varying goals <ref type="bibr" target="#b27">[28]</ref><ref type="bibr" target="#b28">[29]</ref><ref type="bibr" target="#b29">[30]</ref><ref type="bibr" target="#b30">[31]</ref><ref type="bibr" target="#b31">[32]</ref><ref type="bibr" target="#b32">[33]</ref><ref type="bibr" target="#b33">[34]</ref><ref type="bibr" target="#b34">[35]</ref><ref type="bibr" target="#b35">[36]</ref><ref type="bibr" target="#b36">[37]</ref><ref type="bibr" target="#b37">[38]</ref><ref type="bibr" target="#b38">[39]</ref><ref type="bibr" target="#b39">[40]</ref><ref type="bibr" target="#b40">[41]</ref><ref type="bibr" target="#b41">[42]</ref><ref type="bibr" target="#b42">[43]</ref><ref type="bibr" target="#b43">[44]</ref>. For NN decoders to be a viable candidate in universal fault-tolerant quantum computing, they must be fast, scalable, and exhibit competitive performance in the presence of circuit-level noise.</p><p>In this work, we introduce a scalable NN decoding algorithm adapted to work well with circuit-level noise. Our construction is based on fully three-dimensional convolutions and is adapted to work with the rotated surface code <ref type="bibr" target="#b44">[45]</ref>. Our NN decoder works as a local decoder which is applied to all regions of the spacetime volume. By local decoder, we mean that the decoder corrects errors arising from a constant number of faults, with longer error chains left to be corrected by a global decoder. The goal is to reduce the overall decoding time by having a fast implementation of our local decoder, which will remove a large number of errors afflicting the data qubits. If done correctly, removing such errors will reduce the syndrome density, resulting in a faster implementation of the global decoder 1 . We note that in the presence of circuit-level noise, the corrections applied by our local NN decoders can result in the creation of vertical pairs of highlighted syndrome vertices (also referred to as defects in the literature), which if not dealt with could result in an increase in the error syndrome density rather than a reduction. To deal with this problem, we consider two approaches. In the first approach, we introduce the notion of a syndrome collapse, which removes a large subset of vertical pairs while also reducing the number of error syndromes used as input to the global decoder. Our numerical results show that competitive logical error rates can be achieved when performing a syndrome collapse after the application of the local NN decoders, followed by minimum-weight-perfect-matching (MWPM) <ref type="bibr" target="#b45">[46]</ref> used as a global decoder. We achieve a threshold of approximately p th ‚âà 5 √ó 10 -3 , which is less than the threshold of p th ‚âà 7 √ó 10 -3 obtained by a pure MWPM decoder due to information loss when performing the syndrome collapse. However, we observe a significant reduction in the average number of highlighted vertices used by the global decoder. On the other hand, a syndrome collapse reduces the surface codes timelike distance and would thus not be performed during a lattice surgery protocol.</p><p>The second approach consists of directly removing all vertical pairs after the application of the local decoder, but prior to the implementation of the global decoder. When removing vertical pairs, we observe a threshold which is greater than 5 √ó 10 -3 when MWPM is used as a global decoder. We also observe a reduction in the error syndrome density by almost two orders of magnitude in some physical noise rate regimes. This outperforms the reduction achieved by the syndrome collapse strategy, although the size of the decoding graph remains unchanged. We conclude our work with a resource cost estimate of the implementation of our NN decoders on FPGA's, and discuss room for future improvements.</p><p>Our manuscript is structured as follows. In Section II we give a brief review of the rotated surface code and it's properties, and introduce some notation used throughout the manuscript. In Section III, we discuss how buffer times during the implementation of algorithms depend on decoding throughput and latency, and use such results to further motivate the need for fast decoders. Section IV is devoted to the description of our local NN decoder and numerical results. In Section IV A we show how NN's can be used as decoders for quantum error correcting codes in the presence of circuit-level noise, and provide the details of our NN architectures and training methodologies. We discuss how representing the data can significantly impact the performance of our NN's, with more details provided in Appendices A and B. In Section IV B we show how local decoders can introduce vertical pairs of highlighted vertices in the presence of circuit-level noise models, even when correcting all the data qubit errors resulting from such fault mechanisms. We then describe how we perform a syndrome collapse to remove vertical pairs and reduce the number of syndromes needed by the global decoder.</p><p>In Section IV C we provide an example correction from a local NN decoder, which illustrates the creation of vertical pairs of highlighted vertices. We then describe the vertical cleanup scheme for removing vertical pairs. We conclude Section IV by providing numerical results of our decoding protocols applied to the surface code in Section IV D. Lastly, in Section V, we discuss the resource costs of implementing our local decoders on classical hardware.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. BRIEF REVIEW OF THE SURFACE CODE</head><p>In this work we consider the surface code as the code used to correct errors during a quantum computation. An excellent introduction to the surface code is provided in Ref. <ref type="bibr" target="#b6">[7]</ref>. In this section, we briefly review the properties of the rotated surface code <ref type="bibr" target="#b44">[45]</ref> and focus on the main features pertaining to the implementation of our scalable NN decoder.</p><p>The surface code is a two-dimensional planar version of the toric code <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b47">48]</ref>. The code parameters of the surface code are [[d x d z , 1, min (d x , d z )]], where d x and d z are the distances of minimum-weight representatives of the logical X and Z operators of the code (which we refer to as the X and Z distance of the code). The logical X and Z operators of the code form vertical and horizontal string-like excitations. The surface code belongs to the family of Calderbank-Shor-Steane (CSS) codes <ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b49">50]</ref>, with the X and Z-type stabilizers in the bulk of the lattice corresponding to weight-four operators. There are additional weight-two operators along the boundary of the lattice. An example of a d x = d z = 5 surface code is shown in Fig. <ref type="figure" target="#fig_0">1</ref>. The weight-four X and Z-type stabilizers correspond to the red and blue plaquettes in the figure, with the weight-two stabilizers being represented by semicircles. We also define the error syndromes for CSS codes as follows:</p><formula xml:id="formula_0">Definition II.1 (Error syndrome). Let S X = g (X) 1 , g (X) 2 , ‚Ä¢ ‚Ä¢ ‚Ä¢ , g (X) r1 and S Z = g (Z) 1 , g (Z) 2 , ‚Ä¢ ‚Ä¢ ‚Ä¢ , g (Z) r2</formula><p>be the generating set of X and Z-type stabilizers of a CSS code C, and suppose the stabilizer measurements are repeated d m times. We define s X (d m ) to be a bit string (e</p><formula xml:id="formula_1">(1)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>X e</head><p>(2)</p><formula xml:id="formula_2">X ‚Ä¢ ‚Ä¢ ‚Ä¢ e (dm) X</formula><p>) where e</p><formula xml:id="formula_3">(k)</formula><p>X is a bit string of length r 2 with e (k)</p><formula xml:id="formula_4">X (j) = 1 iff g (Z) j</formula><p>is measured non-trivially in the k'th syndrome measurement round, and is zero otherwise. Similarly, we define s Z (d m ) to be a bit string (e</p><p>Z e</p><p>(2)</p><formula xml:id="formula_6">Z ‚Ä¢ ‚Ä¢ ‚Ä¢ e (dm) Z ) where e (k) Z is a bit string of length r 1 with e (k) Z (j) = 1 iff g (X) j</formula><p>is measured non-trivially in the k'th syndrome measurement round, and is zero otherwise. Note that the s X (d m ) and s Z (d m ) syndromes in Definition II.1 can have non-zero bits due to both the presence of data qubit errors as well as measurement errors. We will also be particularly interested in syndrome differences between consecutive rounds, which are defined as follows:</p><formula xml:id="formula_7">Definition II.2 (Syndrome differences). Given the syn- dromes s X (d m ) = (e<label>(1)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>X e</head><p>(2)</p><formula xml:id="formula_8">X ‚Ä¢ ‚Ä¢ ‚Ä¢ e (dm) X ) and s Z (d m ) = (e<label>(1)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Z e</head><p>(2)</p><formula xml:id="formula_9">Z ‚Ä¢ ‚Ä¢ ‚Ä¢ e (dm) Z ) for the code C defined in Definition II.1, we set s diff X (d m ) = (e (1) X ·∫Ω(2) X ‚Ä¢ ‚Ä¢ ‚Ä¢ ·∫Ω(dm) X ), where ·∫Ω(k) X is a bit string of length r 2 and ·∫Ω(k) X (j) = 1 iff the measurement outcome of g (Z) j in round k is different than the mea- surement outcome of g (Z) j in round k -1 (for k &gt; 1). Similarly, we define s diff Z (d m ) = (e (1) Z ·∫Ω(2) Z ‚Ä¢ ‚Ä¢ ‚Ä¢ ·∫Ω(dm) Z ), where ·∫Ω(k) Z is a bit string of length r 1 and ·∫Ω(k) Z (j) = 1 iff the measurement outcome of g (X) j in round k is different than the measurement outcome of g (X) j in round k -1 (for k &gt; 1).</formula><p>The standard decoding protocol used to correct errors with the surface code is by performing MWPM using Edmonds Blossom algorithm <ref type="bibr" target="#b45">[46]</ref>. In particular, a graph G is formed, with edges corresponding to the data qubits (yellow vertices in Fig. <ref type="figure" target="#fig_0">1</ref>) and vertices associated with the stabilizer measurement outcomes (encoded in the grey vertices of Fig. <ref type="figure" target="#fig_0">1</ref>). In order to distinguish measurement errors from data qubit errors, the error syndrome (measurement of all stabilizers) is repeated r times (with r being large enough to ensure fault-tolerance, see for instance the timelike error analysis in Ref. <ref type="bibr" target="#b15">[16]</ref>). Let m (k) (g i ) = 1 if the stabilizer g i in round k is measured non-trivially and zero otherwise. Prior to implementing MWPM, a vertex v (k) (g i ) in G associated with a stabilizer g i in the k'th syndrome measurement round is highlighted iff m (k) (g i ) = m (k-1) (g i ), i.e. the syndrome measurement outcome of g i changes between rounds k -1 and k. More generally, for any fault location l k in the circuits used to measure the stabilizers of the surface code (for instance CNOT gates, idling locations, statepreparation and measurements), we consider all possible Pauli errors P l k (j) at location l k (with k indexing through all possible Pauli's) and propagate such Pauli's. If propagating the Pauli P l k (j) results in two highlighted vertices v (k1) (g j1 ) and v (k2) (g j2 ), an edge e incident to v (k1) (g j1 ) and v (k2) (g j2 ) is added to the matching graph G<ref type="foot" target="#foot_0">2</ref> . For a distance d x = d z = d surface code with d rounds of syndrome measurements, the decoding complexity of MWPM is O(n 3 ) where n ‚àù d 2 and corresponds to the number of highlighted vertices in G (see Ref. <ref type="bibr" target="#b45">[46]</ref> and Section V C for more details). The UF decoder, another graph based decoder, has decoding complexity of O(Œ±n) where Œ± is the inverse of Ackermann's function. Remarkably, UF is able to achieve near linear time decoding while maintaining good performance relative to MWPM <ref type="bibr" target="#b50">[51]</ref>.</p><p>Although MWPM and UF have polynomial decoding time complexities, decoders will need to operator on ¬µs time scales for many practical quantum hardware architectures (see Section III). Achieving such fast decoding times using MWPM and UF appears to be quite challenging <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b51">[52]</ref><ref type="bibr" target="#b52">[53]</ref><ref type="bibr" target="#b53">[54]</ref>. To this end, in Section IV we use scalable NN's as local decoders that have an effective distance d and which can thus correct errors E of weight wt(E) ‚â§ (d -1)/2. MWPM and UF can then be used as a global decoder to correct any remaining errors which were not corrected by the local decoder. The effect of the local decoder is to reduce the value of n by removing many of the errors afflicting the data qubits. NN's have already been used as local decoders in the setting of code capacity noise (where only data qubits can fail, and error syndromes only have to be measured once) and phenomenological noise (where measurements can fail in adition to data qubits) <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b43">44]</ref>. However, the presence of circuit-level noise introduces significant new challenges which require new methods to cope with the more complex fault patterns.</p><p>Throughout the remainder of this manuscript, we con- sider the following circuit-level depolarizing noise for our numerical analyses:</p><p>1. Each single-qubit gate location is followed by a Pauli X, Y or Z error, each with probability p 3 . 2. With probability p, each two-qubit gate is followed by a two-qubit Pauli error drawn uniformly and independently from {I, X, Y, Z} ‚äó2 \{I ‚äó I}.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">With probability 2p</head><p>3 , the preparation of the |0 state is replaced by |1 = X|0 . Similarly, with probability 2p  3 , the preparation of the |+ state is replaced by |-= Z|+ .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">With probability 2p</head><p>3 , any single qubit measurement has its outcome flipped.</p><p>5. Lastly, with probability p, each idle gate location is followed by a Pauli error drawn uniformly and independently from {X, Y, Z}.</p><p>This noise model is similar to the one used in Refs. <ref type="bibr" target="#b54">[55,</ref><ref type="bibr" target="#b55">56]</ref>. However, in this work, we treat each idle location during measurement and reset times of the ancillas as a single idle location failing with probability p (instead of two idling locations each failing with probability p).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. THE EFFECTS OF THROUGHPUT AND LATENCY ON ALGORITHM RUN-TIMES</head><p>In this section we discuss how latency and decoding times affect the run-time of algorithms. In what follows, we refer to inbound latency as the time it takes for the stabilizer measurement outcomes of an error correcting code to be known to the classical computer which implements the decoding task. By classical computer, we mean the classical device which stores and processes syndrome information arising from stabilizer measurements of an error correcting code in order to compute a correction. We specify "inbound" to distinguish this quantity from the "outbound" latency, or delay between the arrival of an error syndrome at the decoder and its resolution. We also refer to throughput as the time it takes for the classical computer to compute a correction based on the syndrome measurement outcome.</p><p>We denote the Clifford group as C which is generated by C = H, S, CNOT , with the matrix representation for the Hadamard and phase gates in the computational basis</p><formula xml:id="formula_10">= (a) = (b)</formula><p>FIG. <ref type="figure">3</ref>. Two equivalent circuits for implementing a T gate. In (a), we show the standard circuit for implementing a T gate using the magic state |T = 1 ‚àö 2 (|0 + e iœÄ/4 |1 ) as a resource state. In (b), we provide an equivalent circuit where the logical CNOT gate is replaced by a Z ‚äó Z Pauli measurement, which can be implemented via lattice surgery, as discussed for instance in Ref. <ref type="bibr" target="#b15">[16]</ref>.</p><formula xml:id="formula_11">expressed as H = 1 ‚àö 2 1 1 1 -1 and S = diag(1, i). The CNOT gate acts as CNOT|a |b = |a |a ‚äï b .</formula><p>Consider the sequence of non-parallel T = diag(1, e iœÄ/4 ) gates shown in Fig. <ref type="figure">2</ref>. Note that T gates are non-Clifford gates, and the set generated by H, S, CNOT, T forms the basis of a universal gate set. We also consider a framework where we keep track of a Pauli frame <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b56">57,</ref><ref type="bibr" target="#b57">58]</ref> throughout the execution of the quantum algorithm. The Pauli frame allows one to keep track of all conditional Pauli's and Pauli corrections arising from error correction (EC) in classical software, thus avoiding the direct implementation in hardware of such gates, which could add additional noise to the device. Since T P T ‚Ä† ‚àà C, when propagating the Pauli frame through a T gate, a Clifford correction may be required in order to restore the Pauli frame. Consequently, buffers are added between the sequence of T gates where repeated rounds of EC are performed until the Pauli frame P j immediately before applying the j'th T gate is known. The buffer immediately after the j'th T gate is labeled as b j . We now show how buffer times increase with circuit depth as a function of inbound latency and throughput.</p><p>We start with a few definitions. Let T bj denote the total waiting time during buffer b j , and T s be the total time it takes to perform one round of stabilizer measurements for a given quantum hardware architecture. Let T l be the time for the stabilizer measurements of one round of EC to be known to the classical computer. An example circuit using the |T = 1 ‚àö 2 (|0 + e iœÄ/4 |1 ) magic state is provided in Fig. <ref type="figure">3</ref>. Lastly, we define T  FIG. <ref type="figure">4</ref>. Plots showing the buffer times T b j as a function of the buffer number bj. We set r1 + r2 = 33 and consider using the surface code for performing EC. The surface code requires four time steps to implement all CNOT gates used to measure the codes stabilizers, and we assume each CNOT gate takes 100ns. We also assume a measurement plus reset time of the ancillas to be 1¬µs, resulting in a total time Ts = 1.4¬µs. In (a), we fix the inbound latency to be T l = 20¬µs, and assume a decoding time which scales as T The j'th buffer time T bj will depend on the particular implementation of the T gate. For many quantum hardware architectures, arbitrary logical CNOT gates must be implemented by lattice surgery <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b58">59,</ref><ref type="bibr" target="#b59">60]</ref>, which would be equivalent to using the circuit in Fig. <ref type="figure">3b</ref>. In such a case, T bj will depend not only on the processing of EC rounds during buffer b j-1 , but also on the processing of the multiple rounds of EC for the Z ‚äó Z measurement via lattice surgery since the measurement outcome is needed in order to restore the Pauli frame. We note however that given access to an extra ancilla qubit, the conditional Clifford in Fig. <ref type="figure">3b</ref> can be replaced with a conditional Pauli (see Fig. <ref type="figure" target="#fig_9">17 (b</ref>) in Ref. <ref type="bibr" target="#b11">[12]</ref>, and for a generalization to CCZ gates, Fig. <ref type="figure">4</ref> in Ref. <ref type="bibr" target="#b60">[61]</ref>). For simplicity, we will use the circuit in Fig. <ref type="figure">3b</ref> as using the circuit in Ref. <ref type="bibr" target="#b11">[12]</ref> would simply change the number of syndrome measurement rounds used in our analysis. Now, consider the wait time T b1 of the first buffer. Since the Pauli frame P 1 and the measurement outcome of the Z ‚äó Z measurement must be known to restore the Pauli frame, we have that</p><formula xml:id="formula_12">T b1 = T (r1+r2) DEC + T l ,<label>(1)</label></formula><p>where we assume r 1 rounds of EC are performed during the waiting time of buffer b 0 and r 2 rounds of EC are needed for the Z ‚äó Z measurement. We also assume that the syndrome measurement outcomes of each EC round have an inbound latency T l , and that the decoder used by the classical computer can begin processing the syndromes after receiving the outcome of the last round.</p><p>In Appendix D we discuss how buffer times can be reduced for decoders implemented using sliding windows. However in this section, we consider the case where the decoder takes as input all syndrome measurement rounds until the last round when the data qubits are measured in some basis. Now let n</p><p>QEC denote the total number of QEC rounds needed during the buffer b j . For b 1 , we have that</p><formula xml:id="formula_14">n (b1) QEC = T b1 /T s ,<label>(2)</label></formula><p>since each syndrome measurement round takes time T s .</p><p>Using Eq. ( <ref type="formula" target="#formula_14">2</ref>), the buffer time T b2 is then</p><formula xml:id="formula_15">T b2 = T (n (b 1 ) QEC ) DEC + T l .</formula><p>Applying the above arguments recursively, the j'th buffer is then </p><formula xml:id="formula_16">T bj = T (n (b j-1 ) QEC ) DEC + T l ,<label>(3) with n</label></formula><formula xml:id="formula_17">T bj = c j r T j-1 s + T l T 1-j s (c j -T j s ) c -T s .<label>(4)</label></formula><p>Plots of Eq. ( <ref type="formula" target="#formula_17">4</ref>) for different values of c and inbound latency times T l are shown in Fig. <ref type="figure">4</ref>. We assume that the surface code is used to perform each round of EC, where the CNOT gates used to measure the stabilizers take four time steps. Each CNOT is assumed to take 100ns, and the measurement and reset time of the ancillas take 1¬µs, as is the case for instance in Ref. <ref type="bibr" target="#b61">[62]</ref>. Therefore we set T s = 1.4¬µs. We also assume that the number of syndrome measurement rounds during the buffer b 0 and first lattice surgery measurement for Z ‚äó Z is r 1 + r 2 = 33, which could be the case for the implementation of medium to large size algorithms with a d ‚âà 20 surface code.</p><p>As can be seen in Fig. <ref type="figure">4a</ref>, where the inbound latency term T l = 20¬µs, if c T s , then the buffer wait times grow in a manageable way. However for larger values of c, there is a large exponential blow-up in the buffer wait times. This can also be seen from the first term in Eq. ( <ref type="formula" target="#formula_17">4</ref>), which grows linearly if c ‚â§ T s . In Fig. <ref type="figure">4b</ref>, we consider how changing the inbound latency T l affects the buffer wait times when keeping c fixed (which we set to c = 1¬µs). As can be seen, increasing inbound latency does not result in an exponential blow-up in buffer wait times. This can also be seen from the second term in Eq. ( <ref type="formula" target="#formula_17">4</ref>) which only depends linearly on T l . As such, we conclude buffer wait times are much more sensitive to decoding throughput times, and it will thus be very important to have fast EC decoders in order to implement quantum algorithms.</p><p>We conclude this section by remarking that increasing buffer times can also lead to an increase in the code distances d x and d z to ensure that logical failure rates remain below the target set by the quantum algorithm. In other words, if the code distance is fixed, buffer times cannot be arbitrarily large. For instance, for a code with full effective code distance (and let d x = d z = d as is the case for a depolarizing noise model), the logical X and Z error rates for d m syndrome measurement rounds scale as</p><formula xml:id="formula_18">p L (p) = udd m (bp) (d+1)/2 ,<label>(5)</label></formula><p>for some constants u and b (see for instance Ref. <ref type="bibr" target="#b15">[16]</ref>). We must also have p L (p) &lt; Œ¥ where Œ¥ is the maximum failure rate allowed for a particular algorithm. Hence for a fixed d, we must have that d m &lt; Œ¥/(ud(bp) (d+1)/2 ). The parameters u and b depend on the details of the noise model and decoding algorithm used, as discussed in Section IV D. The reader may be concerned that a large value of d m imposed by long buffer wait times may require a large increase in the code distance. In Appendix E we show that the code distance d only grows logarithmically with d m .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. USING NN'S AS LOCAL DECODERS FOR CIRCUIT-LEVEL NOISE</head><p>In Section III we motivated the need for fast decoders. In this section, we construct a hierarchical decoding strategy for correcting errors afflicting data qubits encoded in the surface code. Our hierarchical decoder consists of a local decoder which can correct errors of a certain size, and a global decoder which corrects any remaining errors after implementating the local decoder. In this manuscript we use MWPM for the global decoder, though our scheme can easily be adapted to work with other global decoders such as Union Find. We use NN's to train our local decoder arising from the circuit-level noise model described in Section II. Importantly, the NN decoder is scalable and can be applied to arbitrary sized volumes (d x , d z , d m ) where d x and d z are the X and Z distances of the surface code, and d m is the number of syndrome measurement rounds.</p><p>Our local decoder will have an effective distance d eff ‚â§ max (d x , d z ) allowing it to remove errors arising from at most (d eff -1)/2 faults. By removing such errors, the goal is to reduce the syndrome density, i.e. the number of highlighted vertices in the matching graph G used to implement MWPM, thus resulting in a much faster execution of MWPM. We note that hierarchical decoding strategies have previously been considered in the literature <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b43">44]</ref>. In Ref. <ref type="bibr" target="#b26">[27]</ref>, a subset of highlighted vertices in the matching graph G (which we refer to as syndrome density) are removed based on a set of local rules. However, the weight of errors which can be removed by the local rules is limited, and the scheme (analyzed for code capacity noise) requires low physical error rates to see a large reduction in decoding runtimes. The schemes in Refs. <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b43">44]</ref> used NN's to train local decoders. In Ref. <ref type="bibr" target="#b43">[44]</ref>, a two-dimensional fully convolutional NN was used to correct errors arising from code capacity noise. However the scheme does not generalize to phenomenological or circuit-level noise, where repeated rounds of syndrome measurements must be performed. In Ref. <ref type="bibr" target="#b42">[43]</ref>, fully connected layers were used to train a network based on patches of constant size, and the scheme was adapted to also work with phenomonological noise. However, as will be shown in Section IV B, the presence of circuitlevel noise introduces fault patterns which are distinct from code capacity and phenomenological noise. In particular, we find that for a certain subset of failures, the syndrome density is not reduced even if the local decoder removes the errors afflicting the data qubits (vertical pairs of highlighted vertices arise after the correction performed by the NN decoder). In fact, the use of NN's as local decoders can increase the syndrome density if no other operations are performed prior to implementing the global decoder. As such, in Section IV B we introduce the notion of syndrome collapse which not only reduces the syndrome density but also reduces the size of the matching graph G, leading to a much faster implementation of MWPM. We also introduce in Section IV C the notion of a vertical cleanup which directly removes pairs of highlighted vertices after the application of the local NN decoder, without reducing the size of the matching graph. We also point out that larger NN's are required to correct errors arising from the more complex fault-patterns of circuit-level noise than what was previously considered in the literature. In particular, in Section IV A we describe how three-dimensional fully convolutional NN's can be used to train our local decoder.</p><p>Regarding the implementation of our three-dimensional convolutional NN's, we introduce new encoding strategies for representing the data that not only allows the NN to adapt to different boundaries of a surface code lattice, but also significantly enhances its abilities to correct errors in the bulk.</p><p>Lastly, in Section IV D we provide a numerical analysis of our decoding strategy applied to various surface code volumes of size (d x , d z , d m ), showing both the logical error rates and syndrome density reductions after the implementation of our local decoder.</p><p>A. Using NN's to train local decoders.</p><p>Decoding can be considered a pattern recognition task: for each physical data qubit q j used in the encoding of the surface code, given the syndrome measurements within some local volume (d x , d z , d m ) of the lattice, a classifier can predict whether or not there is an error afflicting q j .</p><p>In this work, we design a NN classifier that takes as input a local volume of size (d x , d z , d m ), and train it to correct data-qubit errors arising from at most (d -1)/2 faults, where d = min (d x , d z ). To ensure scalability, our NN classifier must be designed in such a way that it corrects errors arising from at most (d eff -1)/2 faults even when applied to larger surface code volumes (d x , d z , d m ), where d eff ‚â§ d .</p><p>There are many choices for our network architecture. The simplest is a multi-layer perceptron (MLP) with an input layer, hidden layer, and output layer, each of which is a "fully connected" layer where all inputs connect to each neuron in the layer. In this type of network, the (d x , d z , d m ) local volume serves as inputs to a set of N neurons in the input layer. The hidden layer takes those N neurons as inputs for a set of H neurons, and finally the H hidden layer neuron outputs are inputs to the final layer neurons that produce the prediction. We implement a network with two outputs, the occurrence of an X error, and the occurrence of a Z error (with Y errors occurring if both X and Z errors are present).</p><p>For an efficient computation, we transform (and subsequently enhance) the MLP to be a "fully-convolutional" network, where each layer consists of a set of convolution filters. Convolutions efficiently implement a slidingwindow computation<ref type="foot" target="#foot_1">3</ref> to produce an output at each location of an input of arbitrary size. For the case of a network with a (d x , d z , d m ) local input volume, we use a 3-dimensional convolution of the same size, and so the first layer is a set of N (d x , d z , d m ) convolutional filters. This layer, when applied to a local patch of size (d x , d z , d m ), produces N outputs. The hidden layer, accepting these N inputs for H outputs, can be viewed as a set of H 1 √ó 1 √ó 1 convolutional filters. Likewise, the final output layer accepts these H inputs to produce 2 outputs, and can be represented as two 1 √ó 1 √ó 1 conv3d filters.</p><p>The fully-convolutional network produces a prediction for the data qubit at the center of the local volume it analyzes, as it sweeps through the entire lattice. To allow the network to make predictions right up to the boundary of the lattice, the conv layers are chosen to produce a 'same' output, whereby the input is automatically zeropadded beyond the boundary of the lattice. For example, for a convolution of size 9 to produce an output right at the boundary, the boundary is padded with an additional 4 values. Fig. <ref type="figure" target="#fig_13">5</ref> illustrates the NN applied throughout the lattice volume, including computing a prediction right at the border of the lattice, in which case some of it's input field lies outside of the lattice volume and receives zero padded values.</p><p>To improve the representational power of the network, we can replace the first layer of convolutional filters with multiple layers, taking care to preserve the overall receptive field of the network. For example, if the first layer had filters of size (9,9,9), 4 layers with filters of size <ref type="bibr" target="#b2">(3,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b2">3)</ref> will also have an effective filter size of (9,9,9), since each additional layer increases the effective filter width by 2 from the first layer's width of 3. If each layer were linear, the resulting N outputs in the fourth layer would be mathematically equivalent to a single 9 √ó 9 √ó 9 layer with N outputs. However, since each layer is non-linear, with a nonlinear activation function (ReLu in our case), the two networks are no longer equivalent, and the network with 4 layers of <ref type="bibr" target="#b2">(3,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b2">3)</ref>    . The network has a total of 352, 210 parameters. We also use skip connections which becomes more relevant as the number of layers in the network becomes large to avoid exploding/vanishing gradients <ref type="bibr" target="#b62">[63,</ref><ref type="bibr" target="#b63">64]</ref>. For both networks, we perform batch normalization after each layer. All layers use the ReLu activation function except for the last layer, where we use a sigmoid activation function, to generate predictions for physical qubit errors throughout the lattice. We also use the binary cross-entropy loss function to train our networks. In (c), we provide the details of the implementation of the skip connections.</p><formula xml:id="formula_19">ùëì ! = (3,3,3) ùëì ! = (3,3,3) ùëì ! = (3,3,3) ùëì ! = (3,3,3) ùëì ! = (1,1,1) ùëì ! = (1,1,1)<label>(a</label></formula><formula xml:id="formula_20">ùëì ! = (3,3,3) ùëì ! = (3,3,3) ùëì ! = (3,3,3) ùëì ! = (3,3,3) ùëì ! = (1,1,1) ùëì ! = (1,1,1) ùëì ! = (1,1,1) ùëì ! = (1,1,1) ùëì ! = (1,1,1) ùëì ! = (1,1,1) ùëì ! = (1,1,1)<label>(b</label></formula><p>For clarity, we also illustrate the batch normalization step and the implementation of the ReLu activation function.</p><p>learning nonlinear combinations of features-of-features-offeatures. Similarly, we can expand the hidden layer with (1,1,1) filters to become multiple layers of (1,1,1) filters to increase the network's learning capacity.</p><p>In this work we consider two network architectures illustrated in Fig. <ref type="figure" target="#fig_5">6</ref>. The network in Fig. <ref type="figure" target="#fig_5">6a</ref> has 6 layers, with the first 4 layers having filters of size <ref type="bibr" target="#b2">(3,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b2">3)</ref>. The remaining 2 layers have filters of size (1, 1, 1). In Fig. <ref type="figure" target="#fig_5">6b</ref>, the network has 11 layers, with the first 4 layers having filters of size <ref type="bibr" target="#b2">(3,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b2">3)</ref> and the remaining 7 layers have filters of size (1, 1, 1). The networks in Figs. <ref type="figure" target="#fig_5">6a</ref> and<ref type="figure" target="#fig_5">6b</ref> have a total of 221, 600 and 352, 210 parameters, respectively, with the goal that such networks can learn the complex fault patterns arising from circuit-level noise. Another goal is for the networks to correct errors on timescales similar to those discussed in Section III using appropriate hardware. More details on the implementation of these networks on FPGA's are discussed in Section V.</p><p>To obtain the data used to train the fully convolutional NN's, we perform N train Monte Carlo simulations using the circuit-level noise model described in Section II, with the surface code circuit being used to compute the error syndrome. The training data is then stored using the following format. The input to the network, which we label as trainX, is a tensor of shape (N train , d x , d z , d m , 5) for a surface code with X and Z distances d x and d z , with d m syndrome measurement rounds. Following Definition II.2, the first two inputs to trainX contain the syndrome differences s diff X (d m ) and s diff Z (d m ) obtained for d m -1 rounds of noisy syndrome measurements, followed by one round of perfect error correction. Tracking changes in syndrome measurement outcomes between consecutive rounds ensures that the average syndrome density remains constant across different syndrome measurement rounds. The next two inputs of trainX contain spatial information used to enable the network to associate syndrome measurement outcomes with data qubits in both the bulk and along boundaries that can influence the observed outcome. The data is represented as d x by d z binary matrices labelled enc(X) and enc(Z), where 1 values are inserted following a particular mapping between the position of the ancillas (grey vertices in Fig. <ref type="figure" target="#fig_0">1</ref>) and data qubits (yellow vertices in Fig. <ref type="figure" target="#fig_0">1</ref>) which interact with the ancillas. The details of our mapping is described in Appendix A. We note that the matrices enc(X) and enc(Z) are provided for each syndrome measurement round, and are identical in each round unless the lattice changes shape between consecutive syndrome measurement rounds, as would be the case during a lattice surgery protocol <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b58">59,</ref><ref type="bibr" target="#b59">60]</ref>. Further, the syndrome differences stored in the first two inputs of trainX also follow the same mapping used in enc(X) and enc(Z) between stabilizers and entries in the matrix representations, except that a 1 is only inserted for non-zero values of s diff X (d m ) and s diff Z (d m ) (more details are provided in Appendix A). Finally, the fifth input of trainX contains the temporal boundaries, which specify the first and last syndrome measurement round. Since the last syndrome measurement round is a round of perfect error correction 4 , the syndrome measurement outcome will always be compatible with the errors afflicting the data qubits arising from the second last round. As such, since the last syndrome measurement round behaves differently than the other rounds, it is important to specify its location (as well as the location of the first round) in trainX so that the trained network can generalize to volumes with arbitrary d m values. More details for how the data is represented in trainX and the mappings discussed in this paragraph are provided in Appendix A.</p><p>Next, the output targets that the NN will attempt to predict (i.e.</p><p>the locations of X and Z data qubit errors) are stored in a tensor trainY of shape (N train , d x , d z , d m , 2). In particular, trainY contains the X and Z data errors afflicting the data qubits for syndrome measurement rounds 1 to d m . In order for the data stored in trainY to be compatible with trainX, we 4 A round of perfect error correction is a syndrome measurement round where no new errors are introduced, and arises when the data qubits are measured directly in some basis at the end of the computation. A measurement error which occurs when the data qubits are measured directly is equivalent to an error on such data qubits in the prior round. See for instance Appendix I in Ref. <ref type="bibr" target="#b14">[15]</ref>.</p><p>only track changes in data qubit errors between consecutive syndrome measurement rounds, since trainX tracks changes in syndrome measurement outcomes between consecutive rounds. Tracking changes in data qubit errors also ensures that the average error densities are independent of the number of syndrome measurement rounds.</p><p>Otherwise, one would need to train the network over a very large number of syndrome measurement rounds in order for the networks to generalize well to arbitratry values of d m . An illustration showing the increase in the average data qubit error densities with the number of syndrome measurement rounds is shown in Fig. <ref type="figure" target="#fig_6">7</ref>.</p><p>When performing the Monte Carlo simulations to collect the training data, there are many cases where two errors E 1 and E 2 can have the same syndrome (s(E 1 ) = s(E 2 )) with E 1 E 2 = g where g is in the stabilizer group of the surface code. We say that such errors are homologically equivalent. In training the NN's, we found that choosing a particular convention for representing homologically equivalent errors in trainY leads to significant performance improvements, as was also remarked in Ref. <ref type="bibr" target="#b43">[44]</ref>. A detailed description for how we represent homologically equivalent errors in trainY is provided in Appendix B.</p><p>We conclude this section by remarking that the performance of the networks not only depend on the network architecture and how data is represented in trainX and trainY, but also on the depolarizing error rate p used to generate the training data, and the size of the input volume (d x , d z , d m ). For instance, since the local receptive field of the networks in Figs. 6a and 6b is 9x9x9, we used input volumes of size <ref type="bibr" target="#b12">(13,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b17">18)</ref> to allow the network to see spatial and temporal data located purely in the bulk of the volume (i.e. without being influenced by boundary effects). We also trained our networks at error rates p = 0.005 and p = 0.001, and found that training networks at higher physical error rates did not always lead to superior performance relative to networks trained at low physical error rates. More details are provided in Section IV D.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Performing a syndrome collapse by sheets.</head><p>Consider a CNOT failure during a Z-type stabilizer measurement resulting in an X ‚äó I error in the j'th syndrome measurement round, as shown in Fig. <ref type="figure" target="#fig_7">8a</ref>. The failure results in an X error on a data qubit. However, given the ordering of the CNOT gates, only a single Ztype stabilizer detects the error in round j, with two stabilizers detecting the error in round j + 1. We refer to such failure mechanisms as space-time correlated errors. In Fig. <ref type="figure" target="#fig_7">8b</ref> we illustrate the resulting highlighted vertices in a subset of the matching graph G which is used to implement MWPM. As explained in Section II, a vertex in G associated with the stabilizer g k is highlighted in round j if the measurement outcome of g k changes from rounds j -1 to j. Now, suppose a local decoder correctly identifies the observed fault pattern, and removes the X error on the afflicted data qubit. Fig. <ref type="figure" target="#fig_7">8c</ref> shows how G transforms after applying the correction. Importantly, even though the error is removed, a vertical pair of highlighted vertices is created in G. We also note that the creation of vertical pairs arising from a correction performed by the local decoder due to a two-qubit gate failure is intrinsic to circuit-level noise and would not be observed for code capacity or phenomenological noise models. In fact, we observe numerically that the average number of highlighted vertices in G after the corrections applied by the local decoder will increase rather than decrease. However, as the example of Fig. <ref type="figure" target="#fig_7">8</ref> illustrates, many of the highlighted vertices in G will be due to the creation of vertical pairs induced by the corrections arising from the local decoder (see also Fig. <ref type="figure" target="#fig_9">10</ref> in Section IV C). One way to reduce the number of vertical pairs after the correction is applied by the local decoder is to perform what we call a syndrome collapse by sheets. More specifically, consider the syndrome difference</p><formula xml:id="formula_21">s diff X (d m ) = (e (1) X ·∫Ω(2) X ‚Ä¢ ‚Ä¢ ‚Ä¢ ·∫Ω(dm) X</formula><p>) as defined in Definition II.2 and let us assume for simplicity that d m = Œ≥d m for some integer Œ≥. We can partition s diff X (d m ) as</p><formula xml:id="formula_22">s diff X (d m ) = (e (1) X ·∫Ω(2) X ‚Ä¢ ‚Ä¢ ‚Ä¢ ·∫Ω(d m ) X |·∫Ω (d m +1) X ‚Ä¢ ‚Ä¢ ‚Ä¢ ·∫Ω(2d m ) X | ‚Ä¢ ‚Ä¢ ‚Ä¢ | ·∫Ω(dm-d m +1) X ‚Ä¢ ‚Ä¢ ‚Ä¢ ·∫Ω(dm) X ).<label>(6)</label></formula><p>A syndrome collapse by sheets of size d m transforms s diff X (d m ) as</p><formula xml:id="formula_23">s diff X (d m ) = (e<label>(1)</label></formula><p>X e</p><p>(2)</p><formula xml:id="formula_24">X ‚Ä¢ ‚Ä¢ ‚Ä¢ e (Œ≥) X ),<label>(7)</label></formula><p>where e (j)</p><formula xml:id="formula_25">X = d m i=1 ·∫Ω((j-1)d m +i) X ,<label>(8)</label></formula><p>with the sum being performed modulo 2 (if j = 1, the first term in Eq. ( <ref type="formula" target="#formula_25">8</ref>) is e The above steps can also be performed analogously for syndromes corresponding to Z errors.</p><p>Performing a syndrome collapse by sheets reduces the size of the original matching graph G since G contained d m sheets prior to performing the collapse. We label G sc as the graph resulting from performing the syndrome collapse on the original graph G. An illustration of how the syndrome collapse removes vertical pairs is shown in Fig. <ref type="figure" target="#fig_8">9a</ref>. Note that without the presence of a local decoder, one would not perform a syndrome collapse using a MWPM decoder since such an operation would remove the decoders ability to correct errors which are temporally separated. An example is shown in Fig. <ref type="figure" target="#fig_8">9b</ref>. However by performing a syndrome collapse on a surface code of distance d after the application of the local decoder with d m = O(d eff ) where d eff is the effective distance of the local decoder (which depends on the local receptive field and size of the volume the network was trained on), we expect such an operation to result in a global effective distance which is equal or close to d. The reason is that errors contained within each sheet arising from less than or equal to (d eff -1)/2 faults should be removed by the local decoder. We say "should" because local NN decoders are not necessarily guaranteed to correct any error arising from (d eff -1)/2 faults. Since NN decoders offer no fault-tolerance guarantees, we cannot provide a proof giving the effective distance of the surface code decoded using the local NN, followed by a syndrome collapse and application of a global decoder. However, we observed numerically that using larger networks (i.e. a network with more layers and filters per layers) resulted in increased slopes of the logical error rate curves. In Section IV D we present numerical results showing the effective distances of various surface code lattices when performing a syndrome collapse after the application of local decoders implemented by NN's.</p><p>We now give an important remark regarding performing a syndrome collapse during a parity measurement implemented via lattice surgery. As discussed in detail in Ref. <ref type="bibr" target="#b15">[16]</ref>, when performing a parity measurement via lattice surgery, there is a third code distance related to timelike failures, where the wrong parity measurement would be obtained. The timelike distance is given by the number of syndrome measurement rounds which are performed when the surface code patches are merged. If a syndrome collapse were to be performed in the region of the merged surface code patch (see for instance Fig. <ref type="figure" target="#fig_6">7</ref> in Ref. <ref type="bibr" target="#b15">[16]</ref>), the timelike distance would be reduced and would result in timelike failures which would be too large. As such, a syndrome collapse should not be implemented when performing a parity measurement via lattice surgery unless additional syndrome measurement rounds are performed on the merged surface code patches to compensate for the loss in timelike distance. However, the timelike distance can still potentially be made small using a temporal encoding of lattice surgery protocol (TELS) as described in Ref. <ref type="bibr" target="#b15">[16]</ref>. Alternatively, the vertical cleanup protocol described below in Section IV C (which can also significantly reduce the syndrome density) could be used (see also Appendix F regarding the required number of syndrome measurement rounds to maintain the timelike distance).</p><p>Lastly, we conclude by remarking that a NN architecture that performs a correction by identifying edges in the matching and flipping the vertices incident to such edges could potentially avoid creating vertical pairs after performing its corrections. In such settings, a syndrome collapse or a vertical cleanup as described in Section IV C may not be required.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Performing a vertical cleanup</head><p>In Fig. <ref type="figure" target="#fig_9">10</ref> we show an example of the application of the 11-layer NN decoder (trained on an <ref type="bibr" target="#b12">(13,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b17">18</ref>) input volume at p = 0.005) to test set data of size <ref type="bibr" target="#b8">(9,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b8">9)</ref> generated at p = 0.005. In the figure, each row containing a series of plots corresponds to a syndrome measurement round. For a given row, the first plot labelled Xerrors shows changes in X data qubit errors from the previous round, and the second plot labelled syn diff shows changes in observed syndromes from the previous round (see Appendix A for how changes in syndrome measurement outcomes are represented as d x √ód z binary matrices). The third plot labelled pred gives the correction applied by the NN decoder, and the fourth plot labelled syn pred corresponds to the syndrome compatible with the applied correction. The fifth plot labelled syn dif aft cor shows the remaining syndromes after the correction has been applied, and the sixth plot labelled left errors gives any remaining X data qubit errors after the correction has been applied. The last plot labelled vert clean shows the remaining syndromes after all vertical pairs of highlighted vertices have been removed. Vertical pairs are formed when the vertex associated with the measurement of a stabilizer g i is highlighted in two consecutive syndrome measurement rounds.</p><p>Comparing the fifth and seventh plot in any given row, it can be seen that the vast majority of remaining syndromes after the NN decoder has been applied consists of vertical pairs, since removing vertical pairs eliminates nearly all highlighted vertices. In Section IV B we described our protocol for performing a syndrome collapse by sheets, which removes any vertical pairs of highlighted vertices within a given sheet, but not vertical pairs between sheets. As the plots in the last column of Fig. <ref type="figure" target="#fig_9">10</ref> suggest, another strategy which can significantly reduce the density of highlighted vertices is to remove all vertical pairs of highlighted vertices which are present after the local NN decoder has been applied. More specifically, for the syndrome difference</p><formula xml:id="formula_26">s diff X (d m ) = (e (1) X ·∫Ω(2) X ‚Ä¢ ‚Ä¢ ‚Ä¢ ·∫Ω(dm) X</formula><p>), we start with the syndrome in the first round e  <ref type="figure">Appendix B</ref>) and syndrome differences after the correction is applied. The plots in the last column labelled vert clean shows the remaining syndrome differences after all pairs of vertical highlighted vertices have been removed. As can be seen, the vast majority of highlighted vertices after the application of the local NN decoder results in vertical pairs. Further, since the NN sees syndrome differences in both the future and the past given the size of its receptive field, in some cases it performs a correction on a data qubit in a round before the error actually occurs, leading to the creation of a vertical pair of highlighted vertices.</p><p>and for all j ‚àà {1, ‚Ä¢ ‚Ä¢ ‚Ä¢ , r 2 }, and setting them to zero if ·∫Ω(m) X (j) = ·∫Ω(m+1) X (j) = 1. An identical step is performed for the syndrome differences s diff Z (d m ). Note that when performing parity measurements via lattice surgery, there is a preferred direction in which a vertical cleanup should be performed (i.e. staring from the first round and moving upwards to the last vs starting from last round and moving downwards to the first). The particular direction depends on the syndrome densities above and below some reference point, and is used to maintain a higher effective distance for protecting against temporal errors. More details are provided in Appendix F.</p><p>We remark that performing a vertical cleanup without an accompanying local decoder can result in a correctable error no longer being correctable by the global decoder. In Fig. <ref type="figure" target="#fig_11">11</ref>, we show two X-type errors which are temporally separated by one syndrome measurement round, along with the corresponding highlighted vertices in a two-dimensional strip of a d = 5 surface code decoding graph G X , with the subscript X indicating it is a graph for correcting X errors. We assume that all black edges in G X have unit weight. In Fig. <ref type="figure" target="#fig_11">11a</ref>, the green shaded edges correspond to the minimum-weight correction which removes the X errors. In Fig. <ref type="figure" target="#fig_11">11b</ref>, we show the resulting highlighted vertices in G X after performing a vertical cleanup. In this case, one possible minimum-weight correction results in a logical fault as shown by the green shaded edges.</p><p>If a local NN decoder with effective distance d eff = 5 was applied prior to performing a vertical cleanup, such X-type errors would be removed and no logical failures would occur. However, we generally caution that a vertical cleanup could in fact reduce the effective code distance of the surface code if the local NN decoder has an effective distance smaller than the volume to which it is applied. Nonetheless, as shown in Section IV D below, low logical error rates and near optimal effective code distance are indeed achievable with our local NN decoders and vertical cleanup.</p><p>Lastly, at the end of Section IV B, we explained how a syndrome collapse reduces the timelike distance of a lattice surgery protocol. Performing a vertical cleanup does not have the same effect on the timelike distance, and can be applied during a lattice surgery protocol. More details are provided in Appendix F.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Numerical results.</head><p>In this section, we show the logical error rates and and syndrome density reductions achieved by the 6 and 11-layer NN's described in Section IV A (see Fig. <ref type="figure" target="#fig_5">6</ref>). We obtain our numerical results by first applying the trained NN decoder to the input volume (d x , d z , d m ), followed by either performing a syndrome collapse (as described in Section IV B) or a vertical cleanup (as described in Section IV C). After the syndrome collapse or vertical cleanup, any remaining errors are removed by performing MWPM on the resulting graph. We set edges to have unit weights since the error distributions change after applying the local NN decoder.</p><p>In what follows, we define G to be the matching graph with highlighted vertices prior to applying the local NN decoder. Since we consider a symmetric noise model, we focus only on correcting X-type Pauli errors, as Ztype errors are corrected analogously using the same network. To optimize speed, the global decoder uses separate graphs G X and G Z for correcting X and Ztype Pauli errors. However since we focus on results for X-type Paulis, to simplify the discussion we set G = G X . The graph obtained after the application of the NN decoder is labelled G (N ) (which will in general have different highlighted vertices than G), and the reduced graph obtained by performing the syndrome collapse on</p><formula xml:id="formula_27">G (N ) is labelled G (N )</formula><p>sc . Lastly, the graph obtained after applying the local NN decoder followed by a vertical cleanup is labeled G (N ) vc . We trained the 6 and 11-layer networks on data consisting of input volumes of size <ref type="bibr" target="#b12">(13,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b17">18)</ref>. The data was generated for physical depolarizing error rates of p = 10 -3 , p = 2.5 √ó 10 -3 and p = 5 √ó 10 -3 , resulting in a total of six models. For each of the physical error rates mentioned above, we generated 10 7 training examples by performing Monte Carlo simulations using the noise model described in Section II. Both the 6 and 11-layer networks were trained for 40 epochs when p = 10 -3 , and for 80 epochs when p = 2.5 √ó 10 -3 and p = 5 √ó 10 -3 . The networks were then applied to test set data generated at physical error rates in the range 10 -3 ‚â§ p ‚â§ 5 √ó 10 -3 (see Table <ref type="table" target="#tab_5">I</ref> which describes which models gave the best results for a given physical error rate used in the test set data). The networks described in Fig. <ref type="figure" target="#fig_5">6</ref> have a receptive field of size 9 √ó 9 √ó 9, and thus have a maximal effective local distance of d eff ‚â§ 9. Recall that in the last layer we use a sigmoid activation function (instead of ReLu) to ensure that the two output tensors describing X and Z data qubit corrections in each of the d m syndrome measurement rounds consists of numbers between zero and one. If this output is greater than 0.5 we apply a correction to a given qubit, otherwise we do nothing. We found numerically that a decision threshold of 0.5 gave the best results. In other words, the outputs consist of d m matrices of size d x √ó d z for X corrections, and d m matrices of size d x √ó d z for Z corrections. If the (i, j) coordinate of the matrix for X (Z) Pauli corrections in round k is greater than 0.5, we apply an X (Z) Pauli correction on the data qubit at the (i, j) coordinate of the surface code lattice in the k'th syndrome measurement round.</p><p>1. Numerical analysis when performing a syndrome collapse.</p><p>When performing a syndrome collapse, we considered sheets of size d m ‚àà {4, 5, 6}. We found numerically that d= <ref type="bibr" target="#b8">(9,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b8">9)</ref> d= <ref type="bibr" target="#b10">(11,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b10">11)</ref> d= <ref type="bibr" target="#b12">(13,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b12">13)</ref> d= <ref type="bibr" target="#b14">(15,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b14">15)</ref> d= <ref type="bibr" target="#b16">(17,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b16">17)</ref> 0.001 0.002 0.003 0.004 0.005 10 d= <ref type="bibr" target="#b10">(11,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b10">11)</ref> d= <ref type="bibr" target="#b12">(13,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b12">13)</ref> d= <ref type="bibr" target="#b14">(15,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b14">15)</ref> d= <ref type="bibr" target="#b16">(17,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b16">17)</ref> (b) FIG. <ref type="figure" target="#fig_9">13</ref>. After applying corrections from the local NN decoder, we plot the ratio r (sc) a (see the main text) between the average number of highlighted vertices in the matching graph G (N ) sc where a syndrome collapse has been performed (using sheets of size d m = 6) to the average number of highlighted vertices in the original matching graph G prior to the application of the local NN decoder followed by a syndrome collapse. In (a), the results are shown for the 6-layer network whereas in (b) the results are shown for the 11-layer network. the relationship of r (sc) a as a function of the code distance is less intuitive here compared to the results obtained in Fig. <ref type="figure" target="#fig_13">15</ref> for the vertical cleanup. The reason is that the number of two-dimensional sheets in the matching graph depend on the surface code distance, and there can be a jump of one sheet when the distances increase, as is the case for example with the d = 11 and d = 13 graphs. The logical X error rate curves for the 6 and 11-layer networks are shown in Figs. <ref type="figure" target="#fig_9">12a</ref> and<ref type="figure" target="#fig_9">12b</ref>. As a first remark, we point out that networks trained at high physical error rates don't necessarily perform better when applied to test set data obtained at lower error rates (which is in contrast to what was observed in previous works such as Ref. <ref type="bibr" target="#b32">[33]</ref>). In Table <ref type="table" target="#tab_5">I</ref> it can be seen that the 6-layer network trained at p = 0.005 outperforms the model trained at p = 0.0025 and p = 0.001 when applied to test set data generated at p ‚â• 0.0025. However, for test set data generated in the range 0.001 ‚â§ p ‚â§ 0.002, the model trained at p = 0.001 achieves lower total logical error rates. For the 11-layer network, the model trained at p = 0.0025 always outperform the model trained at p = 0.001 for all the sampled physical error rates. The window of out-performance also depends on the surface code volume. For instance, the 11-layer network trained at p = 0.005 outperforms the model trained at p = 0.0025 for p &gt; 0.001 when applied to a (9, 9, 9) surface code volume. However, when applied to a <ref type="bibr" target="#b16">(17,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b16">17)</ref>    the model trained at p = 0.005 for p ‚â§ 0.0025. More details comparing models trained at different physical error rates are discussed in Appendix C and Fig. <ref type="figure" target="#fig_9">21</ref>.</p><p>Note that to achieve better results, one can train a network for each physical error rate used in the test set data. However, in generating our results, one goal was to see how well a network trained at a particular error rate would perform when applied to data generated at a different physical error rate. In realistic settings, it is often difficult to fully characterize the noise model, and circuit-level failure rates can also fluctuate across different regions of the hardware. As such, it is important that our networks trained at a particular value of p perform well when applied to other values of p. An alternative to using models trained at different values of p would be to train a single network with data generated at different values of p. However, doing so might reduce the network's accuracy at any particular error rate. Since in practice one would have some estimate of the error rate, it would be more favorable to train the network near such error rates.</p><p>In general, we expect the logical X error rate polynomial as a function of the code distance and physical error rate p to scale as (assuming</p><formula xml:id="formula_28">d x = d z = d) p (X) L (p) = udd m (bp) (cd+w) ,<label>(9)</label></formula><p>for some parameters u, b, c and w, and where d m is the number of syndrome measurement rounds. Using the data from Fig. <ref type="figure" target="#fig_9">12b</ref>, we find that the 11-layer network has a logical X error rate polynomial  <ref type="formula">10</ref>) and <ref type="bibr" target="#b10">(11)</ref> we added labels to distinguish the polynomials arising from the 11 and 6-layer networks, and to indicate that the results are obtained from performing a syndrome collapse.</p><p>In Fig. <ref type="figure" target="#fig_9">13</ref>, we give the ratio r</p><formula xml:id="formula_29">(sc) a = A syn (G (N )</formula><p>sc )/A syn (G) where A syn (G) corresponds to the average number of "raw" syndrome differences appearing in a given spacetime volume and A syn (G (N ) sc ) corresponds to the average number syndrome differences after the application of the local NN decoder and syndrome collapse. As a side note, we remark that due to the possible creation of vertical pairs of highlighted vertices after the NN has been applied, A syn (G (N ) ) (i.e. the average number of syndrome differences after the application of the NN decoder but before performing a syndrome collapse) may have more highlighted vertices than what would be obtained if no local corrections were performed.</p><p>A small r (sc) a ratio indicates that a large number of highlighted vertices vanish after applying the local NN decoder and performing a syndrome collapse, and results in a faster implementation of MWPM or Union Find. More details on how the ratio r (sc) a affects the throughput performance of a decoder are discussed in Section V C.</p><p>The reader may remark that there are discontinuities in the plots of Figs. <ref type="figure" target="#fig_9">13a</ref> and<ref type="figure" target="#fig_9">13b</ref>, as well as the logical error rate plots in Fig. <ref type="figure" target="#fig_9">12</ref>. There are two reasons contributing to the discontinuities. The first is because the models were trained at different physical error rates; at each error rate p, we chooe the model that performs best as outlined Table <ref type="table" target="#tab_5">I</ref>. However, upon careful inspection the discontinuities are more pronounced for surface code volumes of size <ref type="bibr" target="#b8">(9,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b8">9)</ref> and <ref type="bibr" target="#b10">(11,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b10">11)</ref>. This is because the NN models were trained on a <ref type="bibr" target="#b12">(13,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b17">18)</ref> volume in order for the network to see data which is purely in the bulk (since the local receptive field of our models is 9 √ó 9 √ó 9). We do not expect a model trained on a volume where the receptive field sees data purely in the bulk to generalize well to smaller surface code volumes given the network's local receptive field will always see data containing boundaries in these scenarios. As such, to achieve better performance on volumes with d x = d z &lt; 13, one should train a network on a volume of that size.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Numerical analysis when performing a vertical cleanup.</head><p>The logical X error rates when performing a vertical cleanup after applying the 6 and 11-layer local NN decoders are shown in Figs. <ref type="figure" target="#fig_9">14a</ref> and<ref type="figure" target="#fig_9">14b</ref>. The models trained at p = 0.001, p = 0.0025 and p = 0.005 were applied to the test set data following Table <ref type="table" target="#tab_5">I</ref>. The discontinuities in the logical error rate curves occur for the same reasons as outlined above for the syndrome collapse protocol, and are particularly apparent for the 6-layer network applied to test set data generated on a (9, 9, 9) volume as shown in Fig. <ref type="figure" target="#fig_9">14a</ref>. Comparing the logical X error rate curves in Fig. <ref type="figure" target="#fig_9">14a</ref> and Fig. <ref type="figure" target="#fig_9">14b</ref> also shows the performance improvement that is gained by using a larger network (however for d ‚â• 13, only a small performance gain is observed from using the 11-layer network). The logical error rate polynomial for the 11-layer network is p</p><formula xml:id="formula_30">(X;vc) L;11l (p) = 0.0008198d 2 (107.803p) (d-1)/2 , (<label>12</label></formula><formula xml:id="formula_31">)</formula><p>and for the 6-layer network is p (X;vc)</p><formula xml:id="formula_32">L;7l (p) = 0.001022d 2 (105.752p) (d-1)/2 . (<label>13</label></formula><formula xml:id="formula_33">)</formula><p>As with the syndrome collapse, applying the local NN decoders followed by a vertical cleanup results in an effective distance d eff ‚âà d -2. It can also be observed that at p = 0.005, the logical error rate decreases when increasing the code distance d, indicating a threshold p th &gt; 0.005 when applying the local NN decoder followed by a vertical cleanup. Note that we did not generate data for p &gt; 0.005 since we are primarily concerned with the error rate regime where low logical error rates can be achieved while simultaneously being able to implement our decoders on the fast time scales required by quantum algorithms.</p><p>In Figs. <ref type="figure" target="#fig_13">15a</ref> and<ref type="figure" target="#fig_13">15b</ref> we show the ratio's r</p><formula xml:id="formula_34">(vc) a = A syn (G (N ) vc )/A syn (G) which is identical to r (sc)</formula><p>a , but where a vertical cleanup is performed instead of a syndrome collapse. For p = 0.001 and the distance d = 17 surface code, we see a reduction in the average number of highlighted vertices by nearly two orders of magnitude. Further, comparing with the ratio's r (sc) a obtained in Fig. <ref type="figure" target="#fig_9">13</ref>, we see that performing a vertical cleanup results in fewer highlighted vertices compared to performing a syndrome collapse by sheets. Such a result is primarily due to the fact that vertical pairs of highlighted vertices between sheets do not vanish after performing a syndrome collapse. Lastly we observe an interesting phenomena for the 11-layer networks trained at p = 0.001 and p = 0.0025 when applied to test set data generate near p = 0.001. Although the 11-layer trained at p = 0.0025 achieves a lower total logical failure rate (see Table <ref type="table" target="#tab_5">I</ref>), the network trained at p = 0.001 results in smaller ratio r (vc) a . This can be seen for instance by comparing the results in Figs. <ref type="figure" target="#fig_13">15a</ref> and<ref type="figure" target="#fig_13">15b</ref>, where although the 6-layer network is outperformed by the 11-layer network, a smaller r (vc) a is achieved at p = 0.001 since the the 6-layer network trained at p = 0.001 was applied to the test set data, compared to the 11-layer network which was trained at p = 0.0025.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. HARDWARE IMPLEMENTATION OF OUR NN'S</head><p>Let us now consider possible suitable embodiment's of NN decoders on classical hardware. One of the appealing features of NN evaluation is that it involves very little conditional logic. In theory, this greatly helps in lowering NN evaluation strategies to specialized hardware, where one can discard the bulk of a programmable processor as irrelevant and one can make maximal use of pipelined data pathways. In practice, such lowering comes with significant costs, among them slow design iteration, custom manufacturing, bounded size, and a great many concerns around integration with existing electronics. In this section we consider some candidate technologies which occupy compromise positions among these costs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. FPGA implementation performance</head><p>One option for specialized hardware is a Field-Programmable Gate Array (FPGA). A typical FPGA consists of a fixed set of components, including flip-flops, look-up tables (LUTs), block RAM (BRAM), configurable logic blocks (CLBs), and digital signal processing (DSP) slices, all of whose inputs can be selectively routed into one another to perform elaborate computations ranging from fixed high-performance arithmetic circuits to entire programmable processors. FPGAs have been used for NN evaluation in a variety of real-time applications; one use case particularly close to ours is the recognition of nontrivial events at the Large Hadron Collider. That working group has produced an associated software package hls4ml <ref type="bibr" target="#b64">[65]</ref> which produces a High-Level Synthesis (HLS) description of an evaluation scheme for a given initialized NN, and one can then compile that description into a high-throughput and low-latency FPGA embodiment. The tool hls4ml itself has several tunable parameters which trade between resource occupation on the target FPGA and performance in throughput and latency, e.g.: re-use of DSP slices to perform serial multiply-and-add operations rather than parallel operations; "quantization" of intermediate results to a specified bit width; and so on.</p><p>At the time of this writing, hls4ml does not support 3D convolutional layers. Rather than surmount this ourselves, we explored the realization through hls4ml of 1D and 2D convolutional networks of a similar overall structure and parameter count to the models considered in Section IV A under the assumption that the generalization to 3D will not wildly change the inferred requirements. <ref type="foot" target="#foot_2">5</ref> We report one such experiment in Fig. <ref type="figure" target="#fig_9">16</ref>, which includes both the details of the analogous model and the resulting FPGA resource usage; other networks and other hls4ml settings are broadly similar.</p><p>One way to improve model throughput is by inter-layer pipelining, i.e., deploying its individual layers to different hardware components and connecting those components along communication channels which mimic the structure of the original network. Whereas the throughput of a conventional system is reciprocal to the total time between when input arrives and when output is produced (i.e., the computation latency), the throughput of a pipelined system is reciprocal only to the computation latency of its slowest constituent component. Accordingly, we also report the FPGA resource usage for the largest layer in the network, so as to calculate pipelined throughput.</p><p>Out of the synthesis details, we highlight the re-use parameter R: the set of available such parameter values is discrete and increasingly sparse for large R; latency scales linearly with choice of large values of R and synthesis will not converge for small values of R; and the size of  <ref type="figure" target="#fig_9">16</ref>. FPGA resource costs for an hls4ml embodiment of a NN composed of 2D convolutional layers, each with 3 √ó 3 kernels and 60 output channels, taking an initial 32 √ó 32 trichannel image, for a total of 360, 180 trainable parameters and a per-layer maximum of 32, 580 trainable parameters. This model is chosen so as to limit ourselves to the functionality provided in hls4ml, while maintaining structural similarity to the models of direct interest given in Section IV A. Relative percentages reported are taken against the resources available on a Virtex Ultrascale+ FPGA (XCU250-FIGD2104-2L-E). Note that our strong quantization settings often caused hls4ml to trade DSPs for LUTs to use as multipliers.</p><formula xml:id="formula_35">Layers</formula><p>our model necessitated choosing the rather large re-use parameter R = 540 to achieve synthesis. In fact, even just synthesizing one layer required the same setting of R = 540, which results in rather meager throughput savings achieved by pipelining FPGAs, one per layer. Unfortunately, we conclude these models are nontrivial to realize within the constraints of contemporary FPGA hardware.</p><p>A promising avenue to close this gap may be networks that reduce computational cost by encoding parameters in at most a few bits, while incurring some small loss in accuracy. For instance, authors in Ref. <ref type="bibr" target="#b66">[67]</ref> used an optimized Binary Convolution NN on a Xilinx KCU1500 FPGA with order 100 ¬µs inference latencies on networks with millions of parameters (e.g., AlexNeT, VGGNet, and ResNet).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. ASIC performance and Groq</head><p>The programmability of FPGAs makes them popular for a wide variety of tasks, and hence they appear as components on a wide variety of commodity hardware. However, flexibility is double-edged: FPGAs' general utility means they are likely to be under-optimized for any specific task. Application-Specific Integrated Circuits (ASICs) form an alternative class of devices which are further tailored to a specific application domain, typically at the cost of general programmability. Groq <ref type="bibr" target="#b67">[68]</ref> is an example of a programmable ASIC which is also a strong candidate target: it is tailored toward low-latency, highthroughput evaluation of NN's, but without prescribing at manufacturing time a specific NN to be evaluated.</p><p>We applied Groq's public software tooling to synthesize binaries suitable for execution on their devices. In Figure <ref type="figure" target="#fig_9">17</ref>, we report the synthesis statistics for the 11-layer network of Section IV A and for the single largest layer of the network, both as embodied on a single Groq chip. Otherwise, we left all synthesis settings at their default, without exploring optimizations. Even with these default settings, the reported throughput when performing perlayer pipelining is within 6-10√ó of the target value of ‚âà 40 kHz. We believe that further tuning, perhaps entirely at the software level, could close this gap, amounting to one path to hardware feasibility. Such tunable features include pruning near-zero weights, quantizing the intermediate arithmetic to some lossier format, intra-layer distributed evaluation (i.e., evaluating the outputs of a given convolutional layer in parallel over several chips), instruction timing patterns, and so on.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Effect on global decoders</head><p>In Figs. <ref type="figure" target="#fig_9">13</ref> and<ref type="figure" target="#fig_13">15</ref>, we reported a multiplicative relationship between the number of "raw" syndromes A syn (G) appearing in a given spacetime volume to the number of syndromes A syn (G This value r a has significant implications for the hardware performance requirements of global decoders, which arise from the same need described in Section III to meet overall throughput. For example, the UF decoder is a serial algorithm whose runtime is nearly linear in its inbound syndrome count (see Section II), from which it follows that preceding a UF decoder by a NN preprocessing relaxes its performance requirements by the same factor r a needed meet the same throughput deadline. One can make a similar argument for more elaborate distributed decoders, such as the Blossom variant proposed by Fowler <ref type="bibr" target="#b51">[52]</ref>: if the rate at which a given worker encounters highlighted syndromes is reduced by a factor of r a , then the amount of time it can spend processing a given syndrome is scaled up by a factor of 1/r a , so that minimum performance requirements in turn are scaled up by 1/r a .</p><p>In fact, for the syndrome collapse protocol, these improvements are quite pessimistic. A decoder could take advantage of the simpler edge structure of G (N ) sc relative to G given that the syndrome collapse shrinks the size of the graph. In particular, the number of vertices and edges in G is reduced by a factor of at least d m , with d m being the size of the sheets in a syndrome collapse. For instance, the complete implementation of a serial MWPM decoder can be decomposed into two steps. The first is the construction of the syndrome graph using Dijkstra's algorithm which finds the shortest path between a source highlighted vertex and all other highlighted vertices. The second is the implementation of the serial Blossom algorithm on such graphs. Following Ref. <ref type="bibr" target="#b68">[69]</ref>, the syndrome graph using Dijkstra's algorithm has time complexity O(h(N log(N ) + M )) where h is the number of highlighted vertices in the matching graph (in our case G (N ) sc for the syndrome collapse protocol) with N vertices and M edges. The application of the local NN decoder followed by a syndrome collapse with sheets of size d m reduces h by a factor of r a and N by a factor of d m . M is reduced by a factor greater than d m because not only are there edges incident to vertices for a given syndrome measurement rounds, but there are also vertical and space-time correlated edges incident to vertices in consecutive syndrome measurement rounds. A serial Blossom algorithm when applied to a matching graph with h highlighted vertices has complexity O(h 3 log h). As such, the runtime of the serial blossom algorithm is reduced by a factor of O(1/(r 3 a )). These improvements in speed come algorithmically cheap: the procedures of syndrome collapse and vertical cleanup are both trivially spatially parallelizable, adding O(d m ) operations of preprocessing before applying the global decoder.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. CONCLUSION</head><p>In this work we developed local NN decoders using fully three-dimensional convolutions, and which can be applied to arbitrary sized (d x , d z , d m ) surface code volumes. We discussed more efficient ways of representing the training data for our networks adapted to circuit-level noise, and discussed how vertical pairs of highlighted vertices are created when applying local NN decoders. We showed how applying our local NN decoders paired with a syndrome collapse or vertical cleanup can significantly reduce the average number of highlighted vertices seen by a global decoder, thus allowing for a much faster implementation of such decoders. Performing a syndrome collapse also reduces the size of the matching graph used by the global NN decoder, providing even further runtime improvements. For some code distances and physical error rates, the syndrome densities were reduced by almost two orders of magnitude, and we expect even larger reductions when applying our methods to larger code distances than what was considered in this work. Further, our numerical results showed competitive logical error rates and a threshold of p th ‚âà 5 √ó 10 -3 for the syndrome collapse scheme and p th &gt; 5√ó10 -3 for the vertical cleanup scheme. A trade-off between throughput and performance may be required in order to run algorithms with reasonable hardware overheads while still having fast enough decoders to avoid exponential backlogs during the implementation of algorithms. Although a more direct implementation of our local NN decoders on FPGA's appears challenging, encoding the NN parameters using fewer bits may satisfy the throughput requirements discussed in Section III. Using application-specific integrated circuits (ASICs) may also allow the implementation of our NN's on time scales sufficient for running algorithms.</p><p>There are several avenues of future work. Firstly, adapting our NN decoding protocol to be compatible with sliding windows may lead to improved throughput times, as shown in Appendix D. A broader NN architecture search may lead to networks with fewer parameters that still achieve low logical failure rates with modest hardware resource overhead requirements. For instance, graph based convolutional NN's <ref type="bibr" target="#b69">[70]</ref> appear to be promising in this regard. We can also design a network architecture which removes edges from the matching graph as part of its correction, rather than applying a data qubit correction followed by an error syndrome updated based on the correction. Such an architecture could make the syndrome collapse or vertical cleanup step unnecessary since for instance vertices incident to diagonal edges arising from space-time correlated errors would be flipped. By not performing a syndrome collapse or vertical cleanup, we anticipate that such networks could achieve lower logical error rates. Another important avenue would be to show how local NN architectures can be adapted to lattice surgery settings, where surface code patches change shape through time, and where new fault patterns which are unique to lattice surgery settings can occur <ref type="bibr" target="#b59">[60]</ref>.</p><p>Given the size of the NN's, we only considered performing one pass of the NN prior to implementing MWPM. However, performing additional passes may lead to sparser syndromes, which could be a worthwhile trade-off depending on how quickly the NN's can be implemented in classical hardware.</p><p>The training data also has a large asymmetry between the number of ones and zeros for the error syndromes and data qubit errors, with zeros being much more prevalent than ones. It may be possible to exploit such asymmetries by asymmetrically weighting the two cases.</p><p>Lastly, other classical hardware approaches for implementing local NN decoders, such as ASICs, should be considered. ‚àà {0, 1} where 1 ‚â§ k ‚â§ (d 2 -1)/2 (which is one if the stabilizer is measured non-trivially and zero otherwise) is mapped to a data qubit located at the top left corner of the square if the stabilizer is weight-4, or if it is a weight-2 stabilizer along the right boundary of the lattice. For weight-2 stabilizers along the left boundary of the lattice, the bit is mapped to the top right data qubit. The final binary matrix Msyn X has d rows and d columns, with ones at the circled regions in red if the corresponding stabilizer is measured non-trivially, otherwise the entry is zero. We also label each stabilizer numerically, starting at 1 and increasing by 1 left to right, top to bottom. The corresponding entries in Msyn X are given the same label. (b) Similar to (a), but for Z error syndromes. The X-type red stabilizers map b The first two input channels to trainX correspond to the syndrome difference history s diff X (d m ) and s diff Z (d m ) defined in Definition II.2 where we only track changes in syndromes between consecutive rounds. Further, in order to make it easier for the NN to associate syndrome measurement outcomes with the corresponding data qubit errors resulting in that measured syndrome, syndrome measurement outcomes for the j'th round are converted to two-dimensional d √ó d binary matrices la- belled M syn X (j) and M syn Z (j) following the rules shown in Fig. <ref type="figure" target="#fig_18">18</ref>. Note however that the rules described in Fig. <ref type="figure" target="#fig_18">18</ref> show how to construct the M syn X (j) and M syn Z (j) matrices based on the measurement outcomes of each stabilizer of the surface code in round j. To get the final representation for s diff X (d m ) and s diff Z (d m ), we compute the matrices Msyn X (j) = M syn X (j) ‚äï M syn X (j -1) and Msyn Z (j) = M syn Z (j) ‚äï M syn Z (j -1) for j ‚â• 2, with Msyn X (1) = M syn X (1) and Msyn Z (1) = M syn Z (1).</p><p>As discussed in Section IV A, the next two channels to trainX correspond to the matrices enc(X) and enc(Z) which are identical in each syndrome measurement round unless the surface code lattice changes shape, as would be the case when performing a parity measurement via lattice surgery. The matrices enc(X) and enc(Z) are encoded using the same rules as the encoding of the matrices M syn X and M syn Z , except that a 1 is always inserted regardless of whether a stabilizer is measured non-trivially or not. For instance, for a d = 5 surface code, the matrices enc(X) and enc(Z) (of shape 5x5) would have 1's at all red circular regions in Fig. <ref type="figure" target="#fig_18">18</ref> and 0 for all other positions. So, assuming a surface code patch which doesn't change shape through time, for this d = 5 example we have</p><formula xml:id="formula_36">enc(X) j = Ô£´ Ô£¨ Ô£¨ Ô£¨ Ô£≠ 1 1 0 1 0 1 0 1 0 1 1 1 0 1 0 1 0 1 0 1 0 0 0 0 0 Ô£∂ Ô£∑ Ô£∑ Ô£∑ Ô£∏ , (A1) enc(Z) j = Ô£´ Ô£¨ Ô£¨ Ô£¨ Ô£≠ 1 1 1 1 0 0 1 0 1 0 1 0 1 0 0 0 1 0 1 0 1 0 1 0 0 Ô£∂ Ô£∑ Ô£∑ Ô£∑ Ô£∏ , (A2) where j ‚àà {1, ‚Ä¢ ‚Ä¢ ‚Ä¢ , d m }.</formula><p>When the NN is in the bulk of the lattice, it can be seen from Fig. <ref type="figure" target="#fig_18">18</ref> that syndromes associated with a particular data qubit changes shape depending on which data qubit is observed. For instance, on the second row of the lattice in Fig. <ref type="figure" target="#fig_18">18a</ref>, compare the vertices in red surrounding the qubit in the second column versus those surrounding the qubit in the third column. Since the matrices enc(X) and enc(Z) encode this information, providing such inputs to trainX helps the network distinguish between the different types of data qubits when the network's receptive field only sees qubits in the bulk. Similarly, enc(X) and enc(Z) allow the network to identify data qubits along the boundaries of the lattice. At the boundary, the pattern of 1's and 0's in enc(X) and enc(Z) is different than in the bulk. By using the encoding described by enc(X) and enc(Z), we observed significant performance improvements compared to an encoding which only specifies the location of the boundary X and Z data qubits, which are shown in Figs. <ref type="figure" target="#fig_20">19a</ref> and<ref type="figure" target="#fig_20">19b</ref> for the d = 5 surface code. By boundary X (Z) qubits, we refer to data qubits that result in a single non-trivial stabilizer measurement outcome when afflicted by an X (Z) error.</p><p>Lastly, since the last round of error correction is a round of perfect error correction where the data qubits are measured in some basis, it is also important to specify the temporal boundaries of the lattice. Specifying temporal boundaries allows the network to generalize to arbitrary syndrome measurement rounds. As such, the last channel of trainX contains the temporal boundaries, represented using d x √ó d z binary matrices for each syndrome measure- ment round. We choose an encoding where the matrices are filled with ones for rounds 1 and d m , and filled with zeros for all other rounds.</p><p>Appendix B: Homological equivalence convention for representing data qubit errors</p><p>Let E 1 and E 2 be two data qubit errors. We say that E 1 and E 2 are homologically equivalent for a code C if s(E 1 ) = s(E 2 ), and E 1 E 2 ‚àà S where S is the stabilizer group of C. In other words, E 1 and E 2 are homologically equivalent for a code C if they have the same error syndrome, and are identical up to products of stabilizers.</p><p>In Ref. <ref type="bibr" target="#b43">[44]</ref>, it was shown that training a NN where the data qubit errors were represented using a fixed choice of homological equivalence resulted in better decoding performance. In this appendix, we describe our choice of homological equivalence for representing the data qubit errors in trainY which resulted in improved decoding performance.</p><p>Recall that trainY is a tensor of shape (N train , d k . Similarly, a weight-4 X error with support on g (X) k is equal to g (X) k and can thus be removed entirely. We define the function weightReductionX which applies the weight-reduction transformations described above to each stabilizer. Similarly, weightReductionX also removes weight-2 X errors at weight-2 X-type stabilizers along the top and bottom boundaries of the lattice.</p><p>Let E x be a weight-2 X error with support on a weight-4 stabilizer g (X)</p><p>k , where the top left qubit has coordinates (Œ±, Œ≤). We define the function fixEquivalenceX as follows:</p><p>1. Suppose E x has support at the coordinates (Œ±+1, Œ≤) and (Œ± + 1, Œ≤ + 1). Then fixEquivalenceX maps E x to a weight-2 error at coordinates (Œ±, Œ≤) and (Œ±, Œ≤ + 1). Next, let g (X) k be a weight-2 X-type stabilizer along the top of the surface code lattice, with the left-most qubit in its support having coordinates (Œ±, Œ≤). If E x is a weight-1 error at coordinates (Œ±, Œ≤ + 1), fixEquivalenceX maps E x to a weight-1 error at coordinates (Œ±, Œ≤). On the other hand, if g (X) k is a weight-2 X-type stabilizer along the bottom of the surface code lattice with with the left-most qubit in its support having coordinates (Œ±, Œ≤), and E x is a weight-1 error at coordinates (Œ±, Œ≤), fixEquivalenceX maps E x to a weight-1 error at coordinates (Œ±, Œ≤ + 1).</p><p>Next let simplifyX be a function which applies weightReductionX and fixEquivalenceX to all X-type stabilizers of the surface code lattice in each syndrome measurement round (with weightReductionX being applied first), with E x errors in round 1 ‚â§ j ‚â§ d m described by the binary matrix M (X (Œ±,Œ≤) ) e (j) for all (Œ±, Œ≤) data-qubit coordinates. Thus simplifyX maps matrices M (X (Œ±,Œ≤) ) e (j) to homologically equivalent matrices M (X (Œ±,Œ≤) ) e (j) using the transformations described above. Our homological equivalence convention for X data qubit errors is implemented by repeatedly calling the function simplifyX until all matrices M (X (Œ±,Œ≤) ) e (j) satisfy the condition simplifyX(M</p><formula xml:id="formula_37">(X (Œ±,Œ≤) ) e (j)) = M (X (Œ±,Œ≤) ) e</formula><p>(j) for all syndrome measurement rounds j and data qubit coordinates (Œ±, Œ≤).</p><p>For Z-type data qubit errors, we similarly have a weightReductionZ function which reduces the weights of Z errors at each Z-type stabilizer. The function fixEquivalenceZ is chosen to be rotationally symmetric to the function fixEquivalenceX under a 90 ‚Ä¢ rotation of the surface code lattice. We then define a simplifyZ function in an identical way as simplifyX, but which calls the functions weightReductionZ and fixEquivalenceZ. Errors which are invariant under the transformations simplifyX and simplifyZ are shown in Fig. <ref type="figure" target="#fig_21">20</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix C: Comparing models trained at different error rates</head><p>In this appendix we discuss in more detail the effects of applying a network trained at different physical error rates to the test set data.</p><p>In Fig. <ref type="figure" target="#fig_9">21</ref>, we show the logical X error rate curves of the 6-layer network in Fig. <ref type="figure" target="#fig_5">6</ref> trained at p = 0.005 and p = 0.001 on training data of size <ref type="bibr" target="#b12">(13,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b17">18)</ref> when applied to test set data generated with a volume of size <ref type="bibr" target="#b12">(13,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b12">13)</ref>. The application of the local NN decoder is followed by a syndrome collapse with sheets of size d m = 6 and MWPM to correct any remaining errors. As can be seen in the plot, for p ‚â• 0.0025, the network trained at d= <ref type="bibr" target="#b12">(13,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b12">13)</ref>, p=0.005 d= <ref type="bibr" target="#b12">(13,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b12">13)</ref> p = 0.005 outperforms the network trained at p = 0.001. However, when we apply the network trained at p = 0.005 to test set data generated for p ‚â§ 0.002, not only does the model under-perform the one trained at p = 0.001, but the logical failure rate increases with decreasing p. Such a result suggests that the model trained at p = 0.005 is over-fitting to the data generated at higher physical error rates which has denser syndromes. Consequently, the model does not generalize well to data containing sparser syndromes observed at lower physical error rates. The above results show the importance of training models at different physical error rates when applying such models to the test set data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix D: Effects on buffer times using sliding windows</head><p>In this appendix we show how the buffer times, as described in Section III, can be improved by decoding using sliding windows instead of decoding over all syndrome measurement rounds of the full syndrome measurement volume. In particular, we focus on showing how the expression for T b1 in Eq. ( <ref type="formula" target="#formula_5">1</ref>) is modified in the when using sliding windows.</p><p>Suppose we perform r syndrome measurement rounds. We divide all syndrome measurement rounds into n w windows {w 1 , w 2 , ‚Ä¢ ‚Ä¢ ‚Ä¢ , w nw } with window w j containing r j syndrome measurement rounds. In our analysis we consider two cases. The "slow" case is when decoding r j rounds takes longer than performing r j syndrome measurement rounds as shown in Fig. <ref type="figure">22a</ref>. In this case we have r j T s &lt; T (rj ) DEC . The "fast" case is the opposite where decoding r j rounds takes a shorter amount of time than performing r j syndrome measurement rounds, so that r j T s &gt; T  FIG. <ref type="figure">22</ref>. Dividing the number of syndrome measurement rounds into windows, with the j'th window containing rj rounds. In (a), we consider the "slow" case where decoding rj rounds takes longer than performing rj syndrome measurement rounds so that rjTs &lt; T (r j ) DEC . In (b), we consider the "fast" case where rjTs &gt; T (r j )</p><p>DEC . Here the Q axis indicates operations performed on the quantum computer, and the C axis are operations performed on the classical computer, with T l being the latency time.</p><p>in Fig. <ref type="figure">22b</ref>. In what follows, we define T (wj ) as the time it takes to perform all r j syndrome measurement rounds and decode them for the window w j . Thus we have that</p><formula xml:id="formula_38">T b1 = nw j=1 T (wj ) . (D1)</formula><p>We also assume that T l &lt; r j T s for all 1 ‚â§ j ‚â§ n w .</p><p>For both the fast and slow cases, we have that T (w1) = T l + T (r1) DEC since the classical computer must wait for a time T l from the last syndrome measurement round in the first window before it can begin decoding the r 1 syndrome measurement rounds, which takes time T (r1) DEC . For the second window, if r 2 T s &lt; T (r1) DEC , then the signal from the last syndrome measurement round in the second window will arrive to the classical computer while it is still decoding syndromes from the first window, so that T (w2) = T (r2) DEC . On the other hand, if r 2 T s &gt; T (r1) DEC , then decoding errors in the first window will complete before the syndrome information from the second window is available to the classical computer. As such, the total time to process syndrome in the second window will be T</p><formula xml:id="formula_39">(w2) = g 1 + T (r2)</formula><p>DEC where g 1 is the time it takes for the syndrome information from the second window to be made available to the classical computer after decoding syndromes from the first window. From Fig. <ref type="figure">22b</ref> we see that g 1 = r 2 T s -T (r1) DEC . Summarizing, we have that</p><formula xml:id="formula_40">T (w2) = T (r2) DEC r 2 T s &lt; T (r1) DEC T (r2) DEC + r 2 T s -T (r1) DEC r 2 T s &gt; T (r1) DEC (D2)</formula><p>Implementing the above steps recursively, we find that</p><formula xml:id="formula_41">T b1 = T l + nw i=1 T (ri) DEC r i T s &lt; T (ri-1) DEC T l + nw i=2 r i T s r i T s &gt; T (ri-1) DEC (D3) We see that if r i T s &lt; T (ri-1)</formula><p>DEC for all i ‚àà {1, ‚Ä¢ ‚Ä¢ ‚Ä¢ , n w .}, then the analysis leading to Eq. (D3) shows that the buffer times will satisfy Eq. ( <ref type="formula" target="#formula_17">4</ref>) in the case where T (rj ) DEC = cr j for all j. However for decoding times expressed as a polynomial of degree greater than or equal to 2, summing the terms in Eq. (D3) over smaller window sizes can lead to much smaller buffer times.</p><p>Note that in Fig. <ref type="figure">22b</ref>, we assumed that T l &lt; r j T s . In the large latency regime, where T l &gt; r j T s &gt; T (rj-1) DEC for all j, a quick calculation shows that T b1 = T l + nw i=2 r i T s and so the result is unchanged. in Eq. ( <ref type="formula" target="#formula_30">12</ref>) obtained by applying the 11-layer local NN decoder followed by a vertical cleanup and MWPM. As can be seen in Fig. <ref type="figure">23</ref>, a large increase in d m results in a very modest increase in d, showing that increasing buffer times will not have a large effect on the surface code distance.</p><p>Appendix F: Effects of performing a vertical cleanup during a parity measurement implemented via lattice surgery</p><p>In this appendix we review the effects of performing a vertical cleanup when implementing a multi-qubit Pauli measurement via lattice surgery. For full details on the derivation of the matching graph, and the effects of timelike failures, the reader is referred to Ref. <ref type="bibr" target="#b15">[16]</ref>.</p><p>We consider the simple case of performing an X ‚äó X multi-qubit Pauli measurement using two surface code patches. When performing the X ‚äó X measurement, the two surface code patches are merged into one patch by preparing qubits in the routing region in the |0 state, and performing a gauge fixing step where the X-type operators are measured <ref type="bibr" target="#b70">[71]</ref>. A two-dimensional slice of the matching graph used to correct Z-type errors during the lattice surgery protocol is shown in Fig. <ref type="figure">24</ref>. In particular, in the first round of the merge, the X-type measurements performed in routing space region are random, but the product of all such measurements encode the parity of the logical X ‚äó X operator being measured. However, measurement errors can result in the wrong parity being measured. More generally, any fault mechanism resulting in an error which anticommutes with the X ‚äó X operator being measured will cause the wrong parity to be measured, and is referred to as a timelike failure. As such, repeated rounds of syndrome measurements are performed on the merged surface code patches, with the timelike distance given by the number of syndrome measurement rounds.</p><p>When performing a vertical cleanup however, timelike failures which were correctable if no vertical cleanup were performed may no longer be correctable, with an example given in Fig. <ref type="figure">24</ref>. In particular, strings of measurement errors starting from the first round of the merge patch would be unaffected by the implementation of a vertical cleanup, since a single vertex at the end of the measurement error string would be highlighted. The problematic cases arise when such error strings are combined with data qubit errors resulting in vertical pairs (as shown in Fig. <ref type="figure">24a</ref>).</p><p>We now show that there is preference in the ordering in which a vertical cleanup is performed which depends on the syndrome density either below or above some mid-point round. We also show the minimum temporal distance required to deal with a set of malignant failures, and discuss modifications to the vertical cleanup protocol to mitigate such effects. Note that in what follows, we do not remove vertical pairs between a highlighted vertex and a highlighted temporal boundary vertex.</p><p>Suppose we perform d m syndrome measurement rounds when merging surface code patches to perform a multiqubit Pauli measurement via lattice surgery. Consider the following sequence of measurement errors which occur when measuring some stabilizer g i . In the first syndrome measurement round, a measurement error occurs resulting in the wrong parity of the multi-qubit Pauli measurement. Afterwords, a measurement error occurs every two syndrome measurement rounds, until there are a total of (d m -3)/2 measurement errors. An example of such a sequence of faults is given in the third column of Fig. <ref type="figure" target="#fig_13">25a</ref>. Performing a vertical cleanup starting from the first syndrome measurement round would result in a single highlighted vertex separated to the top temporal boundary by 4 vertical edges. Clearly for large d m and assuming all vertical edges have unit weight, MWPM would choose a path matching to the top temporal boundary resulting in timelike failure. However, if we performed a vertical cleanup starting from the last syndrome measurement round and moving downwards (i.e. towards the first syndrome measurement round), then the remaining highlighted vertex would be separated to the bottom temporal boundary by a single edge of unit weight. In this case, MWPM would correctly identify the parity measurement error. More generally, suppose d m syndrome measurement rounds are performed (with d m being odd) on a merged surface code patch part of parity measurement implemented via lattice surgery. We define the mid-point round to be the round (d m + 1)/2. As can be seen in Fig. <ref type="figure" target="#fig_13">25a</ref>, for a given vertex of the syndrome measurement graph corresponding to particular stabilizer, if such a vertex is highlighted a larger number of times below the mid-point than above, a vertical cleanup on that vertex should be performed from top to bottom (i.e. starting from the round where the data qubits in the routing space are measured, and moving towards the round where they are initialized). On the other hand, if the density above the mid-point is greater than below, a vertical cleanup is performed in For dm syndrome measurement rounds (with dm odd), the round labelled mid-point is the (dm + 1)/2 round. The goal of the figure is to illustrate that if the syndrome density above the mid-point is greater than the one below the mid-point, the vertical cleanup is done from bottom to top. On the other hand, if the syndrome density below the mid-point is greater than above, the vertical cleanup is performed from top to bottom. If they are the same, then a direction for the vertical cleanup is chosen at random. (b) Sequence of measurement errors for 13 syndrome measurement rounds where after performing a vertical cleanup, the minimum-weight correction matches to the temporal boundary thus incorrectly flipping the parity.</p><p>the opposite direction. Various configurations of measurement errors are illustrated in Fig. <ref type="figure" target="#fig_13">25a</ref> showing that choosing the ordering for the vertical cleanup scheme as describe above avoids logical timelike failures. Despite the above, there is still a sequence of measurement errors where regardless of the direction in which a vertical cleanup is performed, a timelike failure will occur. Consider a sequence of measurement errors occurring in two consecutive rounds (where the first round is after the surface code patch has been merged), followed by m -4 measurement errors every two rounds, and terminating with two consecutive measurement errors again, so that the total number of measurement errors is m. An example is shown in Fig. <ref type="figure" target="#fig_13">25b</ref>. After performing a vertical cleanup, there will be two remaining highlighted vertices, associated with the first and last rounds of the sequence of measurement errors. The number of vertical edges connecting the two vertices which don't go through temporal boundary vertices is n v = 2m -3, and the number of vertical edges connecting the two vertices which go through the temporal boundary is n c v = d m -2m + 2. As such, to ensure that MWPM does not map to a temporal boundary, thus incorrectly flipping the parity of the multiqubit Pauli measurement, we must choose a large enough value of d m such that n c v &gt; n v resulting in d m &gt; 4m -5. Such an increase in d m has the effect of roughly doubling the runtime of a quantum algorithm. This increase in d m should be expected since performing a vertical cleanup is equivalent to adding additional measurement errors to the system "by hand", thus requiring a doubling in the code distance to have the same protection compared to a scheme which doesn't perform a vertical cleanup.</p><p>Two variations of the vertical cleanup protocol during a lattice surgery merge may maintain the full timelike distance and thus require fewer syndrome measurement rounds. The first variation would consist of identifying vertical pairs prior to applying the local NN decoder, and only removing new vertical pairs which are created after the local NN decoder is applied. In this case, vertical pairs due to measurement errors would not be removed, although this comes at the the cost of a higher syndrome density. Another approach would be to re-weight vertical edges incident to highlighted vertices which were removed from a vertical cleanup protocol, so that MWPM would give preferences to paths which go through such vertices. Lastly, using a TELS protocol described in Ref. <ref type="bibr" target="#b15">[16]</ref> would allow larger timelike failure rates and thus could be used to avoid having to use a large value of d m when performing a vertical cleanup. We leave the numerical analysis of such protocols, along with using TELS alongside a vertical cleanup strategy, to future work.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>FIG. 1 .</head><label>1</label><figDesc>FIG. 1. Example of a dx = dz = 5 surface code. The distances dx and dz correspond the minimum-weights of logical X and Z operators of the surface code. Minimum-weight representatives for the logical X and Z operators are shown in the figure, and form vertical and horizontal string-like excitations. Data qubits correspond to the yellow vertices in the figure, and ancilla qubits (which are used to store the stabilizer measurement outcomes) are represented by grey vertices. Red plaquettes correspond to X-type stabilizers of the surface code, and blue plaquettes correspond to Z-type stabilizers. Numbers incident to CNOT gates used to measure the stabilizers indicate the time steps in which such gates are applied.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>(r)DEC to be the time it takes the classical computer to compute a correction based on syndrome measurement outcomes arising from r rounds of EC. As such, T (r) DEC corresponds to the throughput for r rounds of EC.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>tDEC = r Œºs tDEC = 1</head><label>1</label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>= cr¬µs. Using Eq. (3), we plot T b j for different values of c. In (b), we fix T (r) DEC = r¬µs and vary the inbound latency T l .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>= T bj /T s . If we assume a linear decoding time of the form T (r) DEC = cr (where the constant c is in microseconds), solving Eqs. (1) to (3) recursively results in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>FIG. 6 .</head><label>6</label><figDesc>FIG.6. NN architectures used to train our local decoders. In (a), we consider a network with 6 layers. The first 4 layers have 50 filters of dimension<ref type="bibr" target="#b2">(3,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b2">3)</ref> and serve as feature extractors with a total receptive field of 9 √ó 9 √ó 9. The last two layers have filters of dimension (1, 1, 1), with 200 filters used in the second last layer. The last layer has 2 filters, to predict the X and Z error outputs. The network has a total of 221, 660 parameters. In (b) we use a network with 11 layers. The first 4 layers have 50 filters of dimension 3 √ó 3 √ó 3, whereas the next 6 layers have 100 filters of dimension (1, 1, 1). The last layers use 2 filters of size (1, 1, 1). The network has a total of 352, 210 parameters. We also use skip connections which becomes more relevant as the number of layers in the network becomes large to avoid exploding/vanishing gradients<ref type="bibr" target="#b62">[63,</ref><ref type="bibr" target="#b63">64]</ref>. For both networks, we perform batch normalization after each layer. All layers use the ReLu activation function except for the last layer, where we use a sigmoid activation function, to generate predictions for physical qubit errors throughout the lattice. We also use the binary cross-entropy loss function to train our networks. In (c), we provide the details of the implementation of the skip connections. For clarity, we also illustrate the batch normalization step and the implementation of the ReLu activation function.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>FIG. 7 .</head><label>7</label><figDesc>FIG.7. Average number of X errors afflicting data qubits of a dx = dz = 9 rotated surface code lattice as a function of the number of syndrome measurement rounds and the circuit-level noise model described in Section II. Results are shown for the depolarizing noise parameter p set to p = 0.001 and p = 0.005. For small noise rates, hundreds of syndrome measurement rounds are required to saturate the average X error density of 50%.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>FIG. 8 .</head><label>8</label><figDesc>FIG. 8. (a) CNOT failure (shown in red) resulting in a X data qubit error in the j'th syndrome measurement round (we only show CNOT gates which are part of the stabilizers used in this example). Due to the time steps in which the CNOT gates are implemented, only a single Z-type stabilizer detects the error in round j, with two stabilizers detecting the error in round j + 1. (b) Subset of the matching graph G associated with the dx = dz = 5 surface code shown in (a). The vertices in G are highlighted (shown in yellow) when changes in syndrome measurement outcomes are detected between consecutive syndrome measurement rounds. (c) Transformation of G after the local decoder applies a correction removing the X error. Even though the local decoder removes the error, the correction creates a vertical pair of highlighted vertices.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>FIG. 9 .</head><label>9</label><figDesc>FIG. 9. (a)On the left is a two-dimensional slice of a subset of the surface code matching graph for 5 rounds of stabilizer measurements. Horizontal edges correspond to data qubits, vertices correspond to stabilizer measurement outcomes, and the blue squares are boundary vertices connected by blue edges of zero weight. The graph has two vertical pairs of highlighted vertices. On the right of the figure is the graph obtained after performing the syndrome collapse. Both vertical pairs vanish after performing the syndrome collapse. (b) On the left of the figure is a sequence of X data qubit errors which are temporally separated, i.e. they occur in different syndrome measurement rounds. The thick green edges show the minimum-weight path which pairs all highlighted vertices (we assume all edges in the graph have unit weight) effectively correcting the errors. On the right of the figure is the graph obtained after performing the syndrome collapse, along with the minimum-weight path pairing the highlighted vertices. The correction thus results in a logical X error.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>( 1 )</head><label>1</label><figDesc>X , without the tilde). Note that if d m is not a multiple of d m , there will be d m /d m sheets with the last sheet having size d m -Œ≤d m where Œ≤ = d m /d m .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>X 5 FIG. 10 .</head><label>510</label><figDesc>FIG.10. Illustration of X-type Pauli errors occurring in a dx = dz = 9 surface code in consecutive syndrome measurement rounds (where we only track changes in errors between consecutive rounds) along with the syndrome differences observed in each round. Note that syndrome differences are mapped to a d √ó d grid following the mapping described in Appendix A. We also show the correction applied by the local NN decoder, and resulting homologically equivalent errors in the column left. errors (see Appendix B) and syndrome differences after the correction is applied. The plots in the last column labelled vert clean shows the remaining syndrome differences after all pairs of vertical highlighted vertices have been removed. As can be seen, the vast majority of highlighted vertices after the application of the local NN decoder results in vertical pairs. Further, since the NN sees syndrome differences in both the future and the past given the size of its receptive field, in some cases it performs a correction on a data qubit in a round before the error actually occurs, leading to the creation of a vertical pair of highlighted vertices.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>FIG. 11</head><label>11</label><figDesc>FIG. 11. (a) Two X-type errors temporally separated by one syndrome measurement round, along with the highlighted vertices in a two-dimensional strip of a subset of a d = 5 surface code decoding graph GX used to correct X-type Pauli errors. All black edges are taken to have unit weight. The green shaded edges correspond to the minimum-weight correction, which correctly removes the errors. (b) Resulting graph after performing the vertical cleanup. The green shaded edges correspond to a minimum-weight correction, which results in a logical fault.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head></head><label></label><figDesc>using sheets of size d m = 4 resulted in worse performance compared to sheets of size d m = 5 and d m = 6. Using sheets of size d m = 5 and d m = 6 resulted in nearly identical performance. However since sheets of size d m = 6 results in a smaller graph G (N ) sc compared to using sheets of size d m = 5, in what follows we give numerical results using sheets of size d m = 6.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>5 √ó</head><label>5</label><figDesc>surface code volume, the model trained at p = 0.0025 outperforms best p train ‚àà {1.0 √ó 10 -3 , 2.5 √ó 10 -3 , 5.0 √ó 10 -3 } layers d x d z d m at p = 1.0 √ó 10 -3 1.5 √ó 10 -3 2.0 √ó 10 -3 2.5 √ó 10 -3 3.0 √ó 10 -3 4.0 √ó 10 -3 5.0 √ó 10 -3 √ó 10 -3 1.0 √ó 10 -3 5.0 √ó 10 -3 5.0 √ó 10 -3 5.0 √ó 10 -3 5.0 √ó 10 -3 11 11 11 1.0 √ó 10 -3 1.0 √ó 10 -3 1.0 √ó 10 -3 5.0 √ó 10 -3 5.0 √ó 10 -3 5.0 √ó 10 -3 5.0 √ó 10 -3 13 13 13 1.0 √ó 10 -3 1.0 √ó 10 -3 1.0 √ó 10 -3 5.0 √ó 10 -3 5.0 √ó 10 -3 5.0 √ó 10 -3 5.0 √ó 10 -3 15 15 15 1.0 √ó 10 -3 1.0 √ó 10 -3 1.0 √ó 10 -3 5.0 √ó 10 -3 5.0 √ó 10 -3 5.0 √ó 10 -3 5.0 √ó 10 -3 17 17 17 1.0 √ó 10 -3 1.0 √ó 10 -3 1.0 √ó 10 -3 5.0 √ó 10 -3 5.0 √ó 10 -3 5.0 √ó 10 -3 5.0 √ó 10 -3 10 -3 5.0 √ó 10 -3 5.0 √ó 10 -3 5.0 √ó 10 -3 5.0 √ó 10 -3 5.0 √ó 10 -3 5.0 √ó 10 -3 11 11 11 2.5 √ó 10 -3 2.5 √ó 10 -3 5.0 √ó 10 -3 5.0 √ó 10 -3 5.0 √ó 10 -3 5.0 √ó 10 -3 5.0 √ó 10 -3 13 13 13 2.5 √ó 10 -3 2.5 √ó 10 -3 2.5 √ó 10 -3 2.5 √ó 10 -3 5.0 √ó 10 -3 5.0 √ó 10 -3 5.0 √ó 10 -3 15 15 15 2.5 √ó 10 -3 2.5 √ó 10 -3 2.5 √ó 10 -3 2.5 √ó 10 -3 5.0 √ó 10 -3 5.0 √ó 10 -3 5.0 √ó 10 -3 17 17 17 2.5 √ó 10 -3 2.5 √ó 10 -3 2.5 √ó 10 -3 2.5 √ó 10 -3 5.0 √ó 10 -3 5.0 √ó 10 -3 5.0 √ó 10 -3</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head></head><label></label><figDesc>(vc) a (see the main text) between the average number of highlighted vertices in the matching graph G (N ) vc where a vertical cleanup has been performed to the average number of highlighted vertices in the original matching graph G prior to the application of the local NN decoder followed by a vertical cleanup. In (a), the results are shown for the 6-layer network whereas in (b) the results are shown for the 11-layer network.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head></head><label></label><figDesc>(p) = 0.000260d 2 (143.084p) (d-1)/2 ,<ref type="bibr" target="#b9">(10)</ref> and the 6-layer network has a logical error rate polynomial p (X;sc)L;7l (p) = 0.000436d 2 (145.277p) (d-1)/2 ,(11)where d m = d for all of our simulations. As such, for a distance d x = d z = d surface code, applying both the 6 and 11-layer local NN decoder followed by a syndrome collapse with each sheet having height d m = 6 results in an effective code distance d eff ‚âà d -2. The plots in Fig. 12 also show a threshold of p th ‚âà 5 √ó 10 -3 . Note that in Eqs. (</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head></head><label></label><figDesc>FIG. 17. Resource costs for single Groq chip embodiments of the 11-layer NN model given in Section IV A.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head></head><label></label><figDesc>after the application of the local NN decoder and syndrome collapse A syn (G (N ) sc ) = r (sc) a ‚Ä¢ A syn (G) or the application of the local NN decoder followed by a vertical cleanup A syn (G (N ) vc ) = r (vc) a ‚Ä¢ A syn (G), where r (sc) a , r (vc) a &lt; 1. In what follows, the reader is to interpret r a to mean either r (sc) a or r (vc) a , according to whether they are applying syndrome collapse or vertical clean-up respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head>FIG. 18 .</head><label>18</label><figDesc>FIG. 18. (a) Mapping of the Z-type stabilizer measurement outcomes for a d = 5 surface code lattice to the matrix Msyn X . For each stabilizer, which we label from 1 to (d 2 -1)/2 going from left to right, top to bottom, the corresponding bit b (X) k</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_19"><head></head><label></label><figDesc>the top-left data qubit, except for weight-2 stabilizers on the top boundary of the lattice, which map b (Z) k to the bottom-left data qubit. Appendix A: Data representation for training the NN's In this appendix we describe how we represent the data used to train our convolutional NN's. In what follows, we refer to trainX as the input data to the NN used during training and trainY as the output targets. As mentioned in Section IV A, trainX is a tensor of shape (N train , d x , d z , d m , 5), where N train is the number of training examples, d x and d z correspond to the size of the vertical and horizontal boundaries of the lattice, and d m corresponds to the number of syndrome measurement rounds, with the last round being a round of perfect error correction where the data qubits are measured in some basis. We also set d x = d z = d.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_20"><head>FIG. 19 .</head><label>19</label><figDesc>FIG. 19. (a) Boundary X qubits, highlighted in green, are located along the horizontal top and bottom boundaries of the lattice. (b) Boundary Z qubits, highlighted in purple, are located along the vertical left and right boundaries of the lattice.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_21"><head>FIG. 20 .</head><label>20</label><figDesc>FIG. 20. Homological equivalence convention as shown on a d = 5 surface code lattice. (a) X error configurations which are invariant under the transformations of the functions weightReductionX and fixEquivalenceX. (b) Z error configurations which are invariant under the transformations of the functions weightReductionZ and fixEquivalenceZ.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_22"><head></head><label></label><figDesc>x , d z , d m , 2) where N train is the number of training examples. For a given training example, the first channel consists of d m binary d √ó d matrices M (X (Œ±,Œ≤) ) e (j), with 1 ‚â§ j ‚â§ d m being the label for a particular syndrome measurement round, and Œ±, Œ≤ ‚àà {1, 2, ‚Ä¢ ‚Ä¢ ‚Ä¢ , d} labelling the data qubit coordinates in the surface code lattice. Since trainY tracks changes in data qubit errors between consecutive syndrome measurement rounds, M (X (Œ±,Œ≤) ) e (j) = 1 if the data qubit at coordinate (Œ±, Œ≤) has a change in an X or Y error between rounds j -1 and j, and is zero otherwise. Similarly, the second channel of trainY consists of d m binary d √ó d matrices M (Z (Œ±,Œ≤) ) e (j) which tracks changes of Z or Y data qubit errors between consecutive syndrome measurement rounds. Now, consider a weight-4 X-type stabilizer g (X) k represented by a red plaquette in Fig. 20 (with 1 ‚â§ k ‚â§ (d 2 -1)/2), and let (Œ±, Œ≤) be the data qubit coordinate at the top left corner of g (X)k . Any weight-3 X error, with support on g (X) k can be reduced to a weight-one error by multiplying the error by g (X)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_23"><head></head><label></label><figDesc>(rj )DEC . An illustration of the fast case is shown</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_25"><head>40 FIG. 23 .</head><label>4023</label><figDesc>FIG.<ref type="bibr" target="#b22">23</ref>. Plot of the surface code distance d as a function of dm using the logical error rate polynomial p (X;vc) L;11l (p) given in Eq. (12). We set p = 10 -3 and plot for different values of Œ¥ with the requirement that p (X;vc) L;11l (p) &lt; Œ¥.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_26"><head>FIG. 24 .FIG. 25 .</head><label>2425</label><figDesc>FIG.<ref type="bibr" target="#b23">24</ref>. Illustration of a slice of a matching graph used to correct errors during an X ‚äó X multi-qubit Pauli measurement performed via lattice surgery. Highlighted vertices are shown in red, and we include temporal edges (shown in pink and purple) incident to vertices in the routing region. (a) Series of Z errors and measurement errors occurring in the routing space region. The measurement error flips the parity of the multi-qubit Pauli measurement. The corrections are shown by the edges highlighted by thick green lines. The Z errors are removed by performing MWPM, and the parity of X ‚äó X is flipped to the correct value. (b) Same as (a) but where we perform a vertical cleanup. In this case, MWPM can perform a string of Z corrections to a Z boundary, resulting in a logical Z error on one of the surface code patches. Another option is to match to the top temporal boundary, which results in a timelike failure. In both (a) and (b), a local NN decoder is not applied, in order to illustrate the effects of performing a vertical cleanup during a lattice surgery protocol.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>FIG.2. Sequence of T gates separated by buffers bj (black rectangles). The Pauli operators Pj indicate the Pauli frame immediately prior to implementing the j'th T gate. During the buffer time bj, repeated rounds of error correction are performed until the Pauli frame immediately prior to the j'th T gate is known.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>filters has more representational power,</figDesc><table><row><cell>conv3D</cell><cell>conv3D</cell><cell>conv3D</cell><cell>conv3D</cell><cell>conv3D</cell><cell>conv3D</cell></row><row><cell>50 filters</cell><cell>50 filters</cell><cell>50 filters</cell><cell>50 filters</cell><cell>200 filters</cell><cell>2 filters</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>)</figDesc><table><row><cell></cell><cell></cell><cell cols="2">Skip connection</cell><cell></cell><cell cols="2">Skip connection</cell><cell></cell><cell cols="2">Skip connection</cell><cell></cell></row><row><cell>conv3D</cell><cell>conv3D</cell><cell>conv3D</cell><cell>conv3D</cell><cell>conv3D</cell><cell>conv3D</cell><cell>conv3D</cell><cell>conv3D</cell><cell>conv3D</cell><cell>conv3D</cell><cell>conv3D</cell></row><row><cell>50 filters</cell><cell>50 filters</cell><cell>50 filters</cell><cell>50 filters</cell><cell>100 filters</cell><cell>100 filters</cell><cell>100 filters</cell><cell>100 filters</cell><cell>100 filters</cell><cell>100 filters</cell><cell>2 filters</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>FIG.12. Logical X error rates for surface code volumes ranging between<ref type="bibr" target="#b8">(9,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b8">9)</ref> and<ref type="bibr" target="#b16">(17,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b16">17)</ref> after the application of the local NN decoder, followed by a syndrome collapse (with the input volumes partitioned into sheets of temporal height d m = 6) and MWPM to correct any remaining errors. In (a) the results are for the 6-layer network whereas in (b) the results are for the 11-layer network.</figDesc><table><row><cell></cell><cell>0.010</cell><cell></cell><cell></cell><cell></cell><cell>0.010</cell><cell>d=(9,9,9)</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>d=(11,11,11)</cell><cell></cell><cell></cell></row><row><cell>Logical X error rate</cell><cell>10 -6 10 -5 10 -4 0.001</cell><cell></cell><cell></cell><cell>Logical X error rate</cell><cell>10 -5 10 -4 0.001</cell><cell>d=(13,13,13) d=(15,15,15) d=(17,17,17)</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>10 -6</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>-7</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">0.001</cell><cell>0.002</cell><cell>0.003</cell><cell>0.004</cell><cell>0.005</cell></row><row><cell></cell><cell></cell><cell>(a)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>(b)</cell><cell></cell></row><row><cell cols="2">0.275</cell><cell></cell><cell></cell><cell>d=(9,9,9)</cell><cell>0.30</cell><cell></cell><cell></cell><cell></cell><cell>d=(9,9,9)</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>d=(11,11,11)</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">0.250</cell><cell></cell><cell></cell><cell>d=(13,13,13)</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">0.225</cell><cell></cell><cell></cell><cell>d=(15,15,15) d=(17,17,17)</cell><cell>0.25</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">0.200</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.20</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">0.175</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.001</cell><cell>0.002</cell><cell>0.003</cell><cell>0.004 0.005</cell><cell>0.15</cell><cell>0.001</cell><cell>0.002</cell><cell>0.003</cell><cell>0.004 0.005</cell></row><row><cell></cell><cell></cell><cell>(a)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE I .</head><label>I</label><figDesc>Table showing the error rates at which the 6 and 11-layer NN were trained to give the lowest total logical X + Z error rate when applied to test set data of volume (dx, dz, dm) and physical error rate p. The first column gives the input volume of the test set data. Subsequent columns give the error rates used to train the best performing NN model when applied to the physical error rates used to generate the test set data given in the top row.</figDesc><table><row><cell></cell><cell>0.010</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.010</cell><cell></cell></row><row><cell></cell><cell></cell><cell>d=(9,9,9)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>d=(9,9,9)</cell></row><row><cell></cell><cell></cell><cell>d=(11,11,11)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>d=(11,11,11)</cell></row><row><cell>Logical X error rate</cell><cell>0.001 10 -6 10 -5 10 -4</cell><cell>d=(13,13,13) d=(15,15,15) d=(17,17,17)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Logical X error rate</cell><cell>0.001 10 -6 10 -5 10 -4</cell><cell>d=(13,13,13) d=(15,15,15) d=(17,17,17)</cell></row><row><cell></cell><cell>10 -7</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>10 -7</cell><cell></cell></row><row><cell></cell><cell></cell><cell>0.001</cell><cell>0.002</cell><cell></cell><cell>0.003</cell><cell>0.004</cell><cell>0.005</cell><cell></cell><cell>0.001</cell><cell>0.002</cell><cell>0.003</cell><cell>0.004</cell><cell>0.005</cell></row><row><cell></cell><cell></cell><cell></cell><cell>(a)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>(b)</cell></row><row><cell></cell><cell>0.20</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>d=(9,9,9) d=(11,11,11)</cell><cell>0.20</cell><cell></cell><cell>d=(9,9,9) d=(11,11,11)</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>d=(13,13,13)</cell><cell></cell><cell></cell><cell>d=(13,13,13)</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>d=(15,15,15)</cell><cell></cell><cell></cell><cell>d=(15,15,15)</cell></row><row><cell></cell><cell>0.10</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>d=(17,17,17)</cell><cell>0.10</cell><cell></cell><cell>d=(17,17,17)</cell></row><row><cell></cell><cell>0.05</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.05</cell><cell></cell></row><row><cell></cell><cell>0.02</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>0.001</cell><cell>0.002</cell><cell>0.003</cell><cell cols="2">0.004 0.005</cell><cell></cell><cell></cell><cell>0.001</cell><cell>0.002</cell><cell>0.003</cell><cell>0.004 0.005</cell></row><row><cell></cell><cell></cell><cell></cell><cell>(a)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>(b)</cell></row></table><note><p><p><p><p><p><p><p><p><p>FIG.</p><ref type="bibr" target="#b13">14</ref></p>. Logical X error rates for surface code volumes ranging between</p><ref type="bibr" target="#b8">(9,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b8">9)</ref> </p>and</p><ref type="bibr" target="#b16">(17,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b16">17)</ref> </p>after the application of the local NN decoder, followed by a vertical cleanup and MWPM to correct any remaining errors. In (a) the results are for the 6-layer network whereas in (b) the results are for the 11-layer network. FIG.</p>15</p>. After applying corrections from the local NN decoder, we plot the ratio r</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>Thus horizontal X errors at the bottom of g to a weight-2 error at coordinates (Œ±, Œ≤ + 1) and (Œ± + 1, Œ≤). Thus diagonal X errors from the top left to bottom right of g</figDesc><table><row><cell>(X) k top of g k . are mapped to horizontal X errors at the (X) 2. Suppose E x has support at the coordinates (Œ±, Œ≤) and (Œ± + 1, Œ≤). Then fixEquivalenceX maps E x to a weight-2 error at coordinates (Œ±, Œ≤ + 1) and (Œ± + 1, Œ≤ + 1). Thus vertical X errors at the left of g (X) k are mapped to vertical X errors at the right of g (X) k . 3. Suppose E (X) k are mapped to diagonal X errors at the top right to bottom left of g (X)</cell></row></table><note><p><p>x has support at the coordinates (Œ±, Œ≤) and (Œ± + 1, Œ≤ + 1). Then fixEquivalenceX maps</p>E x k .</p></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0"><p>For the surface code, a Pauli Y error can result in more than two highlighted vertices, thus requiring hyperedges. Such hyperedges can then be mapped to edges associated with X and Z Pauli errors.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1"><p>In this context, the sliding-window computation should not be confused with the sliding window approach of Ref.<ref type="bibr" target="#b47">[48]</ref>, where MWPM is performed in "chuncks" of size O(d) for a distance d code, with the temporal corrections from the previous window used as input into the MWPM decoder applied to the next window. In our case, the NN takes the entire volume as its input, and performs corrections on each qubit in the volume using only local information.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_2"><p>As evidence, we verified that 1D and 2D convolutional networks of similar shape and size occupy similar FPGA resources under hls4ml. See also the argument in Section 2.2 of Ref.<ref type="bibr" target="#b65">[66]</ref>.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>VII. ACKNOWLEDGEMENTS C.C. would like to thank Aleksander Kubica, Nicola Pancotti, Connor Hann, Arne Grimsmo and Oskar Painter for useful discussions.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix E: Dependence of the surface code distance on dm</head><p>In Section III we showed how the buffer times T bj can increase with the number of consecutive non-Clifford gates in a quantum algorithm. One may be concerned that a large increase in buffer times could result in a much larger surface code distance in order to maintain a target logical failure rate Œ¥ set by a particular quantum algorithm. In this appendix we show that the code distance increase logarithmically with increasing buffer times.</p><p>Recall that the logical X error rate polynomial for a surface of distance (d x , d z ) is given by</p><p>for some constants u, b, c and k and where we assume that d m syndrome measurement rounds were performed. A quantum algorithm will have some target logical error rate Œ¥ with the requirement that p where ProductLog(x) gives the principle solution for w in the equation x = we w . In Fig. <ref type="figure">23</ref> we show a plot of d as a function of d m for various values of Œ¥ and fix p to be p = 10 -3 . We used the logical X error rate polynomial p (X;vc) L;11l (p) given</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Polynomial-time algorithms for prime factorization and discrete logarithms on a quantum computer</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">W</forename><surname>Shor</surname></persName>
		</author>
		<idno>quant-ph/9508027</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th Annual Symposium on Foundations of Computer Science</title>
		<meeting>the 35th Annual Symposium on Foundations of Computer Science<address><addrLine>Santa Fe, NM</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Rapid sampling though quantum computing</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">K</forename><surname>Grover</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th Annual ACM Symposium on the Theory of Computation</title>
		<meeting>the 28th Annual ACM Symposium on the Theory of Computation<address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1996">1996</date>
			<biblScope unit="page" from="212" to="219" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Quantum mechanics helps in searching for a needle in a haystack</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">K</forename><surname>Grover</surname></persName>
		</author>
		<idno type="DOI">10.1103/PhysRevLett.79.325</idno>
	</analytic>
	<monogr>
		<title level="j">Phys. Rev. Lett</title>
		<imprint>
			<biblScope unit="volume">79</biblScope>
			<biblScope unit="page">325</biblScope>
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Fault-tolerant quantum computation</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">W</forename><surname>Shor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 37th Annual Symposium on Foundations of Computer Science</title>
		<meeting>the 37th Annual Symposium on Foundations of Computer Science</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="1996">1996</date>
			<biblScope unit="page" from="56" to="65" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Thresholds for universal concatenated quantum codes</title>
		<author>
			<persName><forename type="first">C</forename><surname>Chamberland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Jochym-O'connor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Laflamme</surname></persName>
		</author>
		<idno type="DOI">10.1103/PhysRevLett.117.010501</idno>
	</analytic>
	<monogr>
		<title level="j">Phys. Rev. Lett</title>
		<imprint>
			<biblScope unit="volume">117</biblScope>
			<biblScope unit="page">10501</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Overhead analysis of universal concatenated quantum codes</title>
		<author>
			<persName><forename type="first">C</forename><surname>Chamberland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Jochym-O'connor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Laflamme</surname></persName>
		</author>
		<idno type="DOI">10.1103/PhysRevA.95.022313</idno>
	</analytic>
	<monogr>
		<title level="j">Phys. Rev. A</title>
		<imprint>
			<biblScope unit="volume">95</biblScope>
			<biblScope unit="page">22313</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Surface codes: Towards practical large-scale quantum computation</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Fowler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mariantoni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Martinis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">N</forename><surname>Cleland</surname></persName>
		</author>
		<idno type="DOI">10.1103/PhysRevA.86.032324</idno>
	</analytic>
	<monogr>
		<title level="j">Phys. Rev. A</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="page">32324</biblScope>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Universal faulttolerant quantum computation with only transversal gates and error correction</title>
		<author>
			<persName><forename type="first">A</forename><surname>Paetznick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">W</forename><surname>Reichardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physical review letters</title>
		<imprint>
			<biblScope unit="volume">111</biblScope>
			<biblScope unit="page">90505</biblScope>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Faulttolerant conversion between the steane and reed-muller quantum codes</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Duclos-Cianci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Poulin</surname></persName>
		</author>
		<idno type="DOI">10.1103/PhysRevLett.113.080501</idno>
	</analytic>
	<monogr>
		<title level="j">Phys. Rev. Lett</title>
		<imprint>
			<biblScope unit="volume">113</biblScope>
			<biblScope unit="page">80501</biblScope>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Universal faulttolerant gates on concatenated stabilizer codes</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">J</forename><surname>Yoder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Takagi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">L</forename><surname>Chuang</surname></persName>
		</author>
		<idno type="DOI">10.1103/PhysRevX.6.031039</idno>
	</analytic>
	<monogr>
		<title level="j">Phys. Rev. X</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">31039</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Fowler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Gidney</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.06709</idno>
		<title level="m">Low overhead quantum computation using lattice surgery</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">A Game of Surface Codes: Large-Scale Quantum Computing with Lattice Surgery</title>
		<author>
			<persName><forename type="first">D</forename><surname>Litinski</surname></persName>
		</author>
		<idno type="DOI">10.22331/q-2019-03-05-128</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">128</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Magic State Distillation: Not as Costly as You Think</title>
		<author>
			<persName><forename type="first">D</forename><surname>Litinski</surname></persName>
		</author>
		<idno type="DOI">10.22331/q-2019-12-02-205</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">205</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Very low overhead faulttolerant magic state preparation using redundant ancilla encoding and flag qubits</title>
		<author>
			<persName><forename type="first">C</forename><surname>Chamberland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Noh</surname></persName>
		</author>
		<idno type="DOI">10.1038/s41534-020-00319-5</idno>
	</analytic>
	<monogr>
		<title level="j">npj Quantum Information</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">91</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Building a fault-tolerant quantum computer using concatenated cat codes</title>
		<author>
			<persName><forename type="first">C</forename><surname>Chamberland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Noh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Arrangoiz-Arriola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">T</forename><surname>Campbell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">T</forename><surname>Hann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Iverson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Putterman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">C</forename><surname>Bohdanowicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">T</forename><surname>Flammia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Keller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Refael</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Preskill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">H</forename><surname>Safavi-Naeini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Painter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">G</forename><surname>Brand√£o</surname></persName>
		</author>
		<idno type="DOI">10.1103/PRXQuantum.3.010329</idno>
	</analytic>
	<monogr>
		<title level="j">PRX Quantum</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">10329</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Universal quantum computing with twist-free and temporally encoded lattice surgery</title>
		<author>
			<persName><forename type="first">C</forename><surname>Chamberland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">T</forename><surname>Campbell</surname></persName>
		</author>
		<idno type="DOI">10.1103/PRXQuantum.3.010331</idno>
	</analytic>
	<monogr>
		<title level="j">PRX Quantum</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">10331</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Theory of fault-tolerant quantum computation</title>
		<author>
			<persName><forename type="first">D</forename><surname>Gottesman</surname></persName>
		</author>
		<idno type="DOI">10.1103/PhysRevA.57.127</idno>
	</analytic>
	<monogr>
		<title level="j">Phys. Rev. A</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="page">127</biblScope>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Quantum error correction for quantum memories</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">M</forename></persName>
		</author>
		<idno type="DOI">10.1103/RevModPhys.87.307</idno>
	</analytic>
	<monogr>
		<title level="j">Rev. Mod. Phys</title>
		<imprint>
			<biblScope unit="volume">87</biblScope>
			<biblScope unit="page">307</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Analysis of quantum error-correcting codes: Symplectic lattice codes and toric codes</title>
		<author>
			<persName><forename type="first">J</forename><surname>Harrington</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">P</forename><surname>Breuckmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Duivenvoorden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Michels</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">M</forename></persName>
		</author>
		<title level="m">Local Decoders for the 2D and 4D Toric Code, Quantum Information and Computation</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page">181</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Cellular automaton decoders of topological quantum memories in the fault tolerant setting</title>
		<author>
			<persName><forename type="first">M</forename><surname>Herold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Kastoryano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">T</forename><surname>Campbell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Eisert</surname></persName>
		</author>
		<idno type="DOI">10.1088/1367-2630/aa7099</idno>
	</analytic>
	<monogr>
		<title level="j">New Journal of Physics</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page">63012</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Cellular-automaton decoders with provable thresholds for topological codes</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kubica</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Preskill</surname></persName>
		</author>
		<idno type="DOI">10.1103/PhysRevLett.123.020501</idno>
	</analytic>
	<monogr>
		<title level="j">Phys. Rev. Lett</title>
		<imprint>
			<biblScope unit="volume">123</biblScope>
			<biblScope unit="page">20501</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Cellular automaton decoders for topological quantum codes with noisy measurements and beyond</title>
		<author>
			<persName><forename type="first">M</forename><surname>Vasmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">E</forename><surname>Browne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kubica</surname></persName>
		</author>
		<idno type="DOI">10.1038/s41598-021-81138-2</idno>
	</analytic>
	<monogr>
		<title level="j">Scientific Reports</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">2027</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Fast decoders for topological quantum codes</title>
		<author>
			<persName><forename type="first">G</forename><surname>Duclos-Cianci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Poulin</surname></persName>
		</author>
		<idno type="DOI">10.1103/PhysRevLett.104.050504</idno>
	</analytic>
	<monogr>
		<title level="j">Phys. Rev. Lett</title>
		<imprint>
			<biblScope unit="volume">104</biblScope>
			<biblScope unit="page">50504</biblScope>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Fault-tolerant renormalization group decoder for abelian topological codes</title>
		<author>
			<persName><forename type="first">G</forename><surname>Duclos-Cianci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Poulin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Quantum Information and Computation</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="721" to="740" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Almost-linear time decoding algorithm for topological codes</title>
		<author>
			<persName><forename type="first">N</forename><surname>Delfosse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">H</forename><surname>Nickerson</surname></persName>
		</author>
		<idno type="DOI">10.22331/q-2021-12-02-595</idno>
	</analytic>
	<monogr>
		<title level="j">Quantum</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">595</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<author>
			<persName><forename type="first">N</forename><surname>Delfosse</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.11427</idno>
		<title level="m">Hierarchical decoding to reduce hardware requirements for quantum computing</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note>arXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Neural decoder for topological codes</title>
		<author>
			<persName><forename type="first">G</forename><surname>Torlai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">G</forename><surname>Melko</surname></persName>
		</author>
		<idno type="DOI">10.1103/PhysRevLett.119.030501</idno>
	</analytic>
	<monogr>
		<title level="j">Phys. Rev. Lett</title>
		<imprint>
			<biblScope unit="volume">119</biblScope>
			<biblScope unit="page">30501</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Deep neural network probabilistic decoder for stabilizer codes</title>
		<author>
			<persName><forename type="first">S</forename><surname>Krastanov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Jiang</surname></persName>
		</author>
		<idno type="DOI">10.1038/s41598-017-11266-1</idno>
	</analytic>
	<monogr>
		<title level="j">Scientific Reports</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">11003</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Decoding small surface codes with feedforward neural networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Varsamopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Criger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Bertels</surname></persName>
		</author>
		<idno type="DOI">10.1088/2058-9565/aa955a</idno>
	</analytic>
	<monogr>
		<title level="j">Quantum Science and Technology</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">15004</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Machine-learning-assisted correction of correlated qubit errors in a topological code</title>
		<author>
			<persName><forename type="first">P</forename><surname>Baireuther</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">E</forename><surname>O'brien</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Tarasinski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">W J</forename><surname>Beenakker</surname></persName>
		</author>
		<idno type="DOI">10.22331/q-2018-01-29-48</idno>
	</analytic>
	<monogr>
		<title level="j">Quantum</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">48</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">P</forename><surname>Breuckmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Ni</surname></persName>
		</author>
		<idno type="DOI">10.22331/q-2018-05-24-68</idno>
		<title level="m">Scalable Neural Network Decoders for Higher Dimensional Quantum Codes</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">68</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Deep neural decoders for near term fault-tolerant experiments</title>
		<author>
			<persName><forename type="first">C</forename><surname>Chamberland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ronagh</surname></persName>
		</author>
		<idno type="DOI">10.1088/2058-9565/aad1f7</idno>
	</analytic>
	<monogr>
		<title level="j">Quantum Science and Technology</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">44002</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Reinforcement learning decoders for faulttolerant quantum computation</title>
		<author>
			<persName><forename type="first">R</forename><surname>Sweke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Kesselring</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">P L</forename><surname>Van Nieuwenburg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Eisert</surname></persName>
		</author>
		<idno type="DOI">10.1088/2632-2153/abc609</idno>
	</analytic>
	<monogr>
		<title level="j">Machine Learning: Science and Technology</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">25005</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Decoding surface code with a distributed neural network-based decoder</title>
		<author>
			<persName><forename type="first">S</forename><surname>Varsamopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Bertels</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">G</forename><surname>Almudever</surname></persName>
		</author>
		<idno type="DOI">10.1007/s42484-020-00015-9</idno>
	</analytic>
	<monogr>
		<title level="j">Quantum Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="2524" to="4914" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Quantum error correction for the toric code using deep reinforcement learning</title>
		<author>
			<persName><forename type="first">P</forename><surname>Andreasson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Johansson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Liljestrand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Granath</surname></persName>
		</author>
		<idno type="DOI">10.22331/q-2019-09-02-183</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">183</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Symmetries for a high-level neural decoder on the toric code</title>
		<author>
			<persName><forename type="first">T</forename><surname>Wagner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kampermann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Bru√ü</surname></persName>
		</author>
		<idno type="DOI">10.1103/PhysRevA.102.042411</idno>
	</analytic>
	<monogr>
		<title level="j">Phys. Rev. A</title>
		<imprint>
			<biblScope unit="volume">102</biblScope>
			<biblScope unit="page">42411</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Comparing neural network based decoders for the surface code</title>
		<author>
			<persName><forename type="first">S</forename><surname>Varsamopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Bertels</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">G</forename><surname>Almudever</surname></persName>
		</author>
		<idno type="DOI">10.1109/TC.2019.2948612</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Computers</title>
		<imprint>
			<biblScope unit="volume">69</biblScope>
			<biblScope unit="page">300</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Deep q-learning decoder for depolarizing noise on the toric code</title>
		<author>
			<persName><forename type="first">D</forename><surname>Fitzek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Eliasson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">F</forename><surname>Kockum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Granath</surname></persName>
		</author>
		<idno type="DOI">10.1103/PhysRevResearch.2.023230</idno>
	</analytic>
	<monogr>
		<title level="j">Phys. Rev. Research</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">23230</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Neural ensemble decoding for topological quantum error-correcting codes</title>
		<author>
			<persName><forename type="first">M</forename><surname>Sheth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">Z</forename><surname>Jafarzadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Gheorghiu</surname></persName>
		</author>
		<idno type="DOI">10.1103/PhysRevA.101.032338</idno>
	</analytic>
	<monogr>
		<title level="j">Phys. Rev. A</title>
		<imprint>
			<biblScope unit="volume">101</biblScope>
			<biblScope unit="page">32338</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<author>
			<persName><forename type="first">X</forename><surname>Ni</surname></persName>
		</author>
		<idno type="DOI">10.22331/q-2020-08-24-310</idno>
	</analytic>
	<monogr>
		<title level="m">Neural Network Decoders for Large-Distance 2D Toric Codes</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">310</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Reinforcement learning for optimal error correction of toric codes</title>
		<author>
			<persName><forename type="first">L</forename><surname>Domingo Colomer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Skotiniotis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Mu√±oz-Tapia</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.physleta.2020.126353</idno>
	</analytic>
	<monogr>
		<title level="j">Physics Letters A</title>
		<imprint>
			<biblScope unit="volume">384</biblScope>
			<biblScope unit="page">126353</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Scalable neural decoder for topological surface codes</title>
		<author>
			<persName><forename type="first">K</forename><surname>Meinerz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-Y</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Trebst</surname></persName>
		</author>
		<idno type="DOI">10.1103/PhysRevLett.128.080505</idno>
	</analytic>
	<monogr>
		<title level="j">Phys. Rev. Lett</title>
		<imprint>
			<biblScope unit="volume">128</biblScope>
			<biblScope unit="page">80505</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Gicev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">C L</forename><surname>Hollenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Usman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.05854</idno>
		<title level="m">A scalable and fast artificial neural network syndrome decoder for surface codes</title>
		<imprint/>
	</monogr>
	<note>arXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Low-distance surface codes under realistic quantum noise</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Tomita</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">M</forename><surname>Svore</surname></persName>
		</author>
		<idno type="DOI">10.1103/PhysRevA.90.062320</idno>
	</analytic>
	<monogr>
		<title level="j">Phys. Rev. A</title>
		<imprint>
			<biblScope unit="volume">90</biblScope>
			<biblScope unit="page">62320</biblScope>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Paths, trees, and flowers</title>
		<author>
			<persName><forename type="first">J</forename><surname>Edmonds</surname></persName>
		</author>
		<idno type="DOI">10.4153/CJM-1965-045-4</idno>
	</analytic>
	<monogr>
		<title level="j">Canadian Journal of mathematics</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page">449</biblScope>
			<date type="published" when="1965">1965</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Fault-tolerant quantum computation by anyons</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Kitaev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals of Physics</title>
		<imprint>
			<biblScope unit="volume">303</biblScope>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Topological quantum memory</title>
		<author>
			<persName><forename type="first">E</forename><surname>Dennis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kitaev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Landahl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Preskill</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Mathematical Physics</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="page">4452</biblScope>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Good quantum errorcorrecting codes exist</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">R</forename><surname>Calderbank</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">W</forename><surname>Shor</surname></persName>
		</author>
		<idno type="DOI">10.1103/PhysRevA.54.1098</idno>
	</analytic>
	<monogr>
		<title level="j">Phys. Rev. A</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="page">1098</biblScope>
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Multiple particle interference and quantum error correction</title>
		<author>
			<persName><forename type="first">A</forename><surname>Steane</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc.Roy.Soc.Lond. A</title>
		<imprint>
			<biblScope unit="volume">452</biblScope>
			<biblScope unit="page">2551</biblScope>
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Fault-tolerant weighted union-find decoding on the toric code</title>
		<author>
			<persName><forename type="first">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Newman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">R</forename><surname>Brown</surname></persName>
		</author>
		<idno type="DOI">10.1103/PhysRevA.102.012419</idno>
	</analytic>
	<monogr>
		<title level="j">Phys. Rev. A</title>
		<imprint>
			<biblScope unit="volume">102</biblScope>
			<biblScope unit="page">12419</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Minimum weight perfect matching of faulttolerant topological quantum error correction in average o(1) parallel time</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Fowler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Quantum Info. Comput</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="145" to="158" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Towards practical classical processing for the surface code: Timing analysis</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Fowler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Whiteside</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">C L</forename><surname>Hollenberg</surname></persName>
		</author>
		<idno type="DOI">10.1103/PhysRevA.86.042313</idno>
	</analytic>
	<monogr>
		<title level="j">Phys. Rev. A</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="page">42313</biblScope>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">A scalable decoder micro-architecture for fault-tolerant quantum computing</title>
		<author>
			<persName><forename type="first">P</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">A</forename><surname>Pattison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Manne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Carmean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">M</forename><surname>Svore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">K</forename><surname>Qureshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Delfosse</surname></persName>
		</author>
		<idno>CoRR abs/2001.06598</idno>
		<imprint>
			<date type="published" when="2001">2020. 2001</date>
			<biblScope unit="page">6598</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Topological and subsystem codes on lowdegree graphs with flag qubits</title>
		<author>
			<persName><forename type="first">C</forename><surname>Chamberland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">J</forename><surname>Yoder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Hertzberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">W</forename><surname>Cross</surname></persName>
		</author>
		<idno type="DOI">10.1103/PhysRevX.10.011022</idno>
	</analytic>
	<monogr>
		<title level="j">Phys. Rev. X</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">11022</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Triangular color codes on trivalent graphs with flag qubits</title>
		<author>
			<persName><forename type="first">C</forename><surname>Chamberland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kubica</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">J</forename><surname>Yoder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="DOI">10.1088/1367-2630/ab68fd</idno>
	</analytic>
	<monogr>
		<title level="j">New Journal of Physics</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page">23019</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Quantum computing with realistically noisy devices</title>
		<author>
			<persName><forename type="first">E</forename><surname>Knill</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">434</biblScope>
			<biblScope unit="page">39</biblScope>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Fault-tolerant quantum computing in the Pauli or Clifford frame with slow error diagnostics</title>
		<author>
			<persName><forename type="first">C</forename><surname>Chamberland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Iyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Poulin</surname></persName>
		</author>
		<idno type="DOI">10.22331/q-2018-01-04-43</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">43</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">Lattice Surgery with a Twist: Simplifying Clifford Gates of Surface Codes</title>
		<author>
			<persName><forename type="first">D</forename><surname>Litinski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">V</forename><surname>Oppen</surname></persName>
		</author>
		<idno type="DOI">10.22331/q-2018-05-04-62</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">62</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Circuit-level protocol and analysis for twist-based lattice surgery</title>
		<author>
			<persName><forename type="first">C</forename><surname>Chamberland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">T</forename><surname>Campbell</surname></persName>
		</author>
		<idno type="DOI">10.1103/PhysRevResearch.4.023090</idno>
	</analytic>
	<monogr>
		<title level="j">Phys. Rev. Research</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">23090</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><surname>Gidney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Fowler</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.08916</idno>
		<title level="m">Flexible layout of surface code computations using AutoCCZ states, arXiv e-prints</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Realizing repeated quantum error correction in a distance-three surface code</title>
		<author>
			<persName><forename type="first">S</forename><surname>Krinner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Lacroix</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Remm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Di Paolo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Genois</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Leroux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Hellings</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lazar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Swiadek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Herrmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">J</forename><surname>Norris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">K</forename><surname>Andersen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>M√ºller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Blais</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Eichler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Wallraff</surname></persName>
		</author>
		<idno type="DOI">10.1038/s41586-022-04566-8</idno>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">605</biblScope>
			<biblScope unit="page" from="669" to="674" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2016.90</idno>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-T</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bailey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Ma</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.05990</idno>
		<title level="m">Skip Connections Matter: On the Transferability of Adversarial Examples Generated with ResNets, arXiv e-prints</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<title level="m" type="main">hls4ml: An open-source codesign workflow to empower scientific low-power machine learning devices</title>
		<author>
			<persName><forename type="first">F</forename><surname>Fahim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Hawks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Herwig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hirschauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jindariani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">P</forename><surname>Carloni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">D</forename><surname>Guglielmo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">C</forename><surname>Harris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Krupa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Rankin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">B</forename><surname>Valentin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Hester</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mamish</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Orgrenci-Memik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Aarrestad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Javed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Loncar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pierini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Pol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Summers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Duarte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hauck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ngadiuba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Hoang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Kreinar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<idno>2103.05579</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<title level="m" type="main">A uniform architecture design for accelerating 2d and 3d cnns on fpgas</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Chow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="DOI">10.3390/electronics8010065</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">8</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">LP-BNN: Ultra-low-latency BNN inference with layer parallelism</title>
		<author>
			<persName><forename type="first">T</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">L</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Herbordt</surname></persName>
		</author>
		<idno type="DOI">10.1109/ASAP.2019.00-43</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Application-Specific Systems, Architectures and Processors</title>
		<meeting>the International Conference on Application-Specific Systems, Architectures and Processors</meeting>
		<imprint>
			<date type="published" when="2019-07-09">2019-July, 9 (2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Think fast: A tensor streaming processor (TSP) for accelerating deep learning workloads</title>
		<author>
			<persName><forename type="first">D</forename><surname>Abts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sparling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wong-Vanharen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hawkins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Thompson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kahsai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Kimmell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Leslie-Hurd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">R</forename><surname>Creswick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Boyd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Venigalla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Laforge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Purdy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Kamath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Maheshwari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Beidler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Rosseel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Ahmad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Gagarin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Czekalski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Werner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sproch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Macias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kurtz</surname></persName>
		</author>
		<idno type="DOI">10.1109/ISCA45697.2020.00023</idno>
	</analytic>
	<monogr>
		<title level="m">47th ACM/IEEE Annual International Symposium on Computer Architecture, ISCA 2020</title>
		<meeting><address><addrLine>Valencia, Spain</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020-06-03">May 30 -June 3, 2020. 2020</date>
			<biblScope unit="page" from="145" to="158" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
		<title level="m" type="main">PyMatching: A Python package for decoding quantum codes with minimum-weight perfect matching</title>
		<author>
			<persName><forename type="first">O</forename><surname>Higgott</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.13082</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note>arXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b69">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Edwards</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Xie</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.08965</idno>
		<title level="m">Graph Based Convolutional Neural Network</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Code deformation and lattice surgery are gauge fixing</title>
		<author>
			<persName><forename type="first">C</forename><surname>Vuillot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Criger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">G</forename><surname>Almud√©ver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Bertels</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">M</forename></persName>
		</author>
		<idno type="DOI">10.1088/1367-2630/ab0199</idno>
	</analytic>
	<monogr>
		<title level="j">New J. Phys</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page">33028</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
