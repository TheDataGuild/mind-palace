<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Machine-learning-assisted correction of correlated qubit errors in a topological code</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2017-12">December 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">P</forename><surname>Baireuther</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Instituut-Lorentz</orgName>
								<orgName type="institution">Universiteit Leiden</orgName>
								<address>
									<postBox>P.O. Box 9506</postBox>
									<postCode>2300 RA</postCode>
									<settlement>Leiden</settlement>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">T</forename><forename type="middle">E</forename><surname>O'brien</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Instituut-Lorentz</orgName>
								<orgName type="institution">Universiteit Leiden</orgName>
								<address>
									<postBox>P.O. Box 9506</postBox>
									<postCode>2300 RA</postCode>
									<settlement>Leiden</settlement>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">B</forename><surname>Tarasinski</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">QuTech</orgName>
								<orgName type="institution">Delft University of Technology</orgName>
								<address>
									<postBox>P.O. Box 5046</postBox>
									<postCode>2600 GA</postCode>
									<settlement>Delft</settlement>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">C</forename><forename type="middle">W J</forename><surname>Beenakker</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Instituut-Lorentz</orgName>
								<orgName type="institution">Universiteit Leiden</orgName>
								<address>
									<postBox>P.O. Box 9506</postBox>
									<postCode>2300 RA</postCode>
									<settlement>Leiden</settlement>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Machine-learning-assisted correction of correlated qubit errors in a topological code</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2017-12">December 2017</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:1705.07855v3[quant-ph]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-10-09T23:23+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>A fault-tolerant quantum computation requires an efficient means to detect and correct errors that accumulate in encoded quantum information. In the context of machine learning, neural networks are a promising new approach to quantum error correction. Here we show that a recurrent neural network can be trained, using only experimentally accessible data, to detect errors in a widely used topological code, the surface code, with a performance above that of the established minimumweight perfect matching (or "blossom") decoder. The performance gain is achieved because the neural network decoder can detect correlations between bit-flip (X) and phase-flip (Z) errors. The machine learning algorithm adapts to the physical system, hence no noise model is needed. The long short-term memory layers of the recurrent neural network maintain their performance over a large number of quantum error correction cycles, making it a practical decoder for forthcoming experimental realizations of the surface code.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>A quantum computer needs the help of a powerful classical computer to overcome the inherent fragility of entangled qubits. By encoding the quantum information in a nonlocal way, local errors can be detected and corrected without destroying the entanglement <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref>. Since the efficiency of the quantum error correction protocol can make the difference between failure and success of a quantum computation, there is a major push towards more and more efficient decoders <ref type="bibr" target="#b2">[3]</ref>. Topological codes such as the surface code, which store a logical qubit in the topology of an array of physical qubits, are particularly attractive because they combine a favorable performance on small circuits with scalability to larger circuits <ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref><ref type="bibr" target="#b5">[6]</ref><ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b8">[9]</ref>.</p><p>In a pioneering work <ref type="bibr" target="#b9">[10]</ref>, Torlai and Melko have shown that the data processing power of machine learning (artificial neural networks <ref type="bibr" target="#b10">[11]</ref><ref type="bibr" target="#b11">[12]</ref><ref type="bibr" target="#b12">[13]</ref>) can be harnessed to produce a flexible, adaptive decoding al-gorithm. A test on a topological code (Kitaev's toric code <ref type="bibr" target="#b13">[14]</ref>) revealed a performance for phase-flip errors that was comparable to decoders based on the minimum-weight perfect matching (MWPM or "blossom") algorithm of Edmonds <ref type="bibr" target="#b14">[15]</ref><ref type="bibr" target="#b15">[16]</ref><ref type="bibr" target="#b16">[17]</ref>. The machine learning paradigm promises a flexibility that the classic algorithms lack, both with respect to different types of topological codes and with respect to different types of errors.</p><p>Several groups are exploring the capabilities of a neural network decoder <ref type="bibr" target="#b17">[18]</ref><ref type="bibr" target="#b18">[19]</ref><ref type="bibr">[20]</ref>, but existing designs cannot yet be efficiently deployed as a decoder in a surface code architecture <ref type="bibr" target="#b19">[21]</ref><ref type="bibr" target="#b20">[22]</ref><ref type="bibr" target="#b21">[23]</ref>. Two key features which are essential for this purpose are 1: The neural network must have a "memory", in order to be able to process repeated cycles of stabilizer measurement whilst detecting correlations between cycles; and 2: The network must be able to learn from measured data, it should not be dependent on the uncertainties of theoretical modeling.</p><p>In this work we design a recurrent neural network decoder that has both these features, and demonstrate a performance improvement over a blossom decoder in a realistic simulation of a forthcoming error correction experiment. Our decoder achieves this improvement through its ability to detect bit-flip (X) and phase-flip (Z) errors separately as well as correlations (Y). The blossom decoder treats a Y-error as a pair of uncorrelated X and Z errors, which explains the improved performance of the neural network. We study the performance of the decoder in a simplified model where the Y-error rate can be adjusted independently of the X-and Z-error rates, and measure the decoder efficiency in a realistic model (density matrix simulation) of a state-of-the-art 17-qubit surface code experiment .</p><p>The outline of this paper is as follows. In the next section 2 we summarize the results from the literature we need on quantum error correction with the surface code. The design principles of the recurrent neural network that we will use are presented in Sec. 3, with particular attention for the need of an internal memory in an efficient decoder. This is one key aspect that differentiates our recurrent network from the feedforward networks proposed independently <ref type="bibr" target="#b17">[18]</ref><ref type="bibr" target="#b18">[19]</ref><ref type="bibr">[20]</ref> (see A further set of two-fold σx and σz measurements are performed on the boundary, bringing the total number of measurements to N -1. Right: Since direct four-fold parity measurements are impractical, the measurements are instead performed by entanglement with an ancilla qubit, followed by a measurement of the ancilla in the computational basis. Both data qubits and ancilla qubits accumulate errors during idle periods (labeled I) and during gate operations (Hadamard H and cnot), which must be accounted for by a decoder. The data qubits are also entangled with the rest of the surface code by the grayed out gates.</p><p>Sec. 4). A detailed description of the architecture and training protocol is given in Sec. 5. In Sec. 6 we compare the performance of the neural network decoder to the blossom decoder for a particular circuit model with varying error rates. We conclude in Sec. 7 with a demonstration of the potential of machine learning for real-world quantum error correction, by decoding data from a realistic quantum simulation of the Surface-17 experiment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Overview of the surface code</head><p>To make this paper self-contained we first describe the operation of the surface code and formulate the decoding problem. The expert reader may skip directly to the next section.</p><p>In a quantum error correcting (QEC) code, single logical qubits (containing the quantum information to be protected) are spread across a larger array of N noisy physical data qubits <ref type="bibr" target="#b22">[24,</ref><ref type="bibr" target="#b23">25]</ref>. The encoding is achieved by N -1 binary parity check measurements on the data qubits <ref type="bibr" target="#b24">[26]</ref>. Before these measurements, the state of the physical system is described by a complex vector |ψ within a 2 N -dimensional Hilbert space H. Each parity check measurement M i projects |ψ onto one of two 2 N -1 -dimensional subspaces, dependent on the outcome s i of the measurement. As all parity check measurements commute, the result of a single cycle of N -1 measurements is to project |ψ into the intersection of all subspaces H s decided by the measurements s = s 1 , . . . , s N -1 (s i ∈ {0, 1}). This is a Hilbert space of dimension 2 N /2 N -1 = 2, giving the required logical qubit |ψ L .</p><p>Repeated parity check measurements s(t) do not affect the qubit within this space, nor entanglement between the logical qubit states and other systems. However, errors in the system will cause the qubit to drift out of the logical subspace. This continuous drift is discretized by the projective measurement, becoming a series of discrete jumps between subspaces H s(t) as time t progresses. Since s(t) is directly measured, the qubit may be corrected, i.e. brought back to the initial logical subspace H s(0) . When performing this correction, a decision must be made on whether to map the logical state |0</p><formula xml:id="formula_0">s(t) L ∈ H s(t) to |0 s(0) L or |1 s(0) L ∈ H s(0)</formula><p>, as no a priori relationship exists between the labels in these two spaces. If this is done incorrectly, the net action of the time evolution and correction is a logical bit-flip error. A similar choice must be made for the {|+ L , |-L } logical states, which if incorrect results in a logical phase-flip error.</p><p>Information about the best choice of correction (to most-likely prevent logical bit-flip or phase-flip errors) is stored within the measurement vectors s, which detail the path the system took in state-space from H s(0) to H s(t) . The non-trivial task of decoding, or extracting this correction, is performed by a classical decoder. Optimal (maximum-likelihood) decoding is an NP-hard problem <ref type="bibr" target="#b25">[27]</ref>, except in the presence of specific error models <ref type="bibr" target="#b26">[28]</ref>. However, a faulttolerant decoder need not be optimal, and polynomial time decoders exist with sufficient performance to demonstrate error mitigation on current quantum hardware <ref type="bibr" target="#b4">[5]</ref>. This sub-optimality is quantified by the decoder efficiency <ref type="bibr" target="#b27">[29]</ref> </p><formula xml:id="formula_1">η d = (opt) L / D L ,<label>(1)</label></formula><p>where D L is the probability of a logical error per cycle using the decoder D, and (opt) L is the probability of a logical error per cycle using the optimal decoder <ref type="bibr" target="#b29">[31]</ref>.</p><p>The QEC code currently holding the record for the best performance under a scalable decoder is the surface code <ref type="bibr" target="#b2">[3]</ref><ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref><ref type="bibr" target="#b15">16]</ref>. As illustrated in Fig. <ref type="figure" target="#fig_0">1</ref>, the surface code is defined on a d × d lattice of data qubits, where d = √ N is the distance of the code. The measurement operators are defined by coloring lattice squares as on a checkerboard. Each square corresponds to a correlated measurement of the stabilizer operator</p><formula xml:id="formula_2">S α = σ a α ⊗ σ b α ⊗ σ c α ⊗ σ d α ,<label>(2)</label></formula><p>with α = z on the green squares and α = x on the blue squares. The operator σ D α is the Pauli matrix acting on the qubit in the D-corner of the square (labeled a,b,c,d in Fig <ref type="figure" target="#fig_0">1</ref>). The checkerboard is extended slightly beyond the boundary of the lattice <ref type="bibr" target="#b30">[32]</ref>, giving an additional set of two-qubit σ D α σ D α measurements, and bringing the total number of measurements to (d -1) 2 + 2(d -1) = N -1, as it should be.</p><p>All measurements commute because green and blue squares either share two corners or none. A bit-flip or phase-flip on any data qubit in the bulk of the code causes two measurements to change sign, producing unit syndrome increments</p><formula xml:id="formula_3">δs i (t) ≡ s i (t) -s i (t -1) mod 2. (<label>3</label></formula><formula xml:id="formula_4">)</formula><p>This theme is continued even when the measurement of s i itself is allowed to be faulty; such measurement errors cause two correlated error signals δs i (t) = 1 separated in time, rather than in space. As all observable errors can be built from combinations of bit-flip and phase-flip errors, these measurements allow the mapping of surface-code decoding to the minimum-weight perfect matching (MWPM) problem <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b15">16]</ref>. Every instance of non-zero δs i (t) is mapped to a vertex in a graph, with an edge between two vertices representing the probability of some combination of errors causing these signals. A 'boundary' vertex is included to account for qubits on the edge of the lattice, whose errors may only cause a single error signal. Then, the most probable matching of vertices, weighted by the product of probabilities on individual edges, gives the required error correction. This matching can be found in polynomial time with Edmonds' blossom algorithm <ref type="bibr" target="#b14">[15]</ref>.</p><p>Under current experimental parameters, with the smallest non-trivial N (N = 9, or distance d = √ N = 3), this blossom decoder already crosses the quantum memory threshold -whereby quantum information on a logical qubit can be stored for a longer time than on any physical component. However, the decoder itself performs only with efficiency η d = 0.64, leaving much room for improvement <ref type="bibr" target="#b27">[29]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Neural network detection of correlated errors</head><p>The sub-optimality of the blossom decoder comes primarily from its inability to optimally detect Pauli-Y (σ y ) errors <ref type="bibr" target="#b27">[29,</ref><ref type="bibr" target="#b29">31,</ref><ref type="bibr" target="#b31">33]</ref>. These errors correspond to a combination of a bit-flip (X) and a phase-flip (Z) on the same qubit, and are thus treated by a MWPM decoder as two independent errors. Since these correlations exist as patterns on the graph, one may expect that the pattern matching capabilities of a neural network could be exploited to identify the correlations, producing an improvement over existing decoders. This is the primary motivation of the research we report in what follows.</p><p>A key issue in the design of any practical decoder is to ensure that the decoder is able to operate for an unspecified number of cycles T . A feedforward neural network is trained on a dataset with a specific fixed T . The central advance of this work is to use a recurrent neural network to efficiently decode an arbitrary, unspecified number of cycles. In order to learn time correlations the network possesses an internal memory that it utilizes to store information about previous cycles. This is important because errors on the ancilla qubits or during ancilla qubit readout lead to error signals that are correlated over several cycles.</p><p>We adopt the recurrent neural network architecture known as a "long short-term memory" (LSTM) layer <ref type="bibr" target="#b32">[34,</ref><ref type="bibr" target="#b33">35]</ref>. (See App. A for details of our network.) These layers have two internal states: a shortterm memory h t , and a long-term memory c t that is updated each cycle and retains information over several cycles. During training, the parameters that characterize the LSTM layers are updated using back propagation, in order to efficiently update and utilize the long-term memory to detect logical errors, even if the corresponding syndrome patterns are non-local in time. The parameters of the LSTM layers themselves are the same for each cycle; only their memory changes. This allows for a very efficient algorithm, whose computational cost per cycle is independent of how many cycles the network has to decode.</p><p>We now formulate the QEC problem that a decoder needs to solve. To be useful for upcoming QEC experiments and future fault-tolerant quantum algorithms, it is critical that any decoder uses data that could be generated by such experiments. This implies that the data available to the neural network, both for input and labels, must be data generated by qubit measurements (as opposed to a listing of occurred errors, which is not available in an actual experiment).</p><p>The data available to the decoder after T cycles are the T syndromes s(t), and a final syndrome f calculated from readout of the final data qubits. From this, a decoder must output a single bit of data, the so-called "final parity correction" that decides on the correction of the final logical state. The decoder may be trained and tested using the scheme described in Ref. <ref type="bibr" target="#b27">[29]</ref>. The system is prepared in a known logical state, chosen from |0 L and |1 L or from |+ L and |-L , which is held for T cycles and then readout. The final logical state can be determined by the parity of all data qubit measurements, to which the final parity correction may be directly added. This gives a standard binary classification problem for the neural network. Since it is a priori unknown in which basis the logical qubit will be measured, we need to train two separate decoders -one for the x-basis and one for the z-basis.</p><p>4 Related Work</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Approaches going beyond blossom decoding</head><p>The neural network decoder improves on the blossom decoder by including correlations between Pauli-X and Pauli-Z errors. It is possible to account for these correlations without using machine learning, by adapting the minimum-weight perfect matching (blossom) algorithm.</p><p>Fowler <ref type="bibr" target="#b31">[33]</ref> and Delfosse and Tillich <ref type="bibr" target="#b34">[36]</ref> achieved this by performing repeated rounds of X-error and Z-error decoding in series. After each round of Xerror decoding, the weights on the Z-graph are updated based on the likelihood of underlying Z-errors assuming the X-matching is correct. The overhead from repeated serial repetitions of the blossom algorithm is limited by restriction to a small window of decoding for each repetition, resulting in a constanttime algorithm.</p><p>We can compare the results obtained in Ref. <ref type="bibr" target="#b31">[33]</ref> to our results by extracting the improvement of correlated over basic fault-tolerant corrections for a distance-3 code. For a depolarization probability comparable to the one we use in Fig. <ref type="figure">3</ref> the improvement is approximately 24%. This is similar to the improvement we obtained with the neural network decoder.</p><p>Both the neural network decoder and the improved blossom decoder perform below the optimal maximum-likelihood decoder. Several approaches exist to reach the optimal limit, we mention the incorporation of X-Z correlations via a belief propagation algorithm <ref type="bibr" target="#b35">[37]</ref>, and approaches based on renormalization group methods or Monte Carlo methods <ref type="bibr" target="#b26">[28,</ref><ref type="bibr" target="#b36">38,</ref><ref type="bibr" target="#b37">39]</ref>.</p><p>Bravyi, Suchara, and Vargo <ref type="bibr" target="#b26">[28]</ref> reported a densitymatrix renormalization group (DMRG) method for exact single-round maximum-likelihood decoding in polynomial time, assuming bit-flip and dephasing noise. Their performance continues to be better than the blossom decoder for multi-round decoding. The method is somewhat limited in the choice of error model; in particular it cannot account for Y-errors.</p><p>The Markov-chain Monte Carlo method of Hutter, Wootton, and Loss <ref type="bibr" target="#b37">[39]</ref> samples over the set of corrections to approximate the maximum-likelihood decoding via the Metropolis algorithm. This again outperforms the blossom decoder, but it suffers from an increased run-time cost, with an additional O(N 2 ) computational cost.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Approaches based on machine learning</head><p>The existence of algorithms <ref type="bibr" target="#b26">[28,</ref><ref type="bibr" target="#b31">33,</ref><ref type="bibr" target="#b34">[36]</ref><ref type="bibr" target="#b35">[37]</ref><ref type="bibr" target="#b36">[38]</ref><ref type="bibr" target="#b37">[39]</ref>] that improve on the blossom decoder does not diminish the appeal of machine learning decoders, since these offer a flexibility to different types of topological codes that a dedicated decoder lacks.</p><p>Torlai and Melko <ref type="bibr" target="#b9">[10]</ref> implemented a machine learning decoder based on a restricted Boltzmann machine, while Varsamopoulos, Criger, and Bertels <ref type="bibr" target="#b17">[18]</ref> and Krastanov and Jiang <ref type="bibr" target="#b18">[19]</ref> used feedforward neural networks. The key distinction with our work is that we use a recurrent neural network, and thereby allow the decoder to detect correlations between arbitrary cycles of stabilizer measurements.</p><p>Refs. <ref type="bibr" target="#b9">[10]</ref> and <ref type="bibr" target="#b18">[19]</ref> were limited to the study of models without circuit-level noise (i.e. without measurement error between repeated cycles), and so no direct quantitative comparison with the performance of our decoder is possible.</p><p>One feedforward neural network in Ref. <ref type="bibr" target="#b17">[18]</ref> was constructed to take the syndrome from 3 cycles as input. While it cannot decode an arbitrary number of cycles, it can account for circuit noise at the 3cycle level. Over that time frame their performance lies within error bars from that of our recurrent neural network. (The equivalence of the Pauli-frame-update error rate of Ref. <ref type="bibr" target="#b17">[18]</ref> and our parity-bit error rate is discussed in App. B.)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Design of the neural network decoder</head><p>The neural network consists of two LSTM layers with internal state sizes N = 64. The LSTM layers receive as input sets of syndrome increments δ s(t) from both the x-stabilizer and the z-stabilizer measurements.</p><p>When a final parity prediction is required from the network at time T , information from the recurrent network is passed to an evaluation layer, along with the syndrome increment</p><formula xml:id="formula_5">δ f (T ) = f -s(T ) mod 2 (4)</formula><p>between final syndrome f calculated from the data qubit measurements and the last syndrome readout s(T ) from the ancilla qubits. Note that, while s(t) is passed to each decoder in both the x-basis and the z-basis, the final syndrome f is only available to a decoder in its own basis. The memory of the recurrent network solves the issue of how to concatenate multiple decoding cycles, but one remaining issue occurs at the end of the computation: the final syndrome breaks timetranslational invariance. Within any cycle, the decoder must account for the possibility that an error signal (δs i (t) = 1) should be propagated forward in time to future cycles. This is not the case for the final syndrome, as this is calculated directly from the data qubit measurements, and any errors in the data qubits do not propagate forward in time.</p><p>To achieve time-translational invariance of the decoder we split the problem into two separate tasks, as shown in Fig. <ref type="figure" target="#fig_2">2</ref>. Task 1 is to estimate the probability p 1 that the parity of bit-flip errors during T cycles is odd, based solely on the syndrome increments δ s(t) up to that point (i.e. those extracted from ancilla measurements). Task 2 is to estimate the probability p 2 that the final data qubit measurements make any adjustment to the final parity measurement, based solely on new information from the final syndrome increment δ f (T ). The final parity probability is then given by the probabilistic sum</p><formula xml:id="formula_6">p = p 1 (1 -p 2 ) + p 2 (1 -p 1 ). (<label>5</label></formula><formula xml:id="formula_7">)</formula><p>We use two separate networks for the two tasks. The first network gets T rounds of syndrome increments δ s(t) but not the final syndrome increment (upper half of Fig. <ref type="figure" target="#fig_2">2</ref>). The second network gets only the last T 0 syndrome increments δ s(t), but its evaluation layer gets the last output of the second LSTM layer concatenated with the final syndrome increment (lower half of Fig. <ref type="figure" target="#fig_2">2</ref>). For Surface-17, we observe optimal performance when we allow the task-2 network a window of T 0 = 3 cycles, giving a decoder that works for experiments of three or more cycles. In general, the number of cycles fed to the second network should be on the order of the length of the longest time-correlations between syndromes. As task 2 only requires decoding of a fixed number of cycles, it could potentially be performed by a simpler feedforward network, but we found it convenient to keep the same architecture as task 1 because of the similarity between the two tasks.</p><p>We discuss the details of the network architecture and training procedure in App. A. The source code is available <ref type="bibr" target="#b39">[41]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Neural network performance</head><p>We determine the neural network performance on the 17-qubit distance-3 surface code, referred to as "Surface-17", which is under experimental development <ref type="bibr" target="#b21">[23]</ref>.</p><p>We take at first a simplified Pauli error channel model <ref type="bibr" target="#b40">[42]</ref>, similar to Refs. <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7]</ref> but without correlated two-qubit errors. In this model the performance of the blossom decoder is understood and individual error types can be focused upon. Stabilizer measurements are made by entangling two or four data qubits with an ancilla qubit, which is readout in the computational basis (right panel in Fig. <ref type="figure" target="#fig_0">1</ref>).</p><p>The process is broken into seven steps: four coherent steps over which cnot gates are performed, two steps in which Hadamard gates are performed, and one measurement step. During idle, Hadamard, and cnot steps, both data and ancilla qubits have independent chances of a σ x error (with probability p x ), Figure <ref type="figure">3</ref>: Comparison of logical qubit decay between blossom and neural network decoders for a Pauli error channel model, with px = py = pz = 0.048% and pm = 0.14%. We plot the probability that the decoder corrects the logical qubit after t cycles of stabilizer measurement and error accumulation. All data is averaged over 5 • 10 4 datasets, with error bars obtained by boot-strapping (using 3σ for the error). Lines are two-parameter fits of the data to Eq. (8).</p><p>a σ y error (with probability p y ), and a σ z error (with probability p z ). This implies that the total probability during any step for a qubit to accumulate a y-error (as opposed to an x-error, a z-error, or no error) is</p><formula xml:id="formula_8">y-error prob. = p y (1 -p x )(1 -p z ) + p x p z (1 -p y ). (6)</formula><p>With this definition p y = 0 implies that x-errors and z-errors are uncorrelated (it does not imply that there are no y-errors).</p><p>Data qubits behave similarly during measurement steps, but ancilla qubits are projected into the computational basis and so cannot incur phase errors. Instead, a measurement has a p m chance of returning the wrong result, without the qubit state being affected. Qubits are reused after measurement without reset, and so the syndromes s i (t) are obtained by changes in the readout m i (t) of an ancilla qubit between rounds,</p><formula xml:id="formula_9">s i (t) = m i (t) -m i (t -1) mod 2. (<label>7</label></formula><formula xml:id="formula_10">)</formula><p>The performance of the logical qubit is measured using the protocol outlined in Ref. <ref type="bibr" target="#b27">[29]</ref> (Methods section). The logical qubit is prepared in the |0 state, held for T cycles, and finally measured and decoded. The decoder seeks to determine whether or not the qubit underwent a logical bit-flip during this time. The probability that the decoder obtains the correct answer gives the logical qubit fidelity, which can be plotted as a function of the number of cycles. Fig. <ref type="figure">3</ref> shows the decay in fidelity over 300 cycles for p x = p y = p z = 0.048% and p m = 0.14%, which corresponds to a physical error rate of approximately 1% per cycle.</p><p>A logical error rate per cycle can be obtained from these figures by a two-parameter fit to the logical fi-Figure <ref type="figure">4</ref>: Comparison of the error rates of a logical qubit decoded by a neural network and a blossom decoder, for different values of the correlated error rate py. As py increases, at fixed px = pz = 0.048% and pm = 0.14%, the blossom decoder (blue) produces a larger error rate than the neural network decoder (red). Data points are obtained by fitting decay curves, as in Fig. <ref type="figure">3</ref>.</p><formula xml:id="formula_11">delity F(t) = 1 2 + 1 2 (1 -2 ) t-t0 , (<label>8</label></formula><formula xml:id="formula_12">)</formula><p>where t 0 is a constant offset to account for the 'majority vote' behavior of the error correcting circuit at low cycle number <ref type="bibr" target="#b27">[29]</ref>, and any additional sample preparation and measurement error. We find = 0.209% for the neural network decoder, a substantial improvement over the value = 0.274% for the blossom decoder <ref type="bibr" target="#b28">[30]</ref>.</p><p>To demonstrate that the performance improvement is due to the capability of the neural network to detect error correlations, we show in Fig. <ref type="figure">4</ref> how the performance varies with varying p y (at fixed p x = p z = 0.048% and p m = 0.14%). When p y = 0, the σ x and σ z errors are independent and the blossom decoder performs near-optimally <ref type="bibr" target="#b27">[29,</ref><ref type="bibr" target="#b29">31]</ref>. The neural network decoder then gives no improvement, but once p y ∼ p x the performance gain is evident.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion and outlook</head><p>In conclusion, we have designed and tested a recurrent neural network decoder that outperforms the standard minimum-weight perfect matching (MWPM, or "blossom") decoder in the presence of correlated bitflip and phase-flip errors. The building block of the network, a long short-term memory layer, allows the decoder to operate over the full duration of a quantum algorithm with multiple cycles. A key feature of our design, which sets it apart from independent proposals <ref type="bibr" target="#b17">[18]</ref><ref type="bibr" target="#b18">[19]</ref><ref type="bibr">[20]</ref>, is that the network can be trained solely on experimental data, without requiring a priori assumptions from theoretical modeling.</p><p>We believe that our neural network decoder provides a realistic option for utilization in forthcoming experimental QEC implementations <ref type="bibr" target="#b21">[23]</ref>. In support  <ref type="figure">3</ref>, but now for a density matrix simulation of an implementation of Surface-17 using superconducting transmon qubits <ref type="bibr" target="#b27">[29]</ref>.</p><p>of this, we have tested the performance in a real-world setting by using a density matrix simulator to model Surface-17 with state-of-the-art experimental parameters for superconducting transmon qubits <ref type="bibr" target="#b27">[29]</ref>. In Fig. <ref type="figure" target="#fig_3">5</ref> we show the decay of the fidelity over 100 cycles for the neural network and blossom decoders, as well as an upper bound on the optimal fidelity. (The latter is extracted directly from the simulation data.) The decoder efficiency (1) of the neural network is η d = 0.81, a 26% improvement over the blossom decoder. This improvement was achieved after training on 4 • 10 6 datasets, which require roughly 60 s to generate on experimental hardware <ref type="bibr" target="#b21">[23]</ref>, making this approach immediately experimentally viable.</p><p>We mention three directions for future research. The first is the extension to other topological codes than the surface code, such as the color code. The neural network itself is agnostic to the type of topological code used, so this extension should be feasible without modifications of the design. Secondly, for low error rates it will be challenging to train a neural network decoder, because then the training dataset is unlikely to contain a sufficient representation of twoqubit errors. This can potentially be overcome by training on data with a higher error rate, but it remains to be seen whether a decoder trained this way will outperform MWPM decoding. Finally, the decoder needs to be scaled-up to surface codes that are deformed by lattice surgery <ref type="bibr" target="#b41">[43]</ref> or braiding <ref type="bibr" target="#b3">[4]</ref> for the execution of logical gates. For this extension the design of the decoder should be modified so that it is not tied to a single code distance.</p><p>neural networks by preventing co-adaptation of feature detectors, arXiv:1207.0580.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Details of the neural network decoder A.1 Architecture</head><p>The decoder is composed of two networks. The first network maps a list of syndrome increments δ s(t) with t = 1, 2, ..., T to a probability p 1 ∈ [0, 1]. The second network maps a list with the last few syndrome increments t = T -T 0 + 1, T -T 0 + 2, ..., T , together with a single final syndrome increment δ f (T ) to a probability p 2 ∈ [0, 1]. The probabilistic sum p = p 1 (1 -p 2 ) + p 2 (1 -p 1 ) of these two outputs is the probability that the logical qubit has incurred a bit-flip error. The cost function we try to minimize is the cross-entropy between this probability and the true final parity of bit-flip errors (labels) plus a small weight regularization term.</p><p>We note that p is invariant under the transformation p 1 → 1 -p 1 and p 2 → 1 -p 2 . This ambiguity in the individual error probabilities is irrelevant for the joint operation of the networks. Moreover, it may be easily removed by testing the trained networks separately on a trivial example where all syndromes are zero and both probabilities should be &lt; 1/2.</p><p>Both networks consist of two LSTM layers with internal states c i t , h i t ∈ R 64 and a fully connected evaluation layer with 64 rectified linear units. The inputs of the first layer are the syndrome increments. The inputs of the second layer are the outputs of the first layer h 1 t . For the first network, the input of the evaluation layer is the final output of the second LSTM layer, subject to a rectified linear activation function ReL( h 2 T ). For the second network, the input of the evaluation layer is ReL( h 2 T ) concatenated with the final syndrome increment δ f (T ).</p><p>The source code including all the network parameters is available <ref type="bibr" target="#b39">[41]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Training and evaluation</head><p>The two networks are trained simultaneously on minibatches of size 64 from a training dataset containing 4 • 10 6 sequences of lengths between T = 11 and T = 20 cycles. At the end of each sequence, the training set contains the final syndrome increment and the final parity of bit-flip errors. One epoch consists of 10 4 mini-batches. The optimization is done using the Adam optimizer <ref type="bibr" target="#b42">[44]</ref> with a learning rate of 10 -3 . For regularization we apply dropout <ref type="bibr" target="#b43">[45]</ref> with a keep probability of 0.8 after each LSTM layer and after the evaluation layer. In addition, we apply weight decay with a prefactor of 10 -5 to the evaluation layer. After each epoch, the decoder is evaluated on a validation dataset, which consists of 10 4 sequences of lengths between T = 81 and T = 100 cycles. If the logical error rate on the validation dataset reaches a new minimum, the network is stored. The training continues until the logical error rate on the validation dataset has not improved for 100 epochs. We train three decoders and choose the instance that has the lowest logical error rate on the validation dataset.</p><p>To evaluate the chosen decoder, we use yet another dataset. This test dataset consists of 5 • 10 4 sequences of length T = 300 for the Pauli error channel model and T = 100 for the density matrix simulation. In contrast to the training and validation datasets, the test dataset contains a final syndrome increment and a final parity of bit-flip errors after each cycle. This cannot be achieved in a real experiment, but is extracted from the simulation to keep the calculation time manageable. We evaluate the decoder on the test dataset for t n = 2 + n n =1 n ≤ T cycles, chosen such that the resolution is high at small cycle numbers and lower at large cycle numbers. If the decoders output is p &lt; 0.5, the final parity of bit-flip errors is predicted to be even and otherwise odd. We then compare this to the true final parity and average over the test dataset to obtain the logical fidelity. Using a two-parameter fit to Eq. ( <ref type="formula" target="#formula_11">8</ref>) we obtain the logical error rate per cycle.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Parity-bit error versus Pauli-frameupdate error</head><p>Ref. <ref type="bibr" target="#b17">[18]</ref> described the error rate of the decoder in terms of its ability to apply the correct Pauli frame update. The error rate from Eq. (8) describes the correctness of the parity bit produced by the decoder, without explicitly referring to a Pauli frame update. Here we show that the two error rates are in fact the same.</p><p>We recall that a Pauli frame is a list of Pauli X, Y, or Z errors that have occurred to data qubits <ref type="bibr" target="#b1">[2]</ref>. Two Pauli frames are equivalent if they are separated by stabilizer measurements, since these act as the identity on the error-free subspace.</p><p>We begin by choosing logical operators X L and Z L in terms of Pauli operators on the physical qubits. The choice is not unique because of a gauge freedom: SX L = X L on the logical subspace for any stabilizer operator S.</p><p>Consider a syndrome s(t) that contains only a single non-zero stabilizer measurement s i (t), corresponding to a stabilizer operator S i . There exist multiple Pauli frames P i that correct S i and which commute with our chosen logical operators. Ref. <ref type="bibr" target="#b17">[18]</ref> considers a 'simple' decoder, which arbitrarily chooses one P i for each S i . Then, given a syndrome s(t) at time t with many non-zero s i , it generates a Pauli frame as P simple = i,si(t)=1 P i .</p><p>The simple decoder is coupled to a neural network decoder, which outputs a parity bit p that determines whether or not to multiply P simple by X L (if the neural network is calculating Z-parity) or Z L (if the neural network is calculating X-parity). We denote the resulting Pauli frame update by P calc . If it differs from the true Pauli frame update P true the decoder has made an error, and the rate at which this happens is the Pauli frame update error rate P .</p><p>To see that this P is equivalent to the parity-bit error rate , we consider for the sake of definiteness a neural network that calculates the Z-parity. The two Pauli frames P calc and P true differ by X L when [P true , Z L ] = [P calc , Z L ]. But [P true , Z L ] is the parity readout of the data qubits, and [P calc , Z L ] is precisely our prediction. Alternatively, note that the simple decoder is constructed to fix [P simple , Z L ] = 0, and the choice to multiply this by X L precisely fixes [P calc , Z L ] = p.</p><p>We finally note that in a physical experiment the Pauli frame P true is undetermined unless the data qubits themselves are measured in the Z or X basis, and the gauge freedom is fixed at random by this measurement. The parity bit p is therefore not only more convenient for a neural network to output than a Pauli frame update, but also more appropriate, as this way the neural network does not spend time trying to predict the outcome of quantum randomness.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Schematic of the surface code. Left: N physical data qubits are arranged on a d × d square lattice (where d = √ N is known as the distance of the code). For each square one makes the four-fold σx or σz correlated measurement of Eq. (2).A further set of two-fold σx and σz measurements are performed on the boundary, bringing the total number of measurements to N -1. Right: Since direct four-fold parity measurements are impractical, the measurements are instead performed by entanglement with an ancilla qubit, followed by a measurement of the ancilla in the computational basis. Both data qubits and ancilla qubits accumulate errors during idle periods (labeled I) and during gate operations (Hadamard H and cnot), which must be accounted for by a decoder. The data qubits are also entangled with the rest of the surface code by the grayed out gates.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>L</head><label></label><figDesc>, and a fully connected evaluation layer with N (E) L neurons. We implement the decoder using the TensorFlow library<ref type="bibr" target="#b38">[40]</ref>, taking N</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure2: Architecture of the recurrent neural network decoder, consisting of two neural networks. The upper half is network 1 and the lower half is network 2. Ovals denote the long short-term memory (LSTM) layers and fully connected evaluation layers, while boxes denote input and output data. Solid arrows denote data flow in the system, and dashed arrows denote the internal memory flow of the LSTM layers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Figure5: Same as Fig.3, but now for a density matrix simulation of an implementation of Surface-17 using superconducting transmon qubits<ref type="bibr" target="#b27">[29]</ref>.</figDesc></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>Accepted in Quantum 2018-01-16, click title to verify</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We have benefited from discussions with B. Criger, L. DiCarlo, A. G. Fowler, V. Ostroukh, and B. Terhal. This research is supported by the Netherlands Organization for Scientific Research (NWO/OCW), an ERC Synergy Grant, and by the Office of the Di-rector of National Intelligence (ODNI), Intelligence Advanced Research Projects Activity (IARPA), via the U.S. Army Research Office grant W911NF-16-1-0071. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements, either expressed or implied, of the ODNI, IARPA, or the U.S. Government. The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes notwithstanding any copyright annotation thereon.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Lidar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">A</forename><surname>Brun</surname></persName>
		</author>
		<title level="m">Quantum error correction</title>
		<imprint>
			<publisher>Cambridge University Press</publisher>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Quantum error correction for quantum memories</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">M</forename></persName>
		</author>
		<idno type="DOI">10.1103/RevModPhys.87.307</idno>
	</analytic>
	<monogr>
		<title level="j">Rev. Mod. Phys</title>
		<imprint>
			<biblScope unit="volume">87</biblScope>
			<biblScope unit="page">307</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Hollenberg, Towards practical classical processing for the surface code</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Fowler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Whiteside</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">C L</forename></persName>
		</author>
		<idno type="DOI">10.1103/PhysRevLett.108.180501</idno>
	</analytic>
	<monogr>
		<title level="j">Phys. Rev. Lett</title>
		<imprint>
			<biblScope unit="volume">108</biblScope>
			<biblScope unit="page">180501</biblScope>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">B</forename><surname>Bravyi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><surname>Kitaev</surname></persName>
		</author>
		<idno type="arXiv">arXiv:quant-ph/9811052</idno>
		<title level="m">Quantum codes on a lattice with boundary</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Surface code quantum computing with error rates over 1%</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Fowler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">C L</forename><surname>Hollenberg</surname></persName>
		</author>
		<idno type="DOI">10.1103/PhysRevA.83.020302</idno>
	</analytic>
	<monogr>
		<title level="j">Phys. Rev. A</title>
		<imprint>
			<biblScope unit="volume">83</biblScope>
			<biblScope unit="page">20302</biblScope>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Surface codes: Towards practical large-scale quantum computation</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Fowler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mariantoni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Martinis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">N</forename><surname>Cleland</surname></persName>
		</author>
		<idno type="DOI">10.1103/PhysRevA.86.032324</idno>
	</analytic>
	<monogr>
		<title level="j">Phys. Rev. A</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="page">32324</biblScope>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Low-distance surface codes under realistic quantum noise</title>
		<author>
			<persName><forename type="first">Yu</forename><surname>Tomita</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">M</forename><surname>Svore</surname></persName>
		</author>
		<idno type="DOI">10.1103/PhysRevA.90.062320</idno>
	</analytic>
	<monogr>
		<title level="j">Phys. Rev. A</title>
		<imprint>
			<biblScope unit="volume">90</biblScope>
			<biblScope unit="page">62320</biblScope>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Proposal for a minimal surface code experiment</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Wootton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Winkler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Loss</surname></persName>
		</author>
		<idno type="DOI">10.1103/PhysRevA.96.032338</idno>
	</analytic>
	<monogr>
		<title level="j">Phys. Rev. A</title>
		<imprint>
			<biblScope unit="volume">96</biblScope>
			<biblScope unit="page">32338</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">H</forename><surname>Nickerson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.01753</idno>
		<title level="m">Error correcting power of small topological codes</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Neural decoder for topological codes</title>
		<author>
			<persName><forename type="first">G</forename><surname>Torlai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">G</forename><surname>Melko</surname></persName>
		</author>
		<idno type="DOI">10.1103/PhysRevLett.119.030501</idno>
	</analytic>
	<monogr>
		<title level="j">Phys. Rev. Lett</title>
		<imprint>
			<biblScope unit="volume">119</biblScope>
			<biblScope unit="page">30501</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">R</forename><surname>Rojas</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-642-61068-4</idno>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<date type="published" when="1996">1996</date>
			<publisher>Springer</publisher>
			<pubPlace>Berlin; Heidelberg</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning deep architectures for AI</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="DOI">10.1561/2200000006</idno>
	</analytic>
	<monogr>
		<title level="j">Foundations and Trends in Machine Learning</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Shalev-Shwartz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ben-David</surname></persName>
		</author>
		<title level="m">Understanding machine learning: From theory to algorithms</title>
		<imprint>
			<publisher>Cambridge University Press</publisher>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Fault-tolerant quantum computation by anyons</title>
		<author>
			<persName><forename type="first">A</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><surname>Kitaev</surname></persName>
		</author>
		<idno type="DOI">10.1016/S0003-4916(02)00018-0</idno>
	</analytic>
	<monogr>
		<title level="j">Physics</title>
		<imprint>
			<biblScope unit="volume">303</biblScope>
			<biblScope unit="page">2</biblScope>
			<date type="published" when="2003">2003</date>
			<pubPlace>Ann</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Paths, trees, and flowers</title>
		<author>
			<persName><forename type="first">J</forename><surname>Edmonds</surname></persName>
		</author>
		<idno type="DOI">10.4153/CJM-1965-045-4</idno>
	</analytic>
	<monogr>
		<title level="j">Canad. J. Math</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page">449</biblScope>
			<date type="published" when="1965">1965</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Topological quantum memory</title>
		<author>
			<persName><forename type="first">E</forename><surname>Dennis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kitaev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Landahl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Preskill</surname></persName>
		</author>
		<idno type="DOI">10.1063/1.1499754</idno>
	</analytic>
	<monogr>
		<title level="j">J. Math. Phys</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="page">4452</biblScope>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Minimum weight perfect matching of fault-tolerant topological quantum error correction in average O(1) parallel time</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Fowler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Quantum Inf. Comput</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page">145</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Decoding small surface codes with feedforward neural networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Varsamopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Criger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Bertels</surname></persName>
		</author>
		<idno type="DOI">10.1088/2058-9565/aa955a</idno>
	</analytic>
	<monogr>
		<title level="j">Quantum Sci. Technol</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">15004</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deep neural network probabilistic decoder for stabilizer codes</title>
		<author>
			<persName><forename type="first">S</forename><surname>Krastanov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Jiang</surname></persName>
		</author>
		<idno type="DOI">10.1038/s41598-017-11266-1</idno>
	</analytic>
	<monogr>
		<title level="j">Sci. Rep</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">11003</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">State preservation by repetitive error detection in a superconducting quantum circuit</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kelly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Barends</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Fowler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Megrant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Jeffrey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">C</forename><surname>White</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Sank</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Y</forename><surname>Mutus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Campbell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Chiaro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Dunsworth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I.-C</forename><surname>Hoi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Neill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J J</forename><surname>O'malley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Quintana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Roushan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vainsencher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wenner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">N</forename><surname>Cleland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Martinis</surname></persName>
		</author>
		<idno type="DOI">10.1038/nature14270</idno>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">519</biblScope>
			<biblScope unit="page">66</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Gambetta, Demonstration of weight-four parity measurements in the surface code architecture</title>
		<author>
			<persName><forename type="first">M</forename><surname>Takita</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">D</forename><surname>Córcoles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Magesan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Abdo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Brink</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Cross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Chow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename></persName>
		</author>
		<idno type="DOI">10.1103/PhysRevLett.117.210505</idno>
	</analytic>
	<monogr>
		<title level="j">Phys. Rev. Lett</title>
		<imprint>
			<biblScope unit="volume">117</biblScope>
			<biblScope unit="page">210505</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Scalable quantum circuit and control for a superconducting surface code</title>
		<author>
			<persName><forename type="first">R</forename><surname>Versluis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Poletto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Khammassi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Tarasinski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Haider</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Michalak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bruno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Bertels</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Dicarlo</surname></persName>
		</author>
		<idno type="DOI">10.1103/PhysRevApplied.8.034021</idno>
	</analytic>
	<monogr>
		<title level="j">Phys. Rev. Applied</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">34021</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Scheme for reducing decoherence in quantum computer memory</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">W</forename><surname>Shor</surname></persName>
		</author>
		<idno type="DOI">10.1103/PhysRevA.52.R2493</idno>
	</analytic>
	<monogr>
		<title level="j">Phys. Rev. A</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="page">2493</biblScope>
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Multiple-particle interference and quantum error correction</title>
		<author>
			<persName><forename type="first">A</forename><surname>Steane</surname></persName>
		</author>
		<idno type="DOI">10.1098/rspa.1996.0136</idno>
	</analytic>
	<monogr>
		<title level="j">Proc. Royal Soc. A</title>
		<imprint>
			<biblScope unit="volume">452</biblScope>
			<biblScope unit="page">2551</biblScope>
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Stabilizer codes and quantum error correction (Doctoral dissertation</title>
		<author>
			<persName><forename type="first">D</forename><surname>Gottesman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1997">1997</date>
		</imprint>
		<respStmt>
			<orgName>California Institute of Technology</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">NP-hardness of decoding quantum error-correction codes</title>
		<author>
			<persName><forename type="first">M.-H</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">Le</forename><surname>Gall</surname></persName>
		</author>
		<idno type="DOI">10.1103/PhysRevA.83.052331</idno>
	</analytic>
	<monogr>
		<title level="j">Phys. Rev. A</title>
		<imprint>
			<biblScope unit="volume">83</biblScope>
			<biblScope unit="page">52331</biblScope>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Efficient algorithms for maximum likelihood decoding in the surface code</title>
		<author>
			<persName><forename type="first">S</forename><surname>Bravyi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Suchara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vargo</surname></persName>
		</author>
		<idno type="DOI">10.1103/PhysRevA.90.032326</idno>
	</analytic>
	<monogr>
		<title level="j">Phys. Rev. A</title>
		<imprint>
			<biblScope unit="volume">90</biblScope>
			<biblScope unit="page">32326</biblScope>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Density-matrix simulation of small surface codes under current and projected experimental noise</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">E</forename><surname>O'brien</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Tarasinski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Di-Carlo</surname></persName>
		</author>
		<idno type="DOI">10.1038/s41534-017-0039-x</idno>
		<ptr target="https://github.com/obriente/surf17circuit" />
	</analytic>
	<monogr>
		<title level="m">npj Quantum Information</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">39</biblScope>
		</imprint>
	</monogr>
	<note>The source code of the Surface-17 simulation can be</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<ptr target="https://github.com/obriente/qgarden" />
		<title level="m">The source code of the blossom decoder can be</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<author>
			<persName><forename type="first">B</forename><surname>Heim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">M</forename><surname>Svore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">B</forename><surname>Hastings</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.06373</idno>
		<title level="m">Optimal circuit-level decoding for surface codes</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Optimal resources for topological two-dimensional stabilizer codes: Comparative study</title>
		<author>
			<persName><forename type="first">H</forename><surname>Bombin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Martin-Delgado</surname></persName>
		</author>
		<idno type="DOI">10.1103/PhysRevA.76.012305</idno>
	</analytic>
	<monogr>
		<title level="j">Phys. Rev. A</title>
		<imprint>
			<biblScope unit="volume">76</biblScope>
			<biblScope unit="page">12305</biblScope>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Optimal complexity correction of correlated errors in the surface code</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Fowler</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1310.0863</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Long shortterm memory</title>
		<author>
			<persName><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
		<idno type="DOI">10.1162/neco.1997.9.8.1735</idno>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">1735</biblScope>
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<author>
			<persName><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.2329</idno>
		<title level="m">Recurrent neural network regularization</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">A decoding algorithm for CSS codes using the X/Z correlations</title>
		<author>
			<persName><forename type="first">N</forename><surname>Delfosse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-P</forename><surname>Tillich</surname></persName>
		</author>
		<idno type="DOI">10.1109/ISIT.2014.6874997</idno>
	</analytic>
	<monogr>
		<title level="m">IEEE International Symposium on Information Theory</title>
		<imprint>
			<date type="published" when="2014">2014. 2014</date>
			<biblScope unit="page">1071</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<author>
			<persName><forename type="first">B</forename><surname>Criger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Ashraf</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1709.02154</idno>
		<title level="m">Multi-path summation for decoding 2D topological codes</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Fast decoders for topological quantum codes</title>
		<author>
			<persName><forename type="first">G</forename><surname>Duclos-Cianci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Poulin</surname></persName>
		</author>
		<idno type="DOI">10.1103/PhysRevLett.104.050504</idno>
	</analytic>
	<monogr>
		<title level="j">Phys. Rev. Lett</title>
		<imprint>
			<biblScope unit="volume">104</biblScope>
			<biblScope unit="page">50504</biblScope>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Efficient Markov chain Monte Carlo algorithm for the surface code</title>
		<author>
			<persName><forename type="first">A</forename><surname>Hutter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Wootton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Loss</surname></persName>
		</author>
		<idno type="DOI">10.1103/PhysRevA.89.022326</idno>
	</analytic>
	<monogr>
		<title level="j">Phys. Rev. A</title>
		<imprint>
			<biblScope unit="volume">89</biblScope>
			<biblScope unit="page">22326</biblScope>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Brevdo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Citro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Harp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Jozefowicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kudlur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Levenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Mané</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Monga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Olah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Talwar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Viégas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Warden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wattenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wicke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zheng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.04467</idno>
		<title level="m">TensorFlow: Large-scale machine learning on heterogeneous distributed systems</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<ptr target="https://github.com/baireuther/neuralnetworkdecoder" />
		<title level="m">The source code of the neural network decoder can be</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<ptr target="https://github.com/baireuther/circuitmodel" />
		<title level="m">The source code of the error model can be</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Surface code quantum computing by lattice surgery</title>
		<author>
			<persName><forename type="first">C</forename><surname>Horsman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Fowler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Devitt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Van Meter</surname></persName>
		</author>
		<idno type="DOI">10.1088/1367-2630/14/12/123011</idno>
	</analytic>
	<monogr>
		<title level="j">New J. Phys</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page">123011</biblScope>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<title level="m">Adam: A method for stochastic optimization</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
