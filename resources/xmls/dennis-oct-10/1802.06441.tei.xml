<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Deep neural decoders for near term fault-tolerant experiments</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2018-04-06">6 Apr 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Christopher</forename><surname>Chamberland</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Institute for Quantum Computing</orgName>
								<orgName type="department" key="dep2">Department of Physics and Astronomy</orgName>
								<orgName type="institution">University of Waterloo</orgName>
								<address>
									<postCode>N2L 3G1</postCode>
									<settlement>Waterloo</settlement>
									<region>Ontario</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Pooya</forename><surname>Ronagh</surname></persName>
							<email>pooya.ronagh@uwaterloo.ca</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Institute for Quantum Computing</orgName>
								<orgName type="department" key="dep2">Department of Physics and Astronomy</orgName>
								<orgName type="institution">University of Waterloo</orgName>
								<address>
									<postCode>N2L 3G1</postCode>
									<settlement>Waterloo</settlement>
									<region>Ontario</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Perimeter Institute for Theoretical Physics</orgName>
								<address>
									<addrLine>3 1QBit</addrLine>
									<postCode>N2L 2Y5, V6C 2B5</postCode>
									<settlement>Waterloo, Vancouver</settlement>
									<region>Ontario, British Columbia</region>
									<country>Canada, Canada</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Deep neural decoders for near term fault-tolerant experiments</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2018-04-06">6 Apr 2018</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:1802.06441v2[quant-ph]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-10-09T23:23+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>numbers: 03</term>
					<term>67</term>
					<term>Pp</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Finding efficient decoders for quantum error correcting codes adapted to realistic experimental noise in fault-tolerant devices represents a significant challenge. In this paper we introduce several decoding algorithms complemented by deep neural decoders and apply them to analyze several fault-tolerant error correction protocols such as the surface code as well as Steane and Knill error correction. Our methods require no knowledge of the underlying noise model afflicting the quantum device making them appealing for real-world experiments. Our analysis is based on a full circuitlevel noise model. It considers both distance-three and five codes, and is performed near the codes pseudo-threshold regime. Training deep neural decoders in low noise rate regimes appears to be a challenging machine learning endeavour. We provide a detailed description of our neural network architectures and training methodology. We then discuss both the advantages and limitations of deep neural decoders. Lastly, we provide a rigorous analysis of the decoding runtime of trained deep neural decoders and compare our methods with anticipated gate times in future quantum devices. Given the broad applications of our decoding schemes, we believe that the methods presented in this paper could have practical applications for near term fault-tolerant experiments.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. Introduction</head><p>Recently, significant progress has been made in building small quantum devices with enough qubits allowing them to be potential candidates for several quantum information experiments <ref type="bibr">[1]</ref><ref type="bibr" target="#b0">[2]</ref><ref type="bibr" target="#b1">[3]</ref><ref type="bibr" target="#b2">[4]</ref>. Fault-tolerant quantum computing is one such avenue that has so far had a very limited experimental analysis <ref type="bibr" target="#b3">[5]</ref>. Given the sensitivity of quantum devices to noise, quantum error correction will be crucial in building quantum devices capable of reliably performing long quantum computations. However, quantum error correction alone is insufficient for achieving the latter goal. Since gates themselves can introduce additional errors into a quantum system, circuits need to be constructed carefully in order to prevent errors that can be corrected by the code from spreading into uncorrectable errors. Fault-tolerant quantum computing provides methods for constructing circuits and codes that achieves this goal. However this is at the expense of a significant increase in the number of qubits and the spacetime overhead of the underlying circuits (although some methods use very few qubits but have a large space-time overhead and vice-versa).</p><p>In recent years, several fault-tolerant protocols for both error correction and universal quantum computation have been proposed, each with their own tradeoffs <ref type="bibr" target="#b4">[6]</ref><ref type="bibr" target="#b5">[7]</ref><ref type="bibr" target="#b6">[8]</ref><ref type="bibr" target="#b7">[9]</ref><ref type="bibr" target="#b8">[10]</ref><ref type="bibr" target="#b9">[11]</ref><ref type="bibr" target="#b10">[12]</ref><ref type="bibr" target="#b11">[13]</ref><ref type="bibr" target="#b12">[14]</ref><ref type="bibr" target="#b13">[15]</ref><ref type="bibr" target="#b14">[16]</ref><ref type="bibr" target="#b15">[17]</ref><ref type="bibr" target="#b16">[18]</ref><ref type="bibr" target="#b17">[19]</ref><ref type="bibr" target="#b18">[20]</ref>. One important aspect of quantum error correcting codes is in finding efficient decoders (the ability to identify the most likely errors which are afflicting the system) that can optimally adapt to noise models afflicting quantum systems in realistic experimental settings. Better decoders result in higher thresholds, and can thus tolerate larger noise rates making near term devices more accessible to fault-tolerant experiments. In <ref type="bibr" target="#b19">[21]</ref>, a hard decoding algorithm was proposed for optimizing thresholds of concatenated codes afflicted by general Markovian noise channels. In <ref type="bibr" target="#b20">[22,</ref><ref type="bibr" target="#b21">23]</ref>, tensor network algorithms were used for simulating the surface code and obtaining efficient decoders for general noise features. However, the above schemes are not adapted to fault-tolerant protocols where gate and measurement errors plays a significant role. Furthermore, some knowledge of the noise is required in order for the decoding protocols to achieve good performance. This can be a significant drawback since it is often very difficult to fully characterize the noise in realistic quantum devices.</p><p>The above challenges motivate alternative methods for finding efficient decoders which can offer improvements over more standard methods such as minimum weight perfect matching for topological codes <ref type="bibr" target="#b22">[24,</ref><ref type="bibr" target="#b23">25]</ref> and message passing for concatenated codes <ref type="bibr" target="#b24">[26]</ref>. One interesting idea is using deep neural networks for constructing decoders which are both efficient and can tolerate large noise rates. The hope is that even if the underlying noise model is completely unknown, with enough experimen-tal data, deep neural networks could learn the probability density functions of the different possible errors corresponding to the sequences of measured syndromes. Note that due to measurement and gate errors, it is often necessary to repeat the syndrome measurements in fault-tolerant protocols as will be explained in Section II.</p><p>The first work in which machine learning was used for decoding was in a paper by Torlai and Melko <ref type="bibr" target="#b25">[27]</ref>. In this paper, a Boltzmann machine was trained to correct phase-flip errors of a 2-dimensional toric code. Krastanov and Jiang obtained a neural network decoder applicable to general stabilizer codes and applied it to the 2-D toric code obtaining a higher code-capacity threshold than previous results. Varsamopoulos, Criger and Bertels used a feed-forward neural network to decode the surface code <ref type="bibr" target="#b26">[28]</ref>. They also applied their decoding scheme to the distance three surface code under a full circuit level noise model. Baireuther, O'Brien, Tarasinski and Beenakker used a recurrent neural network that could be trained with experimental data <ref type="bibr" target="#b27">[29]</ref>. They applied their decoding scheme to compare the lifetime of qubits encoded in a distance-three surface code. The analysis was based on a full circuit level noise model, albeit with a modified CNOT gate error model. Breuckmann and Ni <ref type="bibr" target="#b28">[30]</ref> gave a scalable neural decoder applicable to higher dimensional codes by taking advantage of the fact that these codes have local decoders. To our knowledge, these methods could not be applied to codes of dimensions less than four. Lastly, while preparing the updated version of our manuscript, Maskara, Kubica and Jochym-O'Connor used neural-network decoders to study the code capacity thresholds of color codes <ref type="bibr" target="#b29">[31]</ref>.</p><p>Despite the numerous works in using neural networks for decoding, there are still several open questions that remain:</p><p>1. What are the fastest possible decoders that can be achieved using neural networks and how does the decoding time compare to gate times in realistic quantum devices?</p><p>2. Can neural networks still offer good performance beyond distance three codes in a full circuit level noise model regime? If so, what are the limitations?</p><p>3. How well do neural networks perform near and below typical thresholds of fault-tolerant schemes under full circuit level noise models?</p><p>In this paper we aim to address the above questions. We apply a plethora of neural network methods to analyze several fault-tolerant error correction schemes such as the surface code as well as the CNOT-exRec gate using Steane error correction (EC) and Knill-EC, and consider both distance-three and distance-five codes. We chose the CNOT-exRec circuit since (in most cases) it limits the threshold of the underlying code when used with Steane and Knill-EC units <ref type="bibr" target="#b30">[32]</ref>. Our analysis is done using a full circuit level noise model. Furthermore our methods are designed to work with experimental data; i.e. no knowledge of the underlying noise model is required.</p><p>Lastly, we provide a rigorous analysis of the decoding times of the neural network decoders and compare our results with expected gate delays in future superconducting quantum devices. We suspect that even though inference from a trained neural network is a simple procedure comprising only of matrix multiplications and arithmetic operations, state-of-the-art parallel processing and high performance computing techniques would need to be employed in order for the inference to provide a reliable decoder given the anticipated gate times in future quantum devices.</p><p>The deep neural decoders (DND) we design in this paper assist a baseline decoder. For the baseline decoders, we will use both lookup table and naive decoding schemes which will be described in Section II. The goal of the deep neural decoder is to determine whether to add logical corrections to the corrections provided by the baseline decoders. Although the lookup table decoder is limited to small codes, the naive decoder can efficiently be implemented for arbitrary distance codes.</p><p>We stress that to offer a proper analysis of the performance of neural network decoders, the neural network should be trained for all considered physical error rates. We believe that from an experimental point of view, it is not realistic to apply a network trained for large physical error rates to lower rate noise regimes. The reason is simply that the network will be trained based on the hardware that is provided by the experimentalist. If the experimentalist tunes the device to make it noisier so that fewer non-trivial training samples are provided to the neural network, the decoder could be fine tuned to a different noise model than what was present in the original device. As will be shown, training neural networks at low error rates is a difficult task for machine learning and definitely an interesting challenge.</p><p>Our goal has been to compose the paper in such a way that makes it accessible to both the quantum information scientists and machine learning experts. The paper is structured as follows.</p><p>In Section II we begin by providing a brief review of stabilizer codes followed by the fault-tolerant error correction criteria used throughout this paper as well as the description of our full circuit level noise model. In Section II A, we review the rotated surface code and provide a new decoding algorithm that is particularly well adapted for deep neural decoders. In Sections II B and II C, we review the Steane and Knill fault-tolerant error correction methods and give a description of the distance-three and five color codes that will be used in our analysis of Steane and Knill-EC. In Section II D we give a description of the naive decoder and in Section II E we discuss the decoding complexity of both the lookup table and naive decoders.</p><p>Section III focuses on the deep neural decoders constructed, trained and analyzed in this paper. In Sec-tion III A we give an overview of deep learning by using the application of error decoding as a working example. We introduce three widely used architectures for deep neural networks: (1) simple feedforward networks with fully connected hidden layers, (2) recurrent neural networks, and (3) convolutional neural networks. We introduce hyperparameter tuning as a commonly used technique in machine learning and an important research tool for machine learning experts. In Sections III B and III C we introduce the deep neural network architectures we designed for decoding the CNOT-exRec circuits in the case of Steane-and Knill-EC, and for multiple rounds of EC in the case of the rotated surface code.</p><p>In Section IV we provide our numerical results by simulating the above circuits under a full circuit level depolarizing noise channel, and feeding the results as training and test datasets for various deep neural decoders.</p><p>Finally, in Section V we address the question of practical applicability of deep neural decoders in their inference mode for fault-tolerant quantum error correction. We will address several hardware and software considerations and recommend a new development in machine learning known as network quantization as a suitable technology for decoding quantum error correcting codes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. Fault-tolerant protocols</head><p>In this section we will describe the fault-tolerant protocols considered in this paper. The surface code will be described in Section II A while Steane and Knill error correction will be described in Sections II B and II C. For each protocol, we will also describe the baseline decoder used prior to implementing a deep neural decoder (DND). Since we are focusing on near term fault-tolerant experiments, we will first describe decoding schemes using lookup tables which can be implemented extremely quickly for small distance codes. In Section IV we will show that the lookup table decoding schemes provide very competitive pseudo-thresholds. With existing computing resources and the code families considered in this paper, the proposed decoders can be used for distances d ≤ 7. For example, the distance-nine color code would require 8.8 exabytes of memory to store the lookup table. Lastly, in Section II D we will describe a naive decoder which is scalable and can be implemented efficiently while achieving competitive logical failure rates when paired with a deep neural decoder.</p><p>Before proceeding, and in order to make this paper as self contained as possible, a few definitions are necessary. First, we define the n-qubit Pauli group P A <ref type="bibr">[[n, k, d]</ref>] quantum error correcting code, which encodes k logical qubits into n physical qubits and can cor-rect t = (d -1)/2 errors, is the image space C q of the injection ξ : H k 2 → C q ⊂ H n 2 where H 2 is the twodimensional Hilbert space. Stabilizer codes are codes C q which form the unique subspace of H n 2 fixed by an Abelian stabilizer group S ⊂ P</p><p>(1) n such that for any s ∈ S and any codeword |c ∈ C q , s|c = |c . Any s ∈ S can be written as s</p><formula xml:id="formula_0">= g p1 1 • • • g p n-k</formula><p>n-k where the stabilizer generators g i satisfy g 2 i = I and mutually commute. Thus S = g 1 , • • • , g n-k . We also define N (S) to be the normalizer of the stabilizer group. Thus any nontrivial logical operator on codewords belongs to N (S)\S. The distance d of a code is the lowest weight operator P ∈ N (S) \ S. For more details on stabilizer codes see <ref type="bibr" target="#b31">[33,</ref><ref type="bibr" target="#b32">34]</ref>.</p><p>For a given stabilizer group S = g 1 , • • • , g n-k , we define the error syndrome s(E) of an error E to be a bit string of length nk where the i-th bit is zero if [E, g i ] = 0 and one otherwise. We say operators E and E are logically equivalent, written as E ∼ E , if and only if E ∝ gE for some g ∈ S.</p><p>The goal of an error correction protocol is to find the most likely error E afflicting a system for a given syndrome measurement s(E). However, the gates used to perform a syndrome measurement can introduce more errors into the system. If not treated carefully, errors can spread leading to higher weight errors which are non longer correctable by the code. In order to ensure that correctable errors remain correctable and that logical qubits have longer lifetimes than their un-encoded counterpart (assuming the noise is below some threshold), an error correction protocol needs to be implemented faulttolerantly. More precisely, an error correction protocol will be called fault-tolerant if the following two conditions are satisfied <ref type="bibr" target="#b11">[13,</ref><ref type="bibr" target="#b30">32,</ref><ref type="bibr" target="#b32">34]</ref>:</p><p>Definition 1 (Fault-tolerant error correction). For t = (d-1)/2 , an error correction protocol using a distanced stabilizer code C is t-fault-tolerant if the following two conditions are satisfied: 1. For an input codeword with error of weight s 1 , if s 2 faults occur during the protocol with s 1 + s 2 ≤ t, ideally decoding the output state gives the same codeword as ideally decoding the input state.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2.</head><p>For s faults during the protocol with s ≤ t, no matter how many errors are present in the input state, the output state differs from a codeword by an error of at most weight s.</p><p>A few clarifications are necessary. By ideally decoding, we mean performing fault-free error correction. In the second condition of Definition 1, the output state can differ from any codeword by an error of at most weight s, not necessarily by the same codeword as the input state. It is shown in <ref type="bibr" target="#b11">[13,</ref><ref type="bibr" target="#b30">32]</ref> that both conditions are required to guarantee that errors do not accumulate during multiple error correction rounds and to ensure that error correction extends the lifetime of qubits as long as the noise is below some threshold. In this paper we focus on small distance codes which could potentially be implemented in near term faulttolerant experiments. When comparing the performance of fault-tolerant error correction protocols, we need to consider a full extended rectangle (exRec) which consists of leading and trailing error correction rounds in between logical gates. Note that this also applies to topological codes. An example of an exRec is given in Fig. <ref type="figure" target="#fig_1">1</ref>. We refer the reader to <ref type="bibr" target="#b30">[32,</ref><ref type="bibr" target="#b33">35]</ref> for further details on exRec's.</p><p>In constructing a deep neural decoder for a faulttolerant error correction protocol, our methods will be devised to work for unknown noise models which would especially be relevant to experimental settings. However, throughout several parts of the paper, we will be benchmarking our trained decoder against a full circuit level depolarizing noise channel since these noise processes can be simulated efficiently by the Gottesman-Knill theorem <ref type="bibr" target="#b34">[36]</ref>. A full circuit level depolarizing noise model is described as follows:</p><p>1. With probability p, each two-qubit gate is followed by a two-qubit Pauli error drawn uniformly and independently from {I, X, Y, Z} ⊗2 \ {I ⊗ I}.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">With probability 2p</head><p>3 , the preparation of the |0 state is replaced by |1 = X|0 . Similarly, with probability 2p  3 , the preparation of the |+ state is replaced by |-= Z|+ .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">With probability 2p</head><p>3 , any single qubit measurement has its outcome flipped. 4. Lastly, with probability p, each resting qubit location is followed by a Pauli error drawn uniformly and independently from {X, Y, Z}.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Rotated surface code</head><p>In this section we focus on the rotated surface code <ref type="bibr" target="#b8">[10,</ref><ref type="bibr" target="#b35">[37]</ref><ref type="bibr" target="#b36">[38]</ref><ref type="bibr" target="#b37">[39]</ref><ref type="bibr" target="#b38">[40]</ref><ref type="bibr" target="#b39">[41]</ref>. The rotated surface code is a [[d 2 , 1, d]] stabilizer code with qubits arranged on a 2-dimensional lattice as FIG. <ref type="figure">3</ref>: Fig. <ref type="figure">3a</ref> illustrates the circuit used to measure the stabilizer X ⊗4 and Fig. <ref type="figure">3b</ref> illustrates the circuit used to measure the stabilizer Z ⊗4 . As can be seen, a full surface code measurement cycle is implemented in six time steps.</p><p>shown in Fig. <ref type="figure" target="#fig_2">2</ref>. Any logical X operator has X operators acting on at least d qubits with one X operator in each row of the lattice involving an even number of green faces. Similarly, any logical Z operator has Z operators acting on at least d qubits with one Z operator in every column of the lattice involving an even number of red faces. It is possible to measure all the stabilizer generators by providing only local interactions between the data qubits and neighbouring ancilla qubits. The circuits used to measure both X and Z stabilizers are shown in Fig. <ref type="figure">3</ref>. Note that all stabilizer generators have weight two or four regardless of the size of the lattice.</p><p>Several decoding protocols have been devised for topological codes. Ideally, we would like decoders which have extremely fast decoding times to prevent errors from accumulating in hardware during the classical processing time while also having very high thresholds. The most common algorithm for decoding topological codes is Edmond's perfect matching algorithm (PMA) <ref type="bibr" target="#b22">[24]</ref>. Although the best know thresholds for topological codes under circuit level noise have been achieved using a slightly modified version of PMA <ref type="bibr" target="#b23">[25]</ref>, the decoding algorithm has a worst case complexity of O(n 3 ). Recent progress has shown that minimum weight perfect matching can be performed in O(1) time on average given constant computing resources per unit area on a 2D quantum computer <ref type="bibr" target="#b40">[42]</ref>. With a single processing element and given n detection events, the runtime can be made O(n) <ref type="bibr" target="#b41">[43]</ref>. Renormalization group (RG) decoders have been devised that can achieve O(log n) decoding times under parallelization <ref type="bibr" target="#b42">[44]</ref><ref type="bibr" target="#b43">[45]</ref><ref type="bibr" target="#b44">[46]</ref>. However such decoders typically have lower thresholds than PMA. Wootton and Loss <ref type="bibr" target="#b45">[47]</ref> use a Markov chain Monte Carlo method to obtain near optimal code capacity noise thresholds of the surface code at the cost of slower decoding times compared to other schemes. Recently, Delfosse and Nickerson <ref type="bibr" target="#b46">[48]</ref> have devised a near linear time decoder for topological codes that achieves thresholds slightly lower than PMA for the 2-dimensional toric code.</p><p>Here we construct a decoder for the surface code which has extremely fast decoding times and achieves high pseudo-thresholds which will serve as a core for our deep neural decoder construction of Section III. Our decoder will be based on a lookup table construction which could be used for distances d ≤ 7. Before describing the construction of the lookup table, we point out that a single fault on the second or third CNOT gates in Figs. 3a and 3b can propagate to a data qubit error of weighttwo. Thus for a surface code that can correct t = 2d + 1 errors, a correction E for an error E resulting from t faults, with E ∼ E, must be used when the syndrome s(E) is measured. In other words, the minimum weight correction must not always be used for errors that result from faults occurring at the CNOT gates mentioned above.</p><p>With the above in mind, the lookup table is constructed a follows. For every 1 ≤ m ≤ 2 d 2 -1 , use the lowest weight error E ∼ E such that converting the bit string s(E) to decimal results in m. If E is an error that results from v ≤ t = 2d + 1 faults with wt(E) &gt; t, then use E ∼ E instead of the lowest weight error corresponding to the syndrome s(E). Note that for this method to work, all errors E with wt(E) ≤ t must have distinct syndromes from errors E that arise from v ≤ t faults with wt(E ) &gt; t. However this will always be the case for surface codes with the CNOT ordering chosen in Fig. <ref type="figure">3</ref>.</p><p>Note that with the above construction, after measuring the syndrome s, decoding simply consists of converting s to decimal (say m) and correcting by choosing the error on the m'th row of the lookup table. Note however that this method is not scalable since the number of syndromes scales exponentially with the code distance.</p><p>Lastly, the decoding scheme as currently stated is not fault-tolerant. The reason is that if syndromes are measured only once, in some cases it would be impossible to distinguish data qubit errors from measurement errors. For instance, a measurement error occurring when measuring the green triangle of the upper left corner of Fig. <ref type="figure" target="#fig_2">2</ref> would result in the same syndrome as an X error on the first data qubit. However, with a simple modification, the surface code decoder can be made fault-tolerant. For distance 3 codes, the syndrome is measured three times and we decode using the majority syndrome. If there are no majority syndromes, the syndrome from the last round is used to decode. For instance, suppose that the syndromes s 1 , s 2 , s 2 were obtained, then the syndrome s 2 would be used to decode with the lookup table. However if all three syndromes s 1 , s 2 , s 3 were different, then s 3 would be used to decode with the lookup table. This decoder was shown to be fault-tolerant in <ref type="bibr" target="#b47">[49]</ref>.</p><p>For higher distance codes, we use the following scheme. First, we define the counter n diff (used for keeping track of changes in consecutive syndrome measurements) as Decoding protocol -update rules: Given a sequence of consecutive syndrome measurement outcomes s k and s k+1 :</p><p>1. If n diff did not increase in the previous round, and s k = s k+1 , increase n diff by one.</p><p>We also define E(s) to be the correction obtained from either the lookup table decoder or naive decoder (described in section Section II D) using the syndrome s. With the above definition of n diff , the decoding protocol for a code that can correct any error E with wt(E) ≤ t = (d-1) 2 is implemented as Decoding protocol -corrections: Set n diff = 0. Repeat the syndrome measurement. Update n diff according to the update rule above.</p><p>1. If at anytime n diff = t, repeat the syndrome measurement yielding the syndrome s. Apply the correction E(s).</p><p>2. If the same syndrome s is repeated tn diff + 1 times in a row, apply the correction E(s).</p><p>Note that in the above protocol, the number of times the syndrome is repeated is non-deterministic. The minimum number of syndrome measurement repetitions is t+1 while in <ref type="bibr" target="#b11">[13]</ref> it was shown that the maximum number of syndrome measurement repetitions is 1 2 (t 2 + 3t + 2). Further, a proof that the above protocol satisfies both fault-tolerance criteria in Definition 1 is given in Appendix A of <ref type="bibr" target="#b11">[13]</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Steane error correction</head><p>Calderbank-Shor-Steane (CSS) codes <ref type="bibr" target="#b4">[6,</ref><ref type="bibr" target="#b5">7]</ref> are quantum error correcting codes which are constructed from two classical error correcting codes C 1 and C 2 where</p><formula xml:id="formula_1">C ⊥ 1 ⊆ C 2 .</formula><p>The last condition guarantees that by choosing the X and Z stabilizers to correspond to the parity check matrices H X and H Z of C 1 and C 2 , all operators in H X will commute with those of H Z . Additionally, CSS codes are the only codes such that a transversal CNOT gate performs a logical CNOT.</p><p>Steane error correction <ref type="bibr" target="#b48">[50]</ref> takes advantage of properties of CSS codes to measure the X and Z stabilizers using transversal CNOT gates. To see this, consider the circuit in Fig. <ref type="figure" target="#fig_3">4a</ref>. The transversal CNOT gate between the encoded data block |ψ and ancilla |+ acts trivially (i.e. CNOT|ψ |+ = |ψ |+ ). However, any X errors afflicting the data block would then be copied to the ancilla state. Furthermore, CSS codes have the property that transversally measuring the codeword |+ in the absence of errors would result in a codeword of C 1 chosen uniformly at random. If X errors are present on the codeword |+ , then the transversal measurement would yield the classical codeword e + f + g. Here, (e|0) (written in binary symplectic form) are the X errors on the data qubits, (f |0) are the X errors that arise during the preparation of the |+ state and (g|0) are bit-flip errors that arise during the transversal measurement. Applying the correction X e X f X g on the data would result in an X error of weight f + g. An analogous argument can be made for Z errors using the circuit of Fig. <ref type="figure" target="#fig_3">4b</ref> (note that in this case we measure in the X-basis which maps C 1 → C 2 and Z → X).</p><p>Note that the circuits used to prepared the encoded |+ and |0 states are in general not fault-tolerant. In the case of |+ , low weight errors can spread to highweight X errors which can change the outcome of the measurement and Z errors which can propagate to the data block due to the transversal CNOT gates. However, by preparing extra "verifier" states encoded in |+ and coupling these states to the original |+ ancilla as shown in Fig. <ref type="figure" target="#fig_4">5</ref>, high weight X and Z errors arising from the ancilla can be detected. Furthermore, after a classical error correction step, the eigenvalue of X and Z can be measured. Therefore if a non-trivial syndrome is measured in the verifier states or the -1 eigenvalue of a logical operator is measured, the ancilla qubits are rejected and new ancilla qubits are brought in to start the process anew. We would like to point out that instead of verifying the ancilla qubits for errors and rejecting them when a non-trivial syndrome is measured, it is also possible to replace the verification circuit with a decoding circuit. By performing appropriate measurements on the ancilla qubits and making use of Pauli frames <ref type="bibr" target="#b7">[9,</ref><ref type="bibr" target="#b49">51,</ref><ref type="bibr" target="#b50">52]</ref>, any errors arising from t-faults in the ancilla circuits can be identified and corrected <ref type="bibr" target="#b51">[53]</ref> (note that DiVincenzo and Aliferis provided circuits for Steane's [ <ref type="bibr" target="#b5">[7,</ref><ref type="bibr">1,</ref><ref type="bibr" target="#b1">3]</ref>] code so that t = 1). However in this paper we will focus on ancilla verification methods.</p><p>It can be shown that the Steane-EC circuit of Fig. <ref type="figure" target="#fig_4">5</ref> satisfies both fault-tolerant conditions of Definition 1 for distance-three codes <ref type="bibr" target="#b32">[34]</ref>. It is possible to use the same ancilla verification circuits in some circumstances for higher distance codes by carefully choosing different circuits for preparing the logical |0 and |+ states (see <ref type="bibr" target="#b52">[54]</ref> for some examples). In this paper, we will choose appropriate |0 and |+ states such that the the decoding schemes will be fault-tolerant using the ancilla verification circuits in Fig. <ref type="figure" target="#fig_4">5</ref>. We would like to add that although the order in which transversal measurements to correct bit-flip and phase-flip errors does not affect the fault-tolerant properties of Steane-EC, it does create an ] and [ <ref type="bibr" target="#b17">[19,</ref><ref type="bibr">1,</ref><ref type="bibr" target="#b3">5]</ref>] color code due to the large number of locations and thus makes an ideal circuit to optimize our decoding algorithm using machine learning.</p><p>asymmetry in the X and Z logical failure rates <ref type="bibr" target="#b52">[54]</ref><ref type="bibr" target="#b53">[55]</ref><ref type="bibr" target="#b54">[56]</ref>.</p><p>For instance, an X error arising on the target qubit of the logical CNOT used to detect phase errors would be copied to the |+ ancilla. However a Z error arising on the target of this CNOT or control of the CNOT used to correct bit-flip errors would not be copied to any of the ancilla qubits. We conclude this section by describing the [ <ref type="bibr" target="#b5">[7,</ref><ref type="bibr">1,</ref><ref type="bibr" target="#b1">3]</ref>] and [ <ref type="bibr" target="#b17">[19,</ref><ref type="bibr">1,</ref><ref type="bibr" target="#b3">5]</ref>] CSS color codes <ref type="bibr" target="#b55">[57]</ref> which will be the codes used for optimizing our decoding algorithms with machine learning applied to Steane and Knill error correction (see Section II C for a description of Knill-EC). A pictorial representation for both of these codes is shown in Fig. <ref type="figure" target="#fig_5">6</ref>. Both the Steane code and the 19-qubit color code are self-dual CSS codes (meaning that the X and Z stabilizers are represented by the same parity check matrix). The Steane code has three X and Z stabilizer generators while the 19-qubit color code has nine X and Z stabilizer generators. Since these codes are small, it is possible to use a lookup table decoder similar to the one presented in Section II A to correct errors. The only difference is that we do not have to consider weight-two errors arising from a single fault (since all gates in Steane and Knill-EC are transversal). We will also analyze the performance of both codes using the naive decoder described in Section II D.</p><p>To obtain a pseudo-threshold for both of these codes, we will consider the CNOT-exRec since it is the logical gate with the largest number of locations and thus will limit the performance of both codes <ref type="bibr" target="#b30">[32]</ref> (here we are considering the universal gate set generated by CNOT, T, H where T = diag(1, e iπ/4 ) and H is the Hadamard gate <ref type="bibr" target="#b56">[58]</ref>). The full CNOT-exRec circuit for Steane-EC is shown in Fig. <ref type="figure">7</ref>. Note that the large number of CNOT gates will result in a lot of correlated errors which adds a further motivation to consider several neural networks techniques to optimize the decoding performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Knill error correction</head><p>Steane error correction described in Section II B only applies to stabilizer codes which are CSS codes. Further, the protocol requires two transversal CNOT gates between the data and ancilla qubits. In this section we will give an overview of Knill error correction <ref type="bibr" target="#b6">[8,</ref><ref type="bibr" target="#b7">9]</ref> which is applicable to any stabilizer code. As will be shown Knill-EC only requires a single transversal CNOT gate between the data qubits and ancilla qubits.</p><p>Consider a Pauli operator P acting on the data block of the circuit in Fig. <ref type="figure" target="#fig_6">8</ref>. Consider the same Pauli P (but with a possibly different sign) acting on the first ancilla block of the logical Bell pair. P can be any Pauli but in the argument that follows we will be interested in cases where P ∈ N (S). Taking into account the sign of P and writing it as a product of X and Z, we have that (-1) bi P = i c(P X ,P Z ) (-1) bi P X P Z .</p><p>(1)</p><p>The function c(P X , P Z ) = 0 if P X and P Z commute and one otherwise. The phase i c(P X ,P Z ) comes from the Y operators in P and (-1) bi indicates the sign of the Pauli where i = 0 for the data block and i = 1 for the ancilla block.</p><p>Applying the transversal CNOT's between the ancilla and data block performs the following transformations (-1) b0 P ⊗ I → i c(P X ,P Z ) (-1) b0 P X P Z ⊗ P X , (2) (-1) b1 I ⊗ P → i c(P X ,P Z ) (-1) b1 P Z ⊗ P X P Z , (3) and therefore (-1) b0+b1 P ⊗ P → (-1) b0+b1+c(P X ,P Z ) P X ⊗ P Z . (4) From Eq. ( <ref type="formula">4</ref>), we can deduce that a subsequent measurement of X on each physical data qubit and measurement of Z on each physical qubit in the first ancilla block lets us deduce the eigenvalue of P (since c(P X , P Z ) is known, we learn b 0 + b 1 ).</p><p>Since the above arguments apply to any Pauli, if P is a stabilizer we learn s 0 + s 1 where s 0 is the syndrome of the data block and s 1 is the error syndrome of the first ancilla block. Furthermore, the measurements also allow us to deduce the eigenvalues of the logical Pauli's X i ⊗ X i and Z i ⊗ Z i for every logical qubit i. This means that in addition to error correction we can also perform the logical Bell measurement required to teleport the encoded data to the second ancilla block.</p><p>Note that pre-existing errors on the data or ancilla block can change the eigenvalue of the logical operator P ⊗ P without changing the codeword that would be deduced using an ideal decoder. For instance, if E d is the error on the data block and E a the error on the ancilla block with wt(E d ) + wt(E a ) ≤ t, then if (-1) b is the eigenvalue of P ⊗ P , we would instead measure (-1) b where b = b + c(E d , P ) + c(E a , P ). The same set of measurements also let's us deduce the syndrome</p><formula xml:id="formula_2">s(E d ) + s(E a ) = s(E d E a ). But since wt(E d E a ≤ t), from s(E d E a ) we deduce the error E = E a E d M where M ∈ S.</formula><p>Hence once E is deduced, we also get the correct eigenvalue of P ⊗ P thus obtaining the correct outcome for the logical Bell measurement.</p><p>There could also be faults in the CNOT's and measurements when performing Knill-EC. We can combine the errors from the CNOT's and measurements into the Pauli G on the data block and F on the ancilla block where the weight of GF is less than or equal to the number faults at the CNOT and measurement locations. Given the basis in which the measurements are performed, we can assume that G consists only of Z errors and F of X errors. Consequently, for a full circuit level noise model, the final measured syndrome is s(E d E a GF ).</p><p>As in Steane-EC, the circuits for preparing the logical |0 and |+ states are not fault-tolerant and can result in high weight errors on the data. However, if the error correcting code is a CSS code, then we can use the same ancilla verification method presented in Section II B to make the full Knill-EC protocol fault-tolerant. In Fig. <ref type="figure" target="#fig_7">9</ref> we show the full CNOT-exRec circuit using Knill-EC. Note that for each EC unit, there is an extra idle qubit location compared to Steane-EC. Lastly, we point out that another motivation for using Knill-EC is it's ability to handle leakage errors. A leakage fault occurs when the state of a two-level system, which is part of a higher dimensional subspace, transitions outside of the subspace. In <ref type="bibr" target="#b57">[59]</ref>, it was shown how leakage faults can be reduced to a regular fault (which acts only on the qubit subspace) with the use of Leakage-Reduction Units (LRU's). One of the most natural ways to implement LRU's is through quantum teleportation <ref type="bibr" target="#b58">[60]</ref>. Since Knill-EC teleports the data block to the ancilla block, unlike in Steane-EC, LRU's don't need to be inserted on the input data block. However, LRU's still need to be inserted after the preparation of every |0 and |+ states.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Naive decoder</head><p>Since the lookup table decoder scheme presented in previous sections is not scalable, it would be desirable to have a scalable and fast decoding scheme that can achieve competitive thresholds when paired with a deep neural decoder. In this section we provide a detailed description of a naive decoder which can replace the lookup table scheme in all of the above protocols.</p><p>We first note that the recovery operator R s for a mea-sured syndrome s can be written as <ref type="bibr" target="#b24">[26,</ref><ref type="bibr" target="#b42">44</ref>]</p><formula xml:id="formula_3">R s = L(s)T (s)G(s)<label>(5)</label></formula><p>which we will refer to as the LST decomposition of E. In Eq. ( <ref type="formula" target="#formula_3">5</ref>), L(s) is a product of logical operators (operators in N (S) \ S), G(s) is a product of stabilizers (operators in S) and T (s) is a product of pure errors. Pure errors form an abelian group with the property that T i appears in T (s) if and only if the i'th syndrome bit is 1 (i.e.</p><p>[T i , T j ] = 0 and [T j , g k ] = δ j,k where g k is the k'th stabilizer generator). Thus pure errors can be obtained from Gaussian elimination. Note that the choice of operators in G(s) will not effect the outcome of the recovered state. Consequently, given a measured syndrome s, decoding can be viewed as finding the most likely logical operator in L(s).</p><p>For a measured syndrome s, a naive decoding scheme is to always choose the recovery operator R l = T (s) which is clearly suboptimal. However, for such a decoder, the decoding complexity results simply from performing the matrix multiplication s T where s = (s 1 , s 2 , • • • , s n-k ) is the syndrome written as a 1 × (n -k) vector and T is a (n -k) × n matrix where the j'th row corresponds to T j . The goal of all neural networks considered in Section III will then be to find the most likely operator L(s) from the input syndrome l.</p><p>The set of stabilizer generators, logical operators and pure errors for all the codes considered in this paper are provided in Table <ref type="table" target="#tab_6">VIII</ref>. Lastly, we point out that a version of the above decoding scheme was implemented in <ref type="bibr" target="#b26">[28]</ref> for the distance-three surface code.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Lookup table and naive decoder complexity</head><p>From a complexity theoretic point of view, read-out of an entry of an array or a hash table requires constant time. In hash tables, a hash function is calculated to find the address of the entry inquired. The hash function calculation takes the same processing steps for any entry, making this calculation O(1). In the case of an array, the key point is that the array is a sequential block of the memory with a known initial pointer. Accessing any entry requires calculating its address in the memory by adding its index to the address of the beginning of the array. Therefore, calculating the address of an entry in an array also takes O(1).</p><p>It remains to understand that accessing any location in the memory given its address is also O(1) as far as the working of the memory hardware is concerned. This is the assumption behind random access memory (RAM) where accessing the memory comprises of a constant time operation performed by the multiplexing and demultiplexing circuitry of the RAM. This is in contrast with direct-access memories (e.g. hard disks, magnetic tapes, etc) in which the time required to read and write data depends on their physical locations on the device and the lag resulting from disk rotation and arm movement. Given the explanation above, a decoder that relies solely on accessing recovery operators from an array operates in O(1) time. This includes the lookup table and the inference mapping method of Section V B below.</p><p>For the naive decoder of Section II D, we may also assume that the table of all pure errors (denoted as T in Section II D) is stored in a random access memory. However, the algorithm for generating a recovery from the naive decoder is more complicated than only accessing an element of T . With n qubits and nk syndromes, for every occurrence of 1 in the syndrome string, we access an element of T . The elements accessed in this procedure have to be added together. With parallelization, we may assume that a tree adder is used which, at every stage, adds two of the selected pure error strings to each other. Addition of every two pure error strings is performed modulo 2 which is simply the XOR of the two strings, which takes O(1) time assuming parallel resources. The entire procedure therefore has a time complexity of O((nk) log(n -k)), again assuming parallel digital resources.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. Deep neural decoders</head><p>In most quantum devices, fully characterizing the noise model afflicting the system can be a significant challenge. Furthermore, for circuit level noise models which cannot be described by Pauli channels, efficient simulations of a codes performance in a fault-tolerant implementation cannot be performed without making certain approximations (a few exceptions for repetition codes can be found in <ref type="bibr" target="#b59">[61]</ref>). However, large codes are often required to achieve low failure rates such that long quantum computations can be performed reliably. These considerations motivate fast decoding schemes which can adapt to unknown noise models encountered in experimental settings.</p><p>Recall from Section II D that decoding can be viewed as finding the most likely operator L ∈ L(s) given a measured syndrome s. Since all codes considered in this paper encode a single logical qubit, the recovery operator for a measured syndrome s can be written as</p><formula xml:id="formula_4">R s = X b1(s) L Z b2(s) L T (s)G(s)<label>(6)</label></formula><p>where X L and Z L are the codes logical X and Z operators and b 1 (s), b 2 (s) ∈ Z 2 . In <ref type="bibr" target="#b19">[21]</ref>, a decoding algorithm applicable to general Markovian channels was presented for finding the coefficients b 1 (s) and b 2 (s) which optimized the performance of error correcting codes. However, the algorithm required knowledge of the noise channel and could not be directly applied to circuit level noise thus adding further motivation for a neural network decoding implementation.</p><p>In practice, the deep learning schemes described in this section can be trained as follows. First, to obtain the training set, the data qubits are fault-tolerantly prepared in a known logical |0 or |+ state followed by a round of fault-tolerant error correction (using either the lookup table or naive decoders). The encoded data is then measured in the logical Z or X basis yielding a -1 eigenvalue if a logical X or Z error occurred. The training set is constructed by repeating this sequence several times both for states prepared in |0 or |+ . For each experiment, all syndromes are recorded as well as the outcome of the logical measurement. Given the most likely error E with syndrome s(E) = s (in general E will not be known), the neural network must then find the vector b</p><formula xml:id="formula_5">= (b 1 (s), b 2 (s)) such that X b1(s) L Z b2(s) L</formula><p>R s E = I where R s was the original recovery operator obtained from either the lookup table or naive decoders described in Section II.</p><p>Once the neural network is trained, to use it in the inference mode (as explained in Section V B), a query to the network simply consists of taking as input all the measured syndromes and returning as output the vector b. For Steane and Knill EC, the syndromes are simply the outcomes of the transversal X and Z measurements in the leading and trailing EC blocks. For the surface code, the syndromes are the outcomes of the ancilla measurements obtained from each EC round until the protocols presented in Section II A terminate.</p><p>Lastly, we note that a similar protocol was used in <ref type="bibr" target="#b27">[29]</ref> which also used the outcome of the final measurement on the data qubits to decode. However by using our method, once the neural network is trained, it only takes as input the measured syndromes in an EC round to compute the most likely b.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Deep learning</head><p>Here we explain the generic framework of our deep learning experiments. We refer the reader to <ref type="bibr" target="#b60">[62]</ref> for an introduction to deep learning and to <ref type="bibr" target="#b61">[63]</ref> for machine learning methods in classification tasks.</p><p>Let D ⊆ D be a data set. In our case, D = S × B is the set of all pairs of syndromes and error labels. Every element in D and D is therefore a pair (s, e) of measured syndromes s ∈ S and error labels e ∈ B. The error labels can be different depending on how we model the learning problem. For instance, every e ∈ B can be a bit string carrying a prescription of recovery operators:</p><formula xml:id="formula_6">B = {I, X, Y, Z} #physical qubits .</formula><p>There is however a major drawback in modelling the errors in the above fashion. For deep learning purposes the elements e ∈ B are represented in their 1-hot encoding, i.e. a bit string consisting of only a single 1, and zeros everywhere else. The 1-hot encoding therefore needs |E| bits of memory allocated to itself which by the definitions above, grows exponentially in either the number of physical qubits.</p><p>Our solution for overcoming this exponentially growing model is to take advantage of the decomposition (Eq. ( <ref type="formula" target="#formula_4">6</ref>)) of the recovery operator and only predict vectors b = (b 1 ( ), b 2 ( )) as explained earlier. In other words, the elements of B contain information about the logical errors remaining from the application of another auxiliary encoding scheme:</p><formula xml:id="formula_7">B = {I, X, Y, Z} #logical qubits .</formula><p>The objective function. As customary in machine learning, the occurrences x = (s, b) ∈ D are viewed as statistics gathered from a conditional probability distribution function p(x) = P(b | s) defined over S × E. The goal is then to approximate p by another distribution p w which is easy to compute from a set of real-valued parameters w. The training phase in machine learning consists of optimizing the parameter vector w such that p w is a good approximation of p. The optimization problem to solve is therefore</p><formula xml:id="formula_8">min w ∆(p, p w ). (<label>7</label></formula><formula xml:id="formula_9">)</formula><p>Here ∆ is some notion of distance in the space of probability distribution functions which, when applied to machine learning, is also called the loss function. In our case, the distance is the softmax cross entropy as explained here. The softmax function with respect to p is given via</p><formula xml:id="formula_10">ρ(x) = e p(x) x∈D e p(x) .<label>(8)</label></formula><p>From this definition it is obvious that no normalization of the dataset D is needed since softmax already results in a probability distribution function. The cross entropy function</p><formula xml:id="formula_11">H(π 1 , π 2 ) = H(π 1 ) + D KL (π 1 ||π 2 ) = - x π 1 (x) log π 2 (x)<label>(9)</label></formula><p>is then applied after softmax. This turns <ref type="bibr" target="#b5">(7)</ref> into</p><formula xml:id="formula_12">min w h(w) = H(ρ(p), ρ(p w )). (<label>10</label></formula><formula xml:id="formula_13">)</formula><p>Optimization of the softmax cross-entropy is a common practice in classification problems.</p><p>The neural network. A neural network is a directed graph equipped with a random variable assigned to each of its nodes. The elements of the parameter vector w are assigned either to an edge of the graph or a node of the graph (in the former case they are called weights and in the latter case they are called biases). The roll of the neural network in solving <ref type="bibr" target="#b8">(10)</ref> is to facilitate a gradient descent direction for the vector w in <ref type="bibr" target="#b8">(10)</ref>. This is achieved by imposing the random variables of each node to be a function of the random variables with incoming edges to the former one. The common choice for such a functional relationship is an affine transformation composed with a nonlinear function (called the activation function) with an easy to compute derivative. Given every node v of the neural network, we define:</p><formula xml:id="formula_14">X v = a v u→v w uv X u + w v .<label>(11)</label></formula><p>The simplest activation function is of course the identity. Historically, the sigmoid function σ(x) =</p><formula xml:id="formula_15">1 1+e -x</formula><p>was the most commonly used activation function and is motivated by its appearance in training restricted Boltzmann machines. By performing a change of variables, one obtains the trigonometric activation function tanh(x). These activation functions can cause the learning rate to slow down due to vanishing gradients in the early layers of deep neural networks, and this is the motivation for other proposed activation functions such as the rectified linear unit relu(x). Design and analysis of activation functions is an important step in machine learning <ref type="bibr" target="#b62">[64]</ref><ref type="bibr" target="#b63">[65]</ref><ref type="bibr" target="#b64">[66]</ref>.</p><p>The first and last layers of the network are known as the visible layers and respectively correspond to the input and output data (in our case the tuples (s, b) ∈ S × B as explained above). Successive applications of Eq. ( <ref type="formula" target="#formula_14">11</ref>) restricts the conditional distribution p w (b | s) into a highly nonlinear function f (w, s, b), for which the derivatives with respect to the parameters w are easy to compute via the chain rule. We may therefore devise a gradient descent method for solving Eq. ( <ref type="formula" target="#formula_12">10</ref>) by successive choices of descent directions starting from the deep layers and iterating towards the input nodes. In machine learning, this process is known as back-propagation.</p><p>Remark. The softmax function (Eq. ( <ref type="formula" target="#formula_10">8</ref>)) is in other words the activation function between the last two layers of the neural network.</p><p>Layouts. Although deep learning restricts the approximation of p w (b|s) to functions of the form f (w, s, b) as explained above, the latter has tremendous representation power, specially given the freedom in choice of the layout of the neural network. Designing efficient layouts for various applications is an artful and challenging area of research in machine learning. In this paper, we discuss three such layouts and justify their usage for the purposes of our deep neural decoding.</p><p>Feedforward neural network. By this we mean a multilayer neural network consisting of consecutive layers, each layer fully connected to the next one. Therefore, the underlying undirected subgraph of the neural network consisting of the neurons of two consecutive layers is a complete bipartite graph. In the case that the neural network only consists of the input and output layers (with no hidden layers), the network is a generalization of logistic regression (known as the softmax regression method).</p><p>Recurrent neural network (RNN). RNNs have performed incredibly well in speech recognition and natural language processing tasks <ref type="bibr" target="#b65">[67]</ref><ref type="bibr" target="#b66">[68]</ref><ref type="bibr" target="#b67">[69]</ref><ref type="bibr" target="#b68">[70]</ref>. The network is designed to resemble a temporal sequence of input data, with each input layer connecting to the rest of the network at a corresponding temporal epoch. The hidden cell of the network could be as simple as a single feedforward layer or more complicated. Much of the success of RNNs is based on peculiar designs of the hidden cell such as the Long-Short Term Memory (LSTM) unit as proposed in <ref type="bibr" target="#b69">[71]</ref>.</p><p>Convolutional neural network (CNN). CNNs have been successfully used in image processing tasks <ref type="bibr" target="#b70">[72,</ref><ref type="bibr" target="#b71">73]</ref>. The network is designed to take advantage of local properties of an image by probing a kernel across the input image and calculating the cross-correlation of the kernel vector with the image. By applying multiple kernels, a layer of features is constructed. The features can then be postprocessed via downsizing (called max-pooling) or by yet other feedforward neural networks.</p><p>In sections III B and III C, we present further details about applications of these neural networks to the errordecoding task.</p><p>Stochastic gradient descent. Since the cross-entropy in Eq. ( <ref type="formula" target="#formula_11">9</ref>) is calculated by a weighted sum over all events x ∈ D, it is impractical to exactly calculate it or its derivatives as needed for backpropagation. Instead, one may choose only a single sample x = (s, b) as a representative of the entire D in every iteration. Of course, this is a poor approximation of the true gradient but one hopes that the occurrences of the samples according to the true distribution would allow for the descent method to 'average out' over many iterations. This method is known as stochastic gradient descent (SGD) or online learning. We refer the reader to <ref type="bibr" target="#b72">[74]</ref> and <ref type="bibr" target="#b73">[75]</ref> and the references therein for proofs of convergences and convergence rates of online learning. In practice, a middle ground between passing through the entire dataset and sampling a single example is observed to perform better for machine learning tasks <ref type="bibr" target="#b62">[64]</ref>: we fix a batch size and in every iteration average over a batch of the samples of this size. We call this approach batch gradient descent (also called minibatch gradient descent for better contrast). The result is an update rule for the parameter vector of the form w t+1 ← w t + ∆ t where ∆ t is calculated as</p><formula xml:id="formula_16">∆ t = -η t ∇ t-1 ,</formula><p>for some step size η t , where ∇ t-1 = ∇ wt-1 h(w t-1 ) to simplify the notation. Here h is an approximation of h in (10) by the partial sum over the training batch. Finding a good schedule for η t can be a challenging engineering task that will be addressed in Section III A 5. Depending on the optimization landscape, SGD might require extremely large numbers of iterations for convergence. One way to improve the convergence rate of SGD is to add a momentum term <ref type="bibr" target="#b74">[76]</ref>:</p><formula xml:id="formula_17">∆ t = p∆ t-1 -η t ∇ t-1 .</formula><p>On the other hand, it is convenient to have the schedule of η t be determined through the training by a heuristic algorithm that adapts to the frequency of every event. The method AdaGrad was developed to allow much larger updates for infrequent samples <ref type="bibr" target="#b75">[77]</ref>:</p><formula xml:id="formula_18">∆ t = -diag η √ Σ ti + ε ∇ t-1 .</formula><p>Here Σ ti is the sum of the squares of all previous values of the i-th entry of the gradient. The quantity ε is a small (e.g. 10 -8 ) smoothening factor in order to avoid dividing by zero. The denominator in this formula is called the root mean squared (RMS). An important advantage of AdaGrad is the fact that the freedom in the choice of the step-size schedule is restricted to choosing one parameter η, which is called the learning rate.</p><p>Finally RMSProp is an improvement on AdaGrad in order to slow down the aggressive vanishing rate of the gradients in AdaGrad <ref type="bibr" target="#b76">[78]</ref>. This is achieved by adding a momentum term to the root mean squared:</p><formula xml:id="formula_19">diag(Σ t ) = p diag(Σ t-1 ) + (1 -p)∇ t-1 ∇ T t-1 .</formula><p>Hyperparameter tuning. From the above exposition, it is apparent that a machine learning framework involves many algorithms and design choices. The performance of the framework depends on optimal and consistent choices of the free parameters of each piece, the hyperparameters.</p><p>For example, while a learning rate of 10 -3 might be customary for a small dataset such as that of MNIST digit recognition, it might be a good choice for a small feedforward network and a bad choice for the RNN used in our problem scenario. In our case, the hyperparameters include the decay rate, the learning rate, the momentum in RMSProp, the number of hidden nodes in each layer of the network, the number of hidden layers and filters, and some categorical variables such as the activation function of each layer, the choice of having peepholes or not in the RNN.</p><p>It would be desirable if a metaheuristic can find appropriate choices of hyperparameters. The challenges are 1. Costly function evaluation: the only way to know if a set of hyperparameters is appropriate for the deep learning framework, is to run the deep learning algorithm with these parameters;</p><p>2. Lack of a gradient-based solution: the solution of the deep learning framework does not have a known functional dependence on the hyperparameters. Therefore, the metaheuristic has no knowledge of a steepest descent direction.</p><p>It is therefore required for the metaheuristic to be (1) sample efficient and (2) gradient-free. Having a good metaheuristic as such is extremely desirable, since:</p><p>1. The performance of the ML framework might be more sensitive to some parameters than to others. It is desirable for the metaheuristic to identify this.</p><p>2. Compatibility of the parameters: leaving the hypertuning job to a researcher can lead to search in very specific regimes of hyperparameters that are expected to be good choices individually but not in combination.</p><p>3. Objectivity of the result: a researcher might spend more time tuning the parameters of their proposal than on a competing algorithm. If the same metaheuristic is used to tune various networks, such as feedforward networks, RNNs and CNNs, the result would be a reliable comparison between all suggestions.</p><p>Bayesian optimization. Bayesian optimization <ref type="bibr" target="#b77">[79]</ref> is a nonlinear optimization algorithm that associates a surrogate model to its objective function and modifies it at every function evaluation. It then uses this surrogate model to decide which point to explore next for a better objective value <ref type="bibr" target="#b78">[80]</ref>. Bayesian optimization is a good candidate for hypertuning as it is sample efficient and can perform well for multi-modal functions without a closed formula. A disadvantage of Bayesian optimization to keep in mind is that it relies on design choices and parameters of its own that can affect its performance in a hyperparameter search.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Steane and Knill EC deep neural decoder for the CNOT-exRec</head><p>The simplest deep neural decoder for any dataset is a feedforward network with none or many hidden layers, each layer fully connected to the next one. The input layer receives the bit strings of X and Z syndromes. And the output layer corresponds to the X and Z recovery operators on the physical qubits of the code. Since multiple physical qubits might be used to encode a single logical operator, a better choice is for the output layer to encode whether an auxiliary (but efficient) decoding scheme is causing logical faults or not. The goal would be to predict such logical faults by the deep neural decoder and when the deep neural decoder predicts such a fault, we will impose a logical Pauli operator after the recovery suggested by the auxiliary decoder. The 1-hot encoding in two bits, 10 and 01, respectively stand for I and X for the X-errors, and it stands for I and Z for the Z errors.</p><p>From our early experiments it became apparent that it is beneficial to half separate X and Z neural networks that share a loss function, that is the sum of the soft-max cross entropies of the two networks. Fig. <ref type="figure" target="#fig_1">10</ref> shows the schematics of such a feedforward network.</p><p>The CNOT-exRec RNN. In the case of the CNOT-exRec, the leading EC rounds have temporal precedence to the trailing EC rounds. Therefore a plausible design choice for the deep neural decoder would be to employ an RNN with two iterations on the hidden cell. In the first iteration, the syndrome data from the leading EC rounds are provided and in the second iteration the syndrome data from the trailing EC rounds are provided. A demonstration of this network is given in Fig. <ref type="figure" target="#fig_1">11</ref>. The internal state of the first copy is initialized randomly and the internal state of the last copy is garbage-collected. The hidden state of the last copy of the LSTM unit is then fully connected to a hidden layer with user-defined activation function. This hidden unit is then fully connected to output nodes denoted by 01 and 10 which are respectively the one-hot encoding of the prediction as to whether an X-recovery or a Z-recovery operation is needed on the output qubits from exRec-CNOT. The loss function is the sum of the loss functions of the two networks. Without the red circuits, this neural network is called a simple LSTM unit. The red circuit is called peepholes. An LSTM cell with peepholes can outperform a simple LSTM cell in some tasks. There are four hidden layers with user-defined activation functions in an LSTM unit known as the forget layer (F), input layer (I), hidden layer (H) and the output layer (O). There are four 2 to 1 logical gates in the unit that depending on the sign written on them applies an element-wise operation between the vectors fed into the logical gates. There is also a 1 to 1 logical gate that applies an element-wise tanh function on its input vector. The internal state of an LSTM unit serves as the backbone of a sequence of replications of the LSTM unit. The roll of the internal state is to capture temporal features of the sequence of input data.</p><p>The hidden cell of the RNN may be an LSTM, or an LSTM with peepholes as shown in Fig. <ref type="figure" target="#fig_9">12</ref>. An LSTM cell consists of an internal state which is a vector in charge of carrying temporal information through the unrolling of the LSTM cell in time epochs. There are 4 hidden layers. The layer H is the 'actual' hidden layer including the input data of the current epoch with the previous hidden layer from the previous epoch. The activation of H is usually tanh. The 'input' layer I is responsible for learning to be a bottleneck on how important the new input is, and the 'forget' layer F is responsible for creating a bottleneck on how much to forget about the previous epochs. Finally the 'output' layer O is responsible for creating a bottleneck on how much data is passed through from the new internal state to the new hidden layer. The peepholes in Fig. <ref type="figure" target="#fig_9">12</ref> allow the internal state to also contribute in the hidden layers F , I and O.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Surface code deep neural decoder</head><p>Other than the multi-layer feedforward network of Fig. <ref type="figure" target="#fig_1">10</ref>, there are two other reasonable designs for a deep neural network when applied to the surface code.</p><p>The surface code RNN. In the fault-tolerant scheme of the rotated surface code, multiple rounds of error correction are done in a sequence as explained in Sec. II A. It is therefore encouraging to consider an RNN with inputs as syndromes of the consecutive EC rounds. The network looks similar to that of Fig. <ref type="figure" target="#fig_1">11</ref> except that the number of epochs is equal to the maximum number of EC rounds. In particular, the fault tolerant scheme for the distance-three rotated surface code consists of three EC rounds. In the case of the distance-five surface code, the maximum number of EC rounds through the algorithm of Sec. II A is six. If the rounds of EC stop earlier, then the temporal input sequence of syndrome strings is padded by repeating the last syndrome string. As an example, if after three rounds the fault tolerant scheme terminates, then the input syndromes of epochs three to six of the RNN are all identical and equal to the third syndrome string.</p><p>The surface code CNN. The errors, syndromes and recovery operators of the surface code are locally affected by each other. It is therefore suggestive to treat the syndromes of the surface code as a 2-dimensional array, the same way pixels of an image are treated in image processing tasks. The multiple rounds of EC would account for a sequence of such images, an animation. Therefore a 3-dimensional CNN appears to be appropriate. This means that the kernels of the convolutions are also 3dimensional, probing the animation along the two axes of each image and also along the third axis representative of time.</p><p>Through our test-driven design, it became obvious that treating the X and Z syndromes as channels of the same 3-dimensional input animation is not a good choice. Instead, the X and Z syndromes should be treated as disjoint inputs of disjoint networks which in the end contribute to the same loss function. Notice that in the case of the distance-five rotated surface code, the X network receives a 3D input of dimensions 3×4×6 and the Z network receives a 3D input of dimensions 4×3×6. To create edge features, the inputs were padded outwards symmetrically, i.e. with the same binary values as their adjacent bits. This changes the input dimensions to 4 × 5 × 6 and 5 × 4 × 6 respectively for the X and Z animations. Via similar experiments, we realized that two convolutional layers do a better job in capturing patterns in the syndromes data. The first convolutional layer is probed by a 3 × 3 × 3 kernel, and the second layer is probed by a 4 × 4 × 4 kernel. After convolutional layers, a fully connected feedforward layer with dropouts and relu activations is applied to the extracted features and then the softmax cross-entropy is measured. The schematic of such a neural network is depicted in Fig. <ref type="figure" target="#fig_10">13</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. Numerical experiments</head><p>In the experimental results reported in this section, multiple data sets were generated by various choices of physical error rates ranging between p = 1.0 × 10 -4 to p = 2.0 × 10 -3 . Every data set consisted of simulating the circuit-level depolarizing channel (see Section II for a detailed description of the noise model) for the corresponding circuit, and included the syndrome and re-sulting error bit strings in the data set. Note that the error strings are only used as part of the simulation to compute the vector b of logical faults. In an actual experiment, b would be given directly (see the discussion above Section III A). We excluded the cases were both the syndrome and error strings were all zeros. The simulation was continued until a target number of non-zero training samples were gathered. The target size of the training data set was chosen as 2 × 10 6 for distance-three codes, and as 2 × 10 7 for distance-five codes.</p><p>Hypertuning was performed with the help of BayesOpt <ref type="bibr" target="#b78">[80]</ref>. In every hypertuning experiment, each query consisted of a full round of training the deep learning network on 90% of the entire dataset and cross-validating on the remaining 10%. It is important to add randomness to the selection of the training and cross-validating data sets so that the hyperparameters do not get tuned for a fixed choice of data entries. To this aim, we uniformly randomly choose an initial element in the entire data set, take the 90% of the dataset starting from that initial element (in a cyclic fashion) as the training set, and the following 10% as the test dataset.</p><p>The cross-entropy of the test set is returned as the final outcome of one query made by the hypertuning engine. For all hypertuning experiments, 10 initial queries were performed via Latin hypercube sampling. After the initial queries, 50 iterations of hypertuning were performed.</p><p>For each fault-tolerant error correction scheme, hypertuning was performed on only a single data set (i.e. only for one of the physical error rates). A more meticulous investigation may consist of hypertuning for each individual physical error rate separately but we avoided that, since we empirically observed that the results are independent of the choice of hypertuning data set. At any rate, the data chosen for distance-three codes was the one corresponding to p = 4 × 10 -4 . For the distance-five rotated surface code, p = 6.0 × 10 -4 and for the 19-qubit color code using Steane and Knill-EC, p = 1.0 × 10 -3 were chosen for hypertuning.</p><p>Hyperparameters chosen from this step were used identically for training all other data sets. For every data set (i.e. every choice of physical fault rate p) the deep learning experiment was run 10 times and in the diagrams reported below the average and standard deviations are reported as points and error bars. In every one of the 10 runs, the training was done on 90% of a data set, and cross validation was done on the remaining 10%. All the machine learning experiments were implemented in Python 2.7 using TensorFlow 1.4 <ref type="bibr" target="#b79">[81]</ref> on top of CUDA 9.0 running installed on TitanXp and TitanV GPUs produced by NVIDIA <ref type="bibr" target="#b80">[82]</ref>.</p><p>All experiments are reported in Fig. <ref type="figure" target="#fig_3">14</ref>-Fig. <ref type="figure" target="#fig_4">25</ref>. Before continuing with detailed information on each experiment, we refer the reader to Table <ref type="table" target="#tab_0">I</ref> where we provide the largest ratios of the pseudo-thresholds obtained using a neural network decoder to pseudo-thresholds obtained from bare lookup table decoders of each fault-tolerant protocol considered in this paper.  parameter lower bound upper bound decay rate 0.0 1.0 -10 -6.0 momentum 0.0 1.0 -10 -6.0 learning rate 10 -5.0 10 -1.0 initial std 10 -3.0 10 -1.0 num hiddens 100 1000</p><p>TABLE II: Bayesian optimization parameters for the CNOT-exRec of the [ <ref type="bibr" target="#b5">[7,</ref><ref type="bibr">1,</ref><ref type="bibr" target="#b1">3]</ref>] code using Steane and Knill-EC and the distance-three rotated surface code. Here the decay rate, momentum and learning rate pertain to the parameters of RMSProp. The row 'initial std' refers to the standard deviation of the initial weights in the neural networks, the mean of the weights was set to zero. The initial biases of the neural networks were set to zero. The row 'num hiddens' refers to the number of hidden nodes in the layers of neural network. This parameter is optimized for each layer of the neural network independently (e.g. for a feedforward network consisting of 3 hidden layers, there are 3 numbers of hidden nodes to be tuned). For an RNN this number indicates the number of hidden nodes in every one of the 4 hidden layers of the LSTM unit (all of the same size).</p><p>Steane-EC CNOT-exRec for the [ <ref type="bibr" target="#b5">[7,</ref><ref type="bibr">1,</ref><ref type="bibr" target="#b1">3]</ref>] code. The considered continuous and integer hyperparameters are given in Table <ref type="table" target="#tab_0">II</ref>.</p><p>We also tuned over the categorical parameters of Table III. The categorical parameters are tuned via gridparameter values activation functions relu, tanh, sigmoid, identity numbers of hidden layers 0, 1, 2, . . . TABLE III: Categorical hyperparameters. Optimizations over activation functions was only performed for the distance-three Steane code. Since rectified linear units showed better results, we committed to this choice for all other error correction schemes. However, for the second categorical hyperparameter (the numbers of hidden layers), the search was performed for all error correction schemes separately and was stopped at the numbers of hidden layers where the improvements in the results discontinued.</p><p>search. We observed that for all choices of neural networks (feedforward networks with various numbers of hidden layers and recurrent neural networks with or without peepholes), the rectified linear unit in the hidden layers and identity for the last layer resulted in the best performance. We accepted this choice of activation functions in all other experiments without repeating a grid-search. Figs. 14 and 15 compare the performance of the feedforward and RNN decoders that respectively use the lookup table and naive-decoder as their underlying decoders, respectively referred to as LU-based deep neural decoders (LU-DND) and PE-based deep neural decoders (PE-DND). We use PE since naive-decoders correct by applying pure errors. We observe that softmax regression (i.e. zero hidden layers) is enough to get results on par with the lookup table method in the LU-based training method, this was not the case in the PE-based method. The RNNs perform well but they are outperformed by two-hidden-layer feedforward networks. Additional hidden layers improve the results in deep learning. However, since this is in expense for a cross-entropy optimization in higher dimensions, the training of deeper networks is significantly more challenging. This trade-off is the reason the feedforward networks improve up to two hidden layers, but passing to three and higher numbers of hidden layers gave worse results (not reported in these diagrams).</p><p>We finally observe that PE-DND with even a single hidden layer feedforward network is almost on par with the LU-DND with two hidden layers. This is an impressive result given the fact that a table of pure errors grows linearly in the number of syndromes, but a lookup table grows exponentially. We believe this is a result of the fact that logical faults are much more likely to occur when using recovery operators which only consist of products of pure-errors, the training sets are less sparse and therefore deep learning is able to capture more patterns for the classification task at hand.</p><p>Knill-EC CNOT-exRec for the [ <ref type="bibr" target="#b5">[7,</ref><ref type="bibr">1,</ref><ref type="bibr" target="#b1">3]</ref>] code. The hypertuning of continuous variables was done using the same bounds as in Table <ref type="table" target="#tab_0">II</ref>. Figs. <ref type="figure" target="#fig_5">16</ref> and<ref type="figure" target="#fig_1">17</ref> respectively show the results of LU-DND and PE-DND methods. The best results were obtained by feedforward networks with respectively 3 and 2 hidden layers, in both cases slightly outperforming RNNs.</p><p>Distance-three rotated surface code. Similar to the previous distance-three codes, we compared using RNNs with feedforward networks with multiple hidden layers. We observed that the feedfoward network with a single hidden layer achieves the best performance and RNNs do not improve the results. Also consistent with the distance-three CNOT-exRec results, the PE-based DND can perform as good as the LU-based one (and slightly improves upon it). Results of these experiments are reported in Figs. <ref type="figure" target="#fig_6">18</ref> and<ref type="figure" target="#fig_7">19</ref>.</p><p>Steane-EC CNOT-exRec for the [ <ref type="bibr" target="#b17">[19,</ref><ref type="bibr">1,</ref><ref type="bibr" target="#b3">5]</ref>] code. As the size of the input and output layers of DNNs grow, the ranges of the optimal hyperparameters change. For the distance-five Steane exRec circuit applied to the [ <ref type="bibr" target="#b17">[19,</ref><ref type="bibr">1,</ref><ref type="bibr" target="#b3">5]</ref>] color code, the considered hyperparameter ranges (allowing smaller orders of magnitudes for the initial weight standard deviations and much smaller learning rates) are given in Table <ref type="table" target="#tab_1">IV</ref>.</p><p>parameter lower bound upper bound decay rate 0.0 1.0 -10 -6.0 momentum 0.0 1.0 -10 -6.0 learning rate 10 -7.0 10 -3.0 initial std 10 -5.0 10 -3.0 num hiddens 100 1000 Figs. <ref type="bibr" target="#b18">20</ref> and 21 show that the PE-DNDs has a slightly harder time with pattern recognition compared to the LU-DNDs. Nevertheless, both methods significantly improve the pseudo-thresholds of the distance-five Steane-EC scheme, with no advantage obtained from using an RNN over a 2-hidden layer feedforward network. In both experiments, the 3-hidden layer feedforward networks also did not result any improvements.</p><p>Knill-EC CNOT-exRec for the [ <ref type="bibr" target="#b17">[19,</ref><ref type="bibr">1,</ref><ref type="bibr" target="#b3">5]</ref>] code. The hyperparameter ranges used for hypertuning were similar to those obtained for the Steane-EC CNOT-exRec applied to the [ <ref type="bibr" target="#b17">[19,</ref><ref type="bibr">1,</ref><ref type="bibr" target="#b3">5]</ref>] code. Given the effectiveness of the 2-hidden layer feedforward network, this feedforward neural network was chosen for the Knill exRec d = 5 experiment. We see a similar improvement on the pseudothreshold of the error correction scheme using either of LU-DND and PE-DND.</p><p>Distance-five rotated surface code. For rotated surface codes, we only considered numerical simulations using one EC rather than the full exRec. This choice was made to be consistent with previous analyses of the surface codes performance.</p><p>The hyperparameter ranges used for hypertuning the feedforward neural networks were chosen according to Table <ref type="table" target="#tab_2">V</ref>.</p><p>parameter lower bound upper bound decay rate 0.0 1.0 -10 -6.0 momentum 0.0 1.0 -10 -6.0 learning rate 10 -6.0 10 -2.0 initial std 10 -6.0 10 -2.0 num hiddens 100 1000 As explained in the previous section, a CNN engineered appropriately could be a viable layout design for large surface codes. Beside previous hyperparameters, we now also need to tune the number of filters, and drop-out rate. A summary of the settings for Bayesian optimization are given in Table <ref type="table" target="#tab_2">VI</ref>.</p><p>We compare the PE-based and LU-based feedforward networks with the CNN proposed in Section III C. Figs. <ref type="bibr">24 and 25</ref> show that feedforward networks with 2 hidden layers result in significant improvements both using the PE-based and LU-based DNDs. The 3D-CNN is slightly improving the results of the feedforward network in PE-DND but is only slightly better than the lookup table based method in the LU-DND case. The best overall performance is obtained by using a feedfoward network with 2 hidden layers for the LU-DND. A slightly less performant result can also be obtained if the PE-DND method is used in conjunction with either of the 2-hidden layer feedforward network or the 3D convolutional neural network. In Fig. <ref type="figure" target="#fig_3">14</ref>-Fig. <ref type="figure" target="#fig_7">19</ref> each data point has the height on the vertical axis being the average of 10 logical fault rates collected for each physical fault rate p specified on the horizontal axis. Error bars represent the standard deviation from these average values. For each DND-based decoder, the cuve-fitting method used is a non-linear least square fitting between the average logical fault rates as a function of the physical fault rates, and a quadratic monomial. In Fig. <ref type="figure" target="#fig_2">20</ref>-Fig. <ref type="figure" target="#fig_4">25</ref> data points, averages and error bars are obtaines in a similar fashion to Fig. <ref type="figure" target="#fig_3">14</ref>-Fig. <ref type="figure" target="#fig_7">19</ref>. The cuve-fitting method is also a non-linear least square method, this time fitting a cubic monomial through the data points.</p><p>parameter lower bound upper bound decay rate 0.0 1.0 -10 -6.0 momentum 0.0 1.0 -10 -6.0 learning rate 10 -6.0 10 -<ref type="foot" target="#foot_2">2</ref>.0 initial std 10 -6.0 10 -2.0 num hiddens 100 1000 keep rate 0.0 1.0 num filters <ref type="bibr">5 10</ref> TABLE VI: Bayesian optimization parameters for a 3-dimensional CNN. The filters were fixed to be 3 × 3 × 3 and 4 × 4 × 4 but their quantities were tuned. Since CNNs are larger and deeper than other networks considered in this paper, they are more prone to vanishing gradients. Therefore it is beneficial to consider drop-outs in the hidden layer after feature extraction. The hyperparameter corresponding to drop-outs is 'keep rate' allowing more drop-outs when it is smaller.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. Performance analysis</head><p>In this section we consider the efficiency of the deep neural decoders in comparison to the lookup table decoders described in Sections II A and II B. The size of a lookup table grows exponentially in the number of syndromes therefore making lookup table based decoding intractable as the codes grow. However, it is important to note that as long as the size of the lookup table allows for storage of the entire table in memory, as described in Section II E, the lookup from an array or a hash table happens effectively in O(1) time. Therefore a lookup table based decoding scheme would be the most efficient decoder by far. A similar approach to a lookup table decoder is possible by making an inference mapping from all the possible input strings of a trained neural decoder. This method is discussed in Section V A. For larger codes, neither a lookup table decoder, nor an inference mapping decoder is an option due to exponentially growing memory usage.</p><p>More complicated decoders such as minimum weight perfect matching can be extremely inefficient solutions for decoding despite polynomial asymptotic complexity. With gates operating at 100Mhz (that is 10ns gate times) <ref type="bibr" target="#b81">[83]</ref>, which is much faster than the state of the art<ref type="foot" target="#foot_0">1</ref> , the simplest quantum algorithms foreseen to run on near term devices would require days of runtime on the system <ref type="bibr" target="#b84">[86]</ref>. With the above gate times, the CNOT-exRec using Steane and Knill EC units as well as the multiple rounds of EC for surface codes would take as small as a hundred nanoseconds. In order to perform active error correction, we require classical decoding times to be implemented on (at worst) a comparable time scale as the EC units, and therefore merely a complexity theoretic analysis of a decoding algorithm is not enough for making it a viable solution. Alternatively, given a trained DND, inference of new recovery operations from it is a simple algorithm requiring a sequence of highly parallelizable matrix multiplications. We will discuss this approach in Section V B and Section V C.</p><p>A. Inference mapping from a neural decoder For codes of arbitrary size, the most time-performant way to use a deep neural decoder is to create an array of all inputs and outputs of the DNN in the test mode (i.e. an inference map which stores all possible syndromes obtained from an EC unit and assigns each combination to a recovery operator 2 ). This is possible for distance-three fault-tolerant EC schemes such as Steane, Knill and surface codes (as well as other topological schemes such as those used for color codes). For all these codes, the memory required to store the inference map is 2.10 megabytes. This method is not feasible for larger distance codes. For the Knill and Steane-EC schemes applied to the [ <ref type="bibr" target="#b17">[19,</ref><ref type="bibr">1,</ref><ref type="bibr" target="#b3">5]</ref>] color code, the memory required is 590 exabytes and for the distance-five rotated surface code it is 2.79 × 10 24 exabytes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Fast inference from a trained neural network</head><p>An advantage of a deep neural decoder is that the complications of decoding are to be dealt with in the training mode of the neural network. The trained network is then used to suggest recovery operations. The usage of the neural network in this passive step, i.e. without further training, is called the inference mode. Once the neural network is trained, usage of it in the inference mode requires only a sequence of few simple arithmetic operations between the assigned valued of its input nodes and the trained weights. This makes inference an extremely simple algorithm and therefore a great candidate for usage as a decoder while the quantum algorithm is proceeding.</p><p>However, even for an algorithm as simple as inference, further hardware and software optimization is required. For example, <ref type="bibr" target="#b26">[28]</ref> predicts that on an FPGA (field-programmable gate array) every inference from a single layer feedforward network would take as long as 800ns. This is with the optimistic assumption that floatpoint arithmetic (in 32 and 64-bit precision) takes 2.5 to 5 nanoseconds and only considering a single layer feedforward network.</p><p>In this section, we consider alternative optimization techniques for fast inference. We will consider a feedforward network with two hidden layers given their promising performance in our experiments.</p><p>Network quantization. Fortunately, quantum error correction is not the only place where fast inference is critical. Search engines, voice and speech recognition, image recognition, image tagging, and many more applications of machine learning are nowadays critical functions of smart phones and many other digital devices. As the usage grows, the need for efficient inference from the trained models of these applications grow. It is also convenient to move such inference procedures to the usage platforms (e.g. the users smart phones and other digital devices) than merely a cloud based inference via a data centre. Recent efforts in high performance computing has focused on fabricating ASICs (Application Specific Integrated Circuits) specifically for inference from neural networks. Google's TPU (Tensor Processing Unit) <ref type="bibr" target="#b85">[87]</ref> is being used for inference in Google Search, Google Photos and in DeepMind's AlphaGo against one of the the world's top Go player, Lee Sedol.</p><p>It is claimed that the reduction in precision of a trained neural network from 32-bit float point precision in weights, biases, and arithmetic operations, to only 8-bit fixed point preserves the quality of inference from trained models <ref type="bibr" target="#b86">[88]</ref>. This procedure is called network quantization. There is no mathematical reason to believe that the inference quality should hold up under network quantization. However, the intuitive explanation has been that although the training mode is very sensitive to small variations of parameters and hyperparameters, and fluctuations of the high precision weights of the network in individual iterations of training is very small, the resulting trained network is in principle robust to noise in data and weights.</p><p>The challenge in our case is that in quantum error correction, the input data is already at the lowest possible precision (each neuron attains 0 or 1, therefore only using a single bit). Furthermore, an error in the input neurons results in moving from one input syndrome to a completely different one (for instance, as opposed to moving from a high resolution picture to a low resolution, or poorly communicated one in an image processing task). We therefore see the need to experimentally verify whether network quantization is a viable approach to high-performance inference from a DND. Fig. <ref type="figure" target="#fig_11">26</ref> demonstrates an experiment to validate network quantization on a trained DND. Using 32-bit floatpoint precision, the results of Fig. <ref type="figure" target="#fig_3">14</ref> show that the trained DND improves the logical fault rate from 1.95 × 10 -4 obtained by lookup table methods to 9.45 × 10 -5 obtained by the LU-DND with 2 hidden layers. We observe that this improvement is preserved by the quantized networks with 8 bits and even 7 bits of precision using fix-point arithmetic.</p><p>We now explain how the quantized network for this experiment was constructed. Let us assume the available precision is up to k bits. First, the weights and biases of the network are rescaled and rounded to nearest integers such that the resulting parameters are all integers between -2 k-1 + 1 and 2 k-1 stored as signed k-bit integers. Each individual input neuron only requires a single bit since they store zeros and ones. But we also require that the result of feedforward obtained by multiplications and additions and stored in the hidden layers is also a kbit signed integer. Unlike float-point arithmetic, fixed point arithmetic operations can and often overflow. The result of multiplication of two k-bit fixed-point integers can span 2k bits in the worst case. Therefore the results of each hidden layer has to be shifted to a number of significant digits and the rightmost insignificant digits have to be forgotten. For instance, in the case of the CNOT-exRec with Steane EC units, each input layer has 12 bits, which get multiplied by 12 signed integers each with k-bit fixed point precision. A bias with k-bit fixed point precision is then added to the result. We therefore need at most k + log 2 (13) -bits to store the result. Therefore the rightmost log 2 (13) bits have to be forgotten. If the weights of the trained neural network are symmetric around zero, it is likely that only a shift to the right by 2 bits is needed in this case. Similarly, if each hidden layer has L nodes, the largest shift needed would be log 2 (L + 1) but most likely log 2 (L + 1) -1 shifts suffices. In the experiment of Fig. <ref type="figure" target="#fig_11">26</ref>, each hidden layer had 1000 nodes and the feedforward results were truncated in their rightmost 9 digits.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Classical arithmetic performance</head><p>In the previous section we showed that 8-bit fixed point arithmetic is all that is needed for high quality inference from the trained deep neural decoder. We now consider a customized digital circuit for the inference task and es-timate how fast the arithmetic processing units of this circuit have to be in order for the inference to be of practical use for active quantum error correction.</p><p>The runtime of a digital circuit is estimated by considering the time that is required for the electric signal to travel through the critical path of the logical circuit <ref type="bibr" target="#b87">[89]</ref>, the path with the longest sequence of serial digital operations.</p><p>Fig. <ref type="figure" target="#fig_2">27</ref> shows the critical path of a circuit customized to carry inference in a feedforward network with 2 hidden layers. Since the input neurons represent syndrome bits, multiplying them with the first set of weights can be done with parallel AND between the syndrome bit and the weight bits. The rectified linear unit is efficient since it only requires a NAND between the sign of the 8-bit signed integer with the other 7 bits of it. The most expensive units in this circuit are the 8 × 8 multipliers and adders. Every 8 × 8 multiplier gives a 16-bit fixed point integer which is then shifted 8-bits to the right by ignoring the first 8-bits. The total time delay t TOT of this path in the circuit is</p><formula xml:id="formula_20">t TOT = t AND + log(S + 1) t ADD + t MAX + H i=1 (t NOT + t AND + t MULT + log(L i + 1) t ADD ) (12)</formula><p>where H is the number of hidden layers and L i is the number of neurons in the i-th hidden layer. From a complexity theoretic point of view this is promising since it shows that the cost of inference is logarithmic in the number of syndromes and the size of hidden layers, and linear in the number of hidden layers. For a feedforward network with two hidden layers and at most 1000 neurons in each hidden layer, t TOT = 3 t AND + 2 t NOT + 2 t MULT + t MAX + ( log(S + 1) + 20) t ADD . <ref type="bibr" target="#b11">(13)</ref> Since the adders contribute the most in the above time delay, let us give an upper bound on how fast the adder units need to be in order for the total time delay to be comparable to the runtime of the fault-tolerant quantum error correction protocols considered in this paper.</p><p>In Table <ref type="table" target="#tab_6">VII</ref> we compute upper bounds on the adder units for the fault-tolerant error correction protocols considered in this paper. We emphasize that this estimation is by the optimistic assumption that all independent arithmetic operations are done in parallel. In reality, this is not possible due to limitations in area and power consumption of the ASIC. Also considering that multiple rounds of inference have to happen, a pipeline architecture should be considered for independent batches of inference on the ASIC. Lastly, the time for multiplier unit and the comparator are ignored since (if all independent jobs are done in parallel) there are only two serial multipliers in the critical path. With all of these considerations, the last row of this table should be interpreted FIG. <ref type="figure" target="#fig_2">27:</ref> The critical path of a custom inference circuit. Every syndrome bit represents an input node of the neural network and is multiplied by 8-bit integer weights. A set of such products are added together and together with an 8-bit bias integer to find the activation on a node of the first hidden layer. Given S input syndromes, this amounts to the addition of S + 1 integers which can be done with a tree of 8-bit integer full-adders (Full-Adder Tree or FAT for short) of depth log(S + 1). After the quantized rectified linear unit, a similar procedure is iterated for the first hidden layer with the full-adder tree of depth log(L1 + 1) where L1 is the number of neurons in the first hidden layer. This pattern continues for other hidden layers. The MAX unit compares two 8-bit integers and outputs 0 if the first one is bigger and 1 if the second one is bigger.  For Steane and Knill EC, this is the depth of the CNOT-exRec circuit (excluding the ancilla verification steps) and in the surface code, it is the depth of the circuit for multiple rounds of syndrome measurement (note that for the distance 5 surface code we considered the worst case of 6 syndrome measurement rounds). The syndrome size is only that of one of X and Z since the inference for X and Z logical errors can happen in parallel and independently. The adder time leniency is calculated based on 10ns quantum gate delays. Therefore, it is the depth of the FTEC multiplied by 10ns and divided by the number of adders.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>FTEC</head><p>as an optimistic allowed time for the adder units and that the actual adder delays should be well below these numbers.</p><p>In particular we conclude that in order to perform active error correction with the methods summarized in Table VII on a quantum computer with 10ns gate delays, the classical control unit of the quantum computer has to comprise of arithmetic units that are fast enough to perform arithmetic operations well below the time limits reported in the last column of this table. In hardware engineering, there are many approaches to implementation of arithmetic and logical units <ref type="bibr" target="#b88">[90]</ref>. Without going into the details of the circuit designs we mention that the adder leniencies in Table VII are in reach of high performance VLSI <ref type="bibr" target="#b89">[91,</ref><ref type="bibr" target="#b90">92]</ref>, but could be challenging to achieve using FPGAs <ref type="bibr" target="#b91">[93]</ref><ref type="bibr" target="#b92">[94]</ref><ref type="bibr" target="#b93">[95]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Limitations of deep neural decoders</head><p>We interpret the results of this section to suggest that, once implemented on a high performance computing platform, inference can be computed efficiently from a trained deep neural decoder. Further, the results of Section IV show that with a large enough training set, neural network decoders achieve lower logical failure rates compared to the lookup table schemes presented in this paper. However, this does not imply that deep neural decoders are scalable. As the size of the codes grow, training the neural decoders becomes much more daunting. This is due to the fact that deep learning classifiers are not viable solutions for sparse classification problems. As the codes become better and/or physical error rates become smaller, the training samples become more and more sparse, providing less and less effective training samples for the neural network. Without nontrivial training samples, the neural networks learn "zeros" rather than capturing significant patterns in the data set.</p><p>As evidence for the effect of sparsity of the dataset on successful training of the deep neural decoding we refer the reader to an experiment reported in Fig. <ref type="figure" target="#fig_6">28</ref>. In this experiment, the DND is trained on the dataset corresponding to the highest physical fault rate p = 2 × 10 -3 . The same trained DND is used to cross-validate on test datasets for all other physical fault rates. We observe that this DND is more successful in recovery inference for smaller physical error rates, even though it is trained on a "wrong" dataset. It is important to note that this experiment does not provide an improved method for training a neural network for error correction on a physical realization of a quantum processor. Firstly, in any manufactured quantum device the error model will not be entirely known (and is not necessarily close to a theoretic noise model such as the depolarizing channel). And secondly, the error of the device cannot be intensified intentionally for the purpose of training a deep neural decoder, to be later used on a less noisy device.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. Conclusion</head><p>To conclude, the main contributions of this paper were considering multiple fault-tolerant schemes and using several neural network architectures to train decoders in a full circuit-level noise framework. Although our analysis was done for Pauli channels, we expect that for non-Pauli noise models, the improvements could be even more significant than what was observed in our work. Evidence of this can be found in <ref type="bibr" target="#b19">[21]</ref> where decoders were adapted to non-Pauli noise channels.</p><p>From a machine learning point of view, we applied state-of-the-art techniques used in training neural networks. While considering many network designs, we used Logical fault rate Look up table (4.91e + 05p 3 ) PE-FF2 (2.12e + 05p 3 ) PE-FF2-cross-trained (2.08e + 05p 3 ) FIG. <ref type="figure" target="#fig_6">28:</ref> A comparison between two training procedures for the CNOT-exRec of the [ <ref type="bibr" target="#b17">[19,</ref><ref type="bibr">1,</ref><ref type="bibr" target="#b3">5]</ref>] color code using Steane-EC units. The orange dots are the results of training a feedforward network with 2 hidden layers as reported also in Fig. <ref type="figure" target="#fig_2">20</ref>. In this case, the DND is trained on a given physical error rate p and tested on a test dataset for the same physical error rate. We observe that the logical error rate does not exactly follow a cubic growth since the training is less successful when the physical error rate is small. The green line, demonstrates the performance of the same DND if trained only for the largest physical error rate p = 2 × 10 -3 , and later on tested on test datasets from every other physical error rate. The neural network captured syndrome and recovery patterns occurring in the CNOT-exRec that are valid for all values of physical error rate. As previously explained, such a training scenario is not possible for real-world experiments, or on physical realizations of quantum computers.</p><p>the same hyperparameter tuning methodology to achieve unbiased and reliable results. Consequently, we successfully observed a clear advantage in using deep networks in comparison with single hidden layer networks and regression methods. On the other hand, we provided clear evidence of the realistic limitations of deep learning in low noise rate regimes. In particular, scaling the neural network to large distance codes appears to be a significant challenge. For large scale quantum computations, decoders that work less well than neural decoders trained on small distance codes but which are scalable would clearly be the better option. Lastly, we gave a rigorous account of the digital hardware resources needed for inference and runtime analysis of the critical path of the customized digital circuitry for high performance inference.</p><p>There remains many interesting future directions for designing improved and efficient decoders which work well in fault-tolerant regimes. One such avenue would be to tailor machine learning algorithms specifically designed for decoding tasks. In particular, finding machine learning algorithms which work well with sparse data would be of critical importance. It would also be interesting to apply the methods introduced in this work to actual quantum devices that are currently being developed. It most certainly will be the case that fault-tolerant designs will be tailored to a particular quantum architecture. This would lead to further areas in which machine learning could be extremely useful for finding improved decoders.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>group containing n-fold tensor products of the identity I and Pauli matrices X, Y and Z. The weight of an error E ∈ P (1) n (wt(E)) is the number of non-identity Pauli operators in its decomposition. For example, if E = IXY IZ, then wt(E) = 3.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>FIG. 1 :</head><label>1</label><figDesc>FIG. 1: Illustration of an extended rectangle (exRec) for a logical CNOT gate. The EC box consists of performing a round of fault-tolerant error correction. The error correction rounds prior to applying the logical CNOT gate are referred to as leading-EC's (LEC) and the error correction rounds after the CNOT are referred to as trailing-EC's (TEC).</figDesc><graphic coords="4,83.24,52.08,186.60,116.40" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>FIG. 2 :</head><label>2</label><figDesc>FIG.2: Illustration of the d = 5 rotated surface code. Data qubits are located at the white circles and the ancilla qubits used to measure the stabilizers are located on the black circles of the lattice. Green squares measure the Z stabilizers and red squares measure X stabilizers.</figDesc><graphic coords="4,351.20,52.08,176.70,177.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>FIG. 4 :</head><label>4</label><figDesc>FIG. 4: Circuits for measuring X and Z stabilizers in Steane-EC. The circuit in Fig. 4a measures bit-flip errors whereas the circuit in Fig. 4b measures phase-flip errors. Note that the first block consists of the data qubits encoded in a CSS code. The states |0 and |+ represent logical |0 and |+ states encoded in the same CSS code used to protect the data.</figDesc><graphic coords="6,178.08,52.08,111.78,116.95" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>FIG. 5 :</head><label>5</label><figDesc>FIG. 5: Full Steane error correction circuit. Each line represents encoded data qubits and all CNOT gates and measurements are performed transversally. The circuits used to prepare the encoded |+ and |0 are in general not fault-tolerant. Consequently, extra "verifier" ancilla states are used to detect errors arising during the preparation of |+ and |0 . If the verifier states measure a non-trivial syndrome or the -1 eigenvalue of a logical Pauli is measured, the ancilla states are rejected and new ancilla states are brought in until they pass the verification step.</figDesc><graphic coords="6,358.54,52.08,162.03,172.92" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>FIG. 6 :</head><label>6</label><figDesc>FIG. 6: Fig. 6a is a representation of the [[7, 1, 3]] Steane code. The qubits are located at the white circles of the lattice. Each face corresponds to both a X ⊗4 and Z ⊗4 stabilizer. Fig. 6b is a representation of the [[19, 1, 5]] color code. Like the Steane code, each face corresponds to an X and Z type stabilizer. Notice that there are three weight-six stabilizers of each type.</figDesc><graphic coords="7,56.94,229.23,239.20,238.60" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>FIG. 8 :</head><label>8</label><figDesc>FIG. 8: Knill error correction circuit. As with Steane-EC, all CNOT gates and measurements are performed transversally. The logical |0 and |+ states are also encoded using the same code that protects the data. A transversal CNOT gate is applied between them to form a logical Bell state. The operator Q is used to complete the teleportation protocol of the logical state as well as to correct errors which were on the original data block.</figDesc><graphic coords="7,345.95,52.08,187.20,106.80" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>FIG. 9 :</head><label>9</label><figDesc>FIG.9: Full CNOT-exRec circuit using Knill error correction. Each Pauli operator Q1, Q2, Q3 and Q4 is used to correct errors in the initial data blocks as well as the complete teleportation protocol of the logical Bell measurement.</figDesc><graphic coords="8,317.01,53.07,263.44,223.11" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>FIG. 10 :FIG. 11 :</head><label>1011</label><figDesc>FIG.10: Schematics of a feedforward network consisting of disjoint X and Z networks. There may be none, one or multiple hidden layers with different activation functions. The output layers correspond to logical I-and X-errors for the X network and to logical I-and Z-errors for the Z network. The activation function of the last layer before the error layer is the identity since in the softmax cross entropy loss function, the activation (by softmax) is already included.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>FIG. 12 :</head><label>12</label><figDesc>FIG.12: Schematics of a long-short term memory (LSTM) cell. Without the red circuits, this neural network is called a simple LSTM unit. The red circuit is called peepholes. An LSTM cell with peepholes can outperform a simple LSTM cell in some tasks. There are four hidden layers with user-defined activation functions in an LSTM unit known as the forget layer (F), input layer (I), hidden layer (H) and the output layer (O). There are four 2 to 1 logical gates in the unit that depending on the sign written on them applies an element-wise operation between the vectors fed into the logical gates. There is also a 1 to 1 logical gate that applies an element-wise tanh function on its input vector. The internal state of an LSTM unit serves as the backbone of a sequence of replications of the LSTM unit. The roll of the internal state is to capture temporal features of the sequence of input data.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>FIG. 13 :</head><label>13</label><figDesc>FIG. 13: Schematics of a deep neural decoder for the distance-five rotated surface code. The network consists of two disjoint neural networks contributing to the same loss function via softmax cross entropy. Each neural network consists of two layers of 3D CNNs. The first layer consists of a number of filters, each filter performing a convolution of a 3 × 3 × 3 kernel by the input syndromes. The second 3D CNN layer uses 4 × 4 × 4 kernels. The colored boxes demonstrate how each layer is padded in order for the size of the 3D layers to be preserved. When the kernel dimension is even for instance, the padding from the top and left are of size 1, and the padding from the bottom and right are of size 2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>FIG. 26 :</head><label>26</label><figDesc>FIG. 26: Quantization of the feedforward neural network with 2 hidden layers, trained on the Steane EC dataset at a physical error rate of p = 2 × 10 -4 . Each point is calculated as the average logical error rate obtained from 10 rounds of training and cross-validating similar to the experiments in Section IV.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I :</head><label>I</label><figDesc>Pseudo-thresholds for the 6 fault-tolerant error correction protocols considered in the experiments. The second column corresponds to the highest pseudo-thresholds obtained from a bare lookup table decoder whereas the third column gives the highest pseudo-thresholds using neural network decoders. The last column corresponds to the ratio between the pseudo-thresholds obtained from the best neural network decoders and the lookup table decoders.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE IV :</head><label>IV</label><figDesc>Bayesian optimization parameters for d = 5 Steane and Knill CNOT-exRecs. Given the larger size of the training sets and longer input strings, for these datasets, smaller orders of magnitudes for the initial weight standard deviations and much smaller learning rates were explored.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE V :</head><label>V</label><figDesc>Bayesian optimization parameters for the distance-five rotated surface code. The parameter search is in a slightly tighter domain than in the case of the distance-five Knill and Steane CNOT-exRecs in view of the empirical initial tests performed.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>FIG. 14: LU-DND for the distance-three Steane CNOT-exRec. FIG. 15: PE-DND for the distance-three Steane CNOT-exRec. FIG. 16: LU-DND for the distance-three Knill CNOT-exRec. FIG. 17: PE-DND for the distance-three Knill CNOT-exRec. FIG. 18: LU-DND for the distance-three surface code. FIG. 19: PE-DND for the distance-five surface code.</figDesc><table><row><cell>Logical fault rate</cell><cell>10 4 10 3</cell><cell>Look up table (4.76e + 03p 2 ) LU-FF0 (4.94e + 03p 2 ) LU-FF1 (2.52e + 03p 2 ) LU-FF2 (2.51e + 03p 2 ) LU-RNN (2.51e + 03p 2 )</cell><cell>Logical fault rate</cell><cell>10 4 10 3</cell><cell>Look up table (4.76e + 03p 2 ) PE-FF1 (2.59e + 03p 2 ) PE-RNN (2.55e + 03p 2 ) PE-FF2 (2.54e + 03p 2 )</cell></row><row><cell></cell><cell>10 4</cell><cell>2 × 10 4 Physical fault rate 3 × 10 4 4 × 10 4</cell><cell>6 × 10 4</cell><cell>10 4</cell><cell>2 × 10 4 Physical fault rate 3 × 10 4 4 × 10 4</cell><cell>6 × 10 4</cell></row><row><cell>Logical fault rate</cell><cell>10 3</cell><cell>Look up table (5.68e + 03p 2 ) LU-FF0 (5.88e + 03p 2 ) LU-FF1 (4.77e + 03p 2 ) LU-RNN (4.64e + 03p 2 ) LU-FF2 (4.64e + 03p 2 )</cell><cell>Logical fault rate</cell><cell>10 3</cell><cell>Look up table (5.68e + 03p 2 ) PE-RNN (4.56e + 03p 2 ) PE-FF2 (4.50e + 03p 2 )</cell></row><row><cell></cell><cell>10 4</cell><cell></cell><cell></cell><cell>10 4</cell><cell></cell></row><row><cell></cell><cell>10 4</cell><cell>2 × 10 4 Physical fault rate 3 × 10 4 4 × 10 4</cell><cell>6 × 10 4</cell><cell>10 4</cell><cell>2 × 10 4 Physical fault rate 3 × 10 4 4 × 10 4</cell><cell>6 × 10 4</cell></row><row><cell>Logical fault rate</cell><cell>10 4 10 3</cell><cell>Look up table (3.89e + 03p 2 ) LU-FF0 (3.66e + 03p 2 ) LU-FF1 (3.19e + 03p 2 ) LU-FF2 (3.28e + 03p 2 ) LU-RNN (3.41e + 03p 2 )</cell><cell>Logical fault rate</cell><cell>10 4 10 3</cell><cell>Look up table (3.89e + 03p 2 ) PE-RNN (3.59e + 03p 2 ) PE-FF1 (3.14e + 03p 2 ) PE-FF2 (3.19e + 03p 2 )</cell></row><row><cell></cell><cell>10 4</cell><cell>2 × 10 4 Physical fault rate 3 × 10 4 4 × 10 4</cell><cell>6 × 10 4</cell><cell>10 4</cell><cell>2 × 10 4 Physical fault rate 3 × 10 4 4 × 10 4</cell><cell>6 × 10 4</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>FIG. 20: LU-DND for the distance-five Steane CNOT-exRec. FIG. 21: PE-DND for the distance-five Steane CNOT-exRec. FIG. 22: LU-DND for the distance-five Knill CNOT-exRec. FIG. 23: PE-DND for the distance-five Knill CNOT-exRec. FIG. 24: LU-DND for the distance-five surface code. FIG. 25: PE-DND for the distance-five surface code.</figDesc><table><row><cell></cell><cell></cell><cell cols="3">Look up table (4.91e + 05p 3 ) LU-RNN (2.80e + 05p 3 ) LU-FF2 (2.61e + 05p 3 )</cell><cell></cell><cell></cell><cell cols="2">Look up table (4.91e + 05p 3 ) PE-RNN (2.23e + 05p 3 ) PE-FF2 (2.12e + 05p 3 )</cell></row><row><cell>Logical fault rate</cell><cell>10 3</cell><cell></cell><cell></cell><cell></cell><cell>Logical fault rate</cell><cell>10 3</cell><cell></cell></row><row><cell></cell><cell>10 4</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>10 4</cell><cell></cell></row><row><cell></cell><cell></cell><cell>6 × 10 4</cell><cell></cell><cell>10 3 Physical fault rate</cell><cell>2 × 10 3</cell><cell></cell><cell>6 × 10 4</cell><cell>10 3 Physical fault rate</cell><cell>2 × 10 3</cell></row><row><cell></cell><cell></cell><cell cols="3">Look up table (5.53e + 05p 3 ) LU-FF2 (4.21e + 05p 3 )</cell><cell></cell><cell></cell><cell cols="2">Look up table (5.53e + 05p 3 ) PE-FF2 (4.17e + 05p 3 )</cell></row><row><cell>Logical fault rate</cell><cell>10 3</cell><cell></cell><cell></cell><cell></cell><cell>Logical fault rate</cell><cell>10 3</cell><cell></cell></row><row><cell></cell><cell>10 4</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>10 4</cell><cell></cell></row><row><cell></cell><cell></cell><cell>6 × 10 4</cell><cell></cell><cell>10 3 Physical fault rate</cell><cell>2 × 10 3</cell><cell></cell><cell>6 × 10 4</cell><cell>10 3 Physical fault rate</cell><cell>2 × 10 3</cell></row><row><cell></cell><cell>10 3</cell><cell cols="3">Look up table (2.95e + 06p 3 ) LU-3D-CNN (2.93e + 06p 3 ) LU-RNN (2.00e + 06p 3 ) LU-FF2 (1.98e + 06p 3 )</cell><cell></cell><cell>10 3</cell><cell cols="2">Look up table (2.95e + 06p 3 ) PE-RNN (4.12e + 06p 3 ) PE-FF2 (2.26e + 06p 3 ) PE-3D-CNN (2.18e + 06p 3 )</cell></row><row><cell>Logical fault rate</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Logical fault rate</cell><cell></cell><cell></cell></row><row><cell></cell><cell>10 4</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>10 4</cell><cell></cell></row><row><cell></cell><cell></cell><cell>3 × 10 4</cell><cell>4 × 10 4</cell><cell>Physical fault rate</cell><cell>6 × 10 4</cell><cell></cell><cell>3 × 10 4</cell><cell>4 × 10 4</cell><cell>Physical fault rate</cell><cell>6 × 10 4</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE VII :</head><label>VII</label><figDesc>FTEC depth is the depth of the FTEC circuit.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>In fact, existing prototypes of quantum computers have much longer gate delays. Typical gate times in a superconducting system are 130ns for single-qubit</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="250" xml:id="foot_1"><p>and 250 -450ns for 2-qubit gates. For a trapped-ion system, gate times are even longer, reaching 20µs for single-qubit gates and 250µs for 2-qubit gates<ref type="bibr" target="#b82">[84,</ref><ref type="bibr" target="#b83">85]</ref>.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_2"><p>For the CNOT-exRec, the inference map would map syndromes from all four EC units to a recovery operator. For the surface code, the inference map would map syndromes measured in each round to a recovery operator.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VII. Acknowledgements</head><p>Both authors contributed equally to this work. We acknowledge Steve G. Weiss for providing the necessary computing resources.</p><p>The authors would also like to thank Ben Criger, Raymond Laflamme, Thomas O'Brien, Xiaotong Ni, Barbara Terhal, Giacomo Torlai, Tomas Jochym-O'Connor, Aleksander Kubica and Ehsan Zahedinejad for useful discussions. C.C. acknowledges the support of NSERC through the PGS D scholarship. P.R. acknowledges the support of the government of Ontario and Innovation, Science and Economic Development Canada.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>[ <ref type="bibr" target="#b5">[7,</ref><ref type="bibr">1,</ref><ref type="bibr" target="#b1">3]</ref>] Steane code</p><p>[ <ref type="bibr" target="#b7">[9,</ref><ref type="bibr">1,</ref><ref type="bibr" target="#b1">3]</ref>] (Surface-17) code [ <ref type="bibr" target="#b17">[19,</ref><ref type="bibr">1,</ref><ref type="bibr" target="#b3">5]</ref>] color code [ <ref type="bibr" target="#b23">[25,</ref><ref type="bibr">1,</ref><ref type="bibr" target="#b3">5]</ref>] (Surface-49) code g </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">IBM Q Experience</title>
		<ptr target="https://quantumexperience.ng.bluemix.net/qx/devices" />
		<imprint>
			<biblScope unit="page" from="2018" to="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Riggeti QPU</title>
		<ptr target="http://pyquil.readthedocs.io/en/latest/qpu_overview.html" />
		<imprint>
			<biblScope unit="page" from="2018" to="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">A blueprint for demonstrating quantum supremacy with superconducting qubits</title>
		<author>
			<persName><forename type="first">C</forename><surname>Neill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Roushan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kechedzhi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Boixo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">V</forename><surname>Isakov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Smelyanskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Barends</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Burkett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Chiaro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Dunsworth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Fowler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Foxen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Graff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Jeffrey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kelly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Lucero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Megrant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mutus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Neeley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Quintana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Sank</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vainsencher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wenner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">C</forename><surname>White</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Neven</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Martinis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017-09">Sept. 2017</date>
		</imprint>
	</monogr>
	<note>ArXiv eprints</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Is error detection helpful on ibm 5q chips ?</title>
		<author>
			<persName><forename type="first">C</forename><surname>Vuillot</surname></persName>
		</author>
		<idno>arXiv:quant-ph/1705.08957</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Good quantum errorcorrecting codes exist</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">R</forename><surname>Calderbank</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">W</forename><surname>Shor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Phys. Rev. A</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="page" from="1098" to="1105" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Enlagement of calderbank-shor-steane quantum codes</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">W</forename><surname>Steane</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE. Trans.Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="2492" to="2495" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Fault-tolerant postselected quantum computation: schemes</title>
		<author>
			<persName><forename type="first">E</forename><surname>Knill</surname></persName>
		</author>
		<idno type="arXiv">arXiv:quant-ph/0402171</idno>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Quantum computing with realistically noisy devices</title>
		<author>
			<persName><forename type="first">E</forename><surname>Knill</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">434</biblScope>
			<biblScope unit="issue">7029</biblScope>
			<biblScope unit="page" from="39" to="44" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Surface codes: Towards practical large-scale quantum computation</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Fowler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mariantoni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Martinis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">N</forename><surname>Cleland</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Phys. Rev. A</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="page">32324</biblScope>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Quantum error correction with only two extra qubits</title>
		<author>
			<persName><forename type="first">R</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">W</forename><surname>Reichardt</surname></persName>
		</author>
		<idno>arXiv:quant- ph/1705.02329</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Fault-tolerant quantum computation with few qubits</title>
		<author>
			<persName><forename type="first">R</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">W</forename><surname>Reichardt</surname></persName>
		</author>
		<idno>arXiv:quant- ph/1705.05365</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Flag faulttolerant error correction with arbitrary distance codes</title>
		<author>
			<persName><forename type="first">C</forename><surname>Chamberland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Beverland</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Quantum</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">53</biblScope>
			<date type="published" when="2018-02">Feb. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Threshold accuracy for quantum computation</title>
		<author>
			<persName><forename type="first">E</forename><surname>Knill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Laflamme</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">H</forename><surname>Zurek</surname></persName>
		</author>
		<idno type="arXiv">arXiv:quant-ph/9610011</idno>
		<imprint>
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Using concatenated quantum codes for universal fault-tolerant quantum gates</title>
		<author>
			<persName><forename type="first">T</forename><surname>Jochym-O'connor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Laflamme</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Phys. Rev. Lett</title>
		<imprint>
			<biblScope unit="volume">112</biblScope>
			<biblScope unit="page">10505</biblScope>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Universal faulttolerant quantum computation with only transversal gates and error correction</title>
		<author>
			<persName><forename type="first">A</forename><surname>Paetznick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">W</forename><surname>Reichardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Phys. Rev. Lett</title>
		<imprint>
			<biblScope unit="volume">111</biblScope>
			<biblScope unit="page">90505</biblScope>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Faulttolerant conversion between the steane and reed-muller quantum codes</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Duclos-Cianci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Poulin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Phys. Rev. Lett</title>
		<imprint>
			<biblScope unit="volume">113</biblScope>
			<biblScope unit="page">80501</biblScope>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Dimensional jump in quantum error correction</title>
		<author>
			<persName><forename type="first">H</forename><surname>Bombín</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.5079</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Universal fault-tolerant gates on concatenated stabilizer codes</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">J</forename><surname>Yoder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Takagi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">L</forename><surname>Chuang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Phys. Rev. X</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">31039</biblScope>
			<date type="published" when="2016-09">Sep 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">The surface code with a twist</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">J</forename><surname>Yoder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">H</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Quantum</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2017-04">Apr. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Hard decoding algorithm for optimizing thresholds under general markovian noise</title>
		<author>
			<persName><forename type="first">C</forename><surname>Chamberland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Wallman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Beale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Laflamme</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Phys. Rev. A</title>
		<imprint>
			<biblScope unit="volume">95</biblScope>
			<biblScope unit="page">42332</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Tensor-network simulations of the surface code under realistic noise</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Darmawan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Poulin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Phys. Rev. Lett</title>
		<imprint>
			<biblScope unit="volume">119</biblScope>
			<biblScope unit="page">40502</biblScope>
			<date type="published" when="2017-07">Jul 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">An efficient general decoding algorithm for the surface code</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Darmawan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Poulin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.01879</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Paths, trees, and flowers</title>
		<author>
			<persName><forename type="first">J</forename><surname>Edmonds</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Canadian Journal of mathematics</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="449" to="467" />
			<date type="published" when="1965">1965</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Topological code autotune</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Fowler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Whiteside</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Mcinnes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rabbani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Phys. Rev. X</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">41003</biblScope>
			<date type="published" when="2012-10">Oct 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Optimal and efficient decoding of concatenated quantum block codes</title>
		<author>
			<persName><forename type="first">D</forename><surname>Poulin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Phys. Rev. A</title>
		<imprint>
			<biblScope unit="volume">74</biblScope>
			<biblScope unit="page">52333</biblScope>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Neural decoder for topological codes</title>
		<author>
			<persName><forename type="first">G</forename><surname>Torlai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">G</forename><surname>Melko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Phys. Rev. Lett</title>
		<imprint>
			<biblScope unit="volume">119</biblScope>
			<biblScope unit="page">30501</biblScope>
			<date type="published" when="2017-07">Jul 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Decoding small surface codes with feedforward neural networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Varsamopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Criger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Bertels</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Quantum Science and Technology</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">15004</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Machine-learning-assisted correction of correlated qubit errors in a topological code</title>
		<author>
			<persName><forename type="first">P</forename><surname>Baireuther</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">E</forename><surname>O'brien</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Tarasinski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">W J</forename><surname>Beenakker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Quantum</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">48</biblScope>
			<date type="published" when="2018-01">Jan. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Scalable neural network decoders for higher dimensional quantum codes</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">P</forename><surname>Breuckmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Ni</surname></persName>
		</author>
		<idno>arXiv:quant-ph/1710.09489</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<author>
			<persName><forename type="first">N</forename><surname>Maskara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kubica</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Jochym-O'connor</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.08680</idno>
		<title level="m">Advantages of versatile neural-network decoding for topological codes</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Quantum accuracy threshold for concatenated distance-3 codes</title>
		<author>
			<persName><forename type="first">P</forename><surname>Aliferis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Gottesman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Preskill</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Quant. Inf. Comput</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="97" to="165" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Stabilizer Codes and Quantum Error Correction</title>
		<author>
			<persName><forename type="first">D</forename><surname>Gottesman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1997">1997</date>
		</imprint>
		<respStmt>
			<orgName>California Institute of Technology</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">An introduction to quantum error correction and fault-tolerant quantum computation</title>
		<author>
			<persName><forename type="first">D</forename><surname>Gottesman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Symposia in Applied Mathematics</title>
		<meeting>Symposia in Applied Mathematics</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="volume">68</biblScope>
			<biblScope unit="page" from="13" to="58" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Accuracy threshold for postselected quantum computation</title>
		<author>
			<persName><forename type="first">P</forename><surname>Aliferis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Gottesman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Preskill</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Quant. Inf. Comput</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="181" to="244" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">The heisenberg representation of quantum computers, talk at</title>
		<author>
			<persName><forename type="first">D</forename><surname>Gottesman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Group Theoretic Methods in Physics</title>
		<imprint>
			<publisher>Citeseer</publisher>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Quantum codes on a lattice with boundary</title>
		<author>
			<persName><forename type="first">S</forename><surname>Bravyi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kitaev</surname></persName>
		</author>
		<idno type="arXiv">arXiv:quant-ph/9811052</idno>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Topological quantum memory</title>
		<author>
			<persName><forename type="first">E</forename><surname>Dennis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kitaev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Landhal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Preskill</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Mathematical Physics</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="page" from="4452" to="4505" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Fault-tolerant quantum computation by anyons</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kitaev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals of Physics</title>
		<imprint>
			<biblScope unit="volume">303</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="2" to="30" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Low-distance surface codes under realistic quantum noise</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Tomita</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">M</forename><surname>Svore</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Phys. Rev. A</title>
		<imprint>
			<biblScope unit="volume">90</biblScope>
			<biblScope unit="page">62320</biblScope>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Quantum orders in an exact soluble model</title>
		<author>
			<persName><forename type="first">X.-G</forename><surname>Wen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Phys. Rev. Lett</title>
		<imprint>
			<biblScope unit="volume">90</biblScope>
			<biblScope unit="page">16803</biblScope>
			<date type="published" when="2003-01">Jan 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Minimum weight perfect matching of fault-tolerant topological quantum error correction in average o(1) parallel time</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Fowler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Quantum Info. Comput</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="145" to="158" />
			<date type="published" when="2015-01">Jan. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Towards practical classical processing for the surface code: Timing analysis</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Fowler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Whiteside</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">C L</forename><surname>Hollenberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Phys. Rev. A</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="page">42313</biblScope>
			<date type="published" when="2012-10">Oct 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Fast decoders for topological quantum codes</title>
		<author>
			<persName><forename type="first">G</forename><surname>Duclos-Cianci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Poulin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Phys. Rev. Lett</title>
		<imprint>
			<biblScope unit="volume">104</biblScope>
			<biblScope unit="page">50504</biblScope>
			<date type="published" when="2010-02">Feb 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Fault-tolerant renormalization group decoder for abelian topological codes</title>
		<author>
			<persName><forename type="first">G</forename><surname>Duclos-Cianci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Poulin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Quant. Inf. Comput</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="721" to="0740" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Quantum self-correction in the 3d cubic code model</title>
		<author>
			<persName><forename type="first">S</forename><surname>Bravyi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Haah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Phys. Rev. Lett</title>
		<imprint>
			<biblScope unit="volume">111</biblScope>
			<biblScope unit="page">200501</biblScope>
			<date type="published" when="2013-11">Nov 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">High threshold error correction for the surface code</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Wootton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Loss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Phys. Rev. Lett</title>
		<imprint>
			<biblScope unit="volume">109</biblScope>
			<biblScope unit="page">160503</biblScope>
			<date type="published" when="2012-10">Oct 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Almost-linear time decoding algorithm for topological codes</title>
		<author>
			<persName><forename type="first">N</forename><surname>Delfosse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">H</forename><surname>Nickerson</surname></persName>
		</author>
		<idno>arXiv:quant- ph/1709.06218</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Conrad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Chamberland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">P</forename><surname>Breuckmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">M</forename></persName>
		</author>
		<idno>arXiv:quant-ph/1712.07666</idno>
		<title level="m">The small stellated dodecahedron code and friends</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Active stabilization, quantum computation, and quantum state synthesis</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">W</forename><surname>Steane</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Phys. Rev. Lett</title>
		<imprint>
			<biblScope unit="volume">78</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page">2252</biblScope>
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Quantum error correction for quantum memories</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">M</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Reviews of Modern Physics</title>
		<imprint>
			<biblScope unit="volume">87</biblScope>
			<biblScope unit="page">307</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Fault-tolerant quantum computing in the Pauli or Clifford frame with slow error diagnostics</title>
		<author>
			<persName><forename type="first">C</forename><surname>Chamberland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Iyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Poulin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Quantum</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">43</biblScope>
			<date type="published" when="2018-01">Jan. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Effective fault-tolerant quantum computation with slow measurements</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Divincenzo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Aliferis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Phys. Rev. Lett</title>
		<imprint>
			<biblScope unit="volume">98</biblScope>
			<biblScope unit="page">20501</biblScope>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Fault-tolerant ancilla preparation and noise threshold lower bounds for the 23-qubit golay code</title>
		<author>
			<persName><forename type="first">A</forename><surname>Paetznick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">W</forename><surname>Reichardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Quant. Inf. Compt</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="1034" to="1080" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Thresholds for universal concatenated quantum codes</title>
		<author>
			<persName><forename type="first">C</forename><surname>Chamberland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Jochym-O'connor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Laflamme</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Phys. Rev. Lett</title>
		<imprint>
			<biblScope unit="volume">117</biblScope>
			<biblScope unit="page">10501</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Overhead analysis of universal concatenated quantum codes</title>
		<author>
			<persName><forename type="first">C</forename><surname>Chamberland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Jochym-O'connor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Laflamme</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Phys. Rev. A</title>
		<imprint>
			<biblScope unit="volume">95</biblScope>
			<biblScope unit="page">22313</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Topological quantum distillation</title>
		<author>
			<persName><forename type="first">H</forename><surname>Bombin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Martin-Delgado</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Phys. Rev. Lett</title>
		<imprint>
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="page">180501</biblScope>
			<date type="published" when="2006-10">Oct 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">On universal and fault-tolerant quantum computing: A novel basis and a new constructive proof of universality for shor&apos;s basis</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">O</forename><surname>Boykin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pulver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Roychowdhury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Vatan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Foundations of Computer Science, 1999. 40th Annual Symposium on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="486" to="494" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Fault-tolerant quantum computation for local leakage faults</title>
		<author>
			<persName><forename type="first">P</forename><surname>Aliferis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">M</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Quant. Inf. Comput</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="139" to="156" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Anyon computers with smaller groups</title>
		<author>
			<persName><forename type="first">C</forename><surname>Mochon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Phys. Rev. A</title>
		<imprint>
			<biblScope unit="volume">69</biblScope>
			<biblScope unit="page">32306</biblScope>
			<date type="published" when="2004-03">Mar 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Efficient simulation of quantum error correction under coherent error based on the nonunitary free-fermionic formalism</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Suzuki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Fujii</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Koashi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Phys. Rev. Lett</title>
		<imprint>
			<biblScope unit="volume">119</biblScope>
			<biblScope unit="page">190503</biblScope>
			<date type="published" when="2017-11">Nov 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<ptr target="http://www.deeplearningbook.org" />
		<title level="m">Deep Learning</title>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><surname>Bishop</surname></persName>
		</author>
		<title level="m">Pattern Recognition and Machine Learning. Information science and statistics</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">B</forename><surname>Orr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">R</forename><surname>Müller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Efficient BackProp</title>
		<imprint>
			<biblScope unit="page" from="9" to="50" />
			<date type="published" when="1998">1998</date>
			<publisher>Springer</publisher>
			<pubPlace>Berlin, Heidelberg; Berlin Heidelberg</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Rectified linear units improve restricted boltzmann machines</title>
		<author>
			<persName><forename type="first">V</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International Conference on International Conference on Machine Learning, ICML&apos;10, (USA)</title>
		<meeting>the 27th International Conference on International Conference on Machine Learning, ICML&apos;10, (USA)</meeting>
		<imprint>
			<publisher>Omnipress</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="807" to="814" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Understanding the difficulty of training deep feedforward neural networks</title>
		<author>
			<persName><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics</title>
		<editor>
			<persName><forename type="first">Y</forename><forename type="middle">W</forename><surname>Teh</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Titterington</surname></persName>
		</editor>
		<meeting>the Thirteenth International Conference on Artificial Intelligence and Statistics<address><addrLine>Sardinia, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Chia Laguna Resort</publisher>
			<date type="published" when="2010-05">May 2010</date>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="13" to="15" />
		</imprint>
	</monogr>
	<note>Proceedings of Machine Learning Research</note>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">An application of recurrent neural networks to discriminative keyword spotting</title>
		<author>
			<persName><forename type="first">S</forename><surname>Fernández</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 17th International Conference on Artificial Neural Networks, ICANN&apos;07</title>
		<meeting>the 17th International Conference on Artificial Neural Networks, ICANN&apos;07<address><addrLine>Berlin, Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="220" to="229" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International Conference on Neural Information Processing Systems</title>
		<meeting>the 27th International Conference on Neural Information Processing Systems<address><addrLine>Cambridge, MA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
	<note>NIPS&apos;14</note>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<title level="m" type="main">Exploring the limits of language modeling</title>
		<author>
			<persName><forename type="first">R</forename><surname>Jozefowicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
		<title level="m" type="main">Multilingual Language Processing From Bytes</title>
		<author>
			<persName><forename type="first">D</forename><surname>Gillick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Brunk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Subramanya</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015-11">Nov. 2015</date>
		</imprint>
	</monogr>
	<note>ArXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Comput</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997-11">Nov. 1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Multi-column deep neural networks for image classification</title>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), CVPR &apos;12</title>
		<meeting>the 2012 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), CVPR &apos;12<address><addrLine>Washington, DC, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="3642" to="3649" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">ImageNet Large Scale Visual Recognition Challenge</title>
		<author>
			<persName><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision (IJCV)</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Robust stochastic approximation approach to stochastic programming</title>
		<author>
			<persName><forename type="first">A</forename><surname>Nemirovski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Juditsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Shapiro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. on Optimization</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="1574" to="1609" />
			<date type="published" when="2009-01">Jan. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Large scale online learning</title>
		<author>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">L</forename><surname>Cun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">S</forename><surname>Thrun</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><forename type="middle">K</forename><surname>Saul</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</editor>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="217" to="224" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Parallel distributed processing: Explorations in the microstructure of cognition</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">E</forename><surname>Rumelhart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ch. Learning Internal Representations by Error Propagation</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="318" to="362" />
			<date type="published" when="1986">1986</date>
			<publisher>MIT Press</publisher>
			<pubPlace>Cambridge, MA, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<monogr>
		<title level="m" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<editor>EMNLP</editor>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<monogr>
		<title level="m" type="main">Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude</title>
		<author>
			<persName><forename type="first">T</forename><surname>Tieleman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
	<note>tech. rep.</note>
</biblStruct>

<biblStruct xml:id="b77">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Mockus</surname></persName>
		</author>
		<title level="m">Bayesian approach to global optimization: theory and applications. Mathematics and its applications</title>
		<imprint>
			<publisher>Kluwer Academic</publisher>
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
	<note>Soviet series</note>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Bayesopt: A bayesian optimization library for nonlinear optimization, experimental design and bandits</title>
		<author>
			<persName><forename type="first">R</forename><surname>Martinez-Cantin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="3915" to="3919" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<monogr>
		<title level="m" type="main">TensorFlow: Largescale machine learning on heterogeneous systems</title>
		<author>
			<persName><forename type="first">M</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Brevdo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Citro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Harp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Jozefowicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kudlur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Levenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Mané</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Monga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Olah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Talwar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Viégas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Warden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wattenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wicke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zheng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note>Software available from tensorflow.org</note>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Scalable parallel programming with cuda</title>
		<author>
			<persName><forename type="first">J</forename><surname>Nickolls</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Buck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Garland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Skadron</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Queue</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="40" to="53" />
			<date type="published" when="2008-03">Mar. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Quantum information processing with superconducting circuits: a review</title>
		<author>
			<persName><forename type="first">G</forename><surname>Wendin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Reports on Progress in Physics</title>
		<imprint>
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page">106001</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">"ibm</forename><surname>Qiskit</surname></persName>
		</author>
		<ptr target="https://github.com/QISKit/ibmqx-backend-information/tree/master/backends/ibmqx3" />
		<imprint>
			<biblScope unit="page" from="2018" to="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Experimental comparison of two quantum computing architectures</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">M</forename><surname>Linke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Maslov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Roetteler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Debnath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Figgatt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">A</forename><surname>Landsman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Wright</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Monroe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">114</biblScope>
			<biblScope unit="issue">13</biblScope>
			<biblScope unit="page" from="3305" to="3310" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">Elucidating reaction mechanisms on quantum computers</title>
		<author>
			<persName><forename type="first">M</forename><surname>Reiher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Wiebe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">M</forename><surname>Svore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wecker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Troyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Science</title>
		<imprint>
			<biblScope unit="volume">114</biblScope>
			<biblScope unit="page" from="7555" to="7560" />
			<date type="published" when="2017-07">July 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">P</forename><surname>Jouppi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Patil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Patterson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Bajwa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bates</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bhatia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Boden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Borchers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Boyle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>-L. Cantin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Coriell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Daley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Dau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Gelb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Vazir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ghaemmaghami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Gottipati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Gulland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">R</forename><surname>Hagmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hogberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Hundt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hurt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ibarz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jaffey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jaworski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Khaitan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lacy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Laudon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Law</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Leary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Lundin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mackean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Maggiore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Mahony</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Nagarajan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Narayanaswami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Nix</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Norrie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Omernick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Penukonda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Phelps</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Salek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Samadiani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Severn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sizikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Snelham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Souter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Steinberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Swing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Thorson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Toma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Tuttle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Walter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">H</forename><surname>Wilcox</surname></persName>
		</author>
		<author>
			<persName><surname>Yoon</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017-04">Apr. 2017</date>
		</imprint>
	</monogr>
	<note>In-Datacenter Performance Analysis of a Tensor Processing Unit,&quot; ArXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">Improving the speed of neural networks on cpus</title>
		<author>
			<persName><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">Z</forename><surname>Mao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Deep Learning and Unsupervised Feature Learning Workshop</title>
		<imprint>
			<date type="published" when="2011">2011. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<monogr>
		<title level="m" type="main">Digital Integrated Circuits</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Rabaey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Chandrakasan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Nikolic</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
			<publisher>Prentice Hall Press</publisher>
			<biblScope unit="volume">3</biblScope>
			<pubPlace>Upper Saddle River, NJ, USA</pubPlace>
		</imprint>
	</monogr>
	<note>rd ed.</note>
</biblStruct>

<biblStruct xml:id="b88">
	<monogr>
		<title level="m" type="main">Computer Arithmetic: Algorithms and Hardware Designs</title>
		<author>
			<persName><forename type="first">B</forename><surname>Parhami</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000">2000</date>
			<publisher>Oxford University Press</publisher>
			<pubPlace>Oxford, UK</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<analytic>
		<title level="a" type="main">Approaching a nanosecond: a 32 bit adder</title>
		<author>
			<persName><forename type="first">G</forename><surname>Bewick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">D</forename><surname>Micheli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Flynn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings 1988 IEEE International Conference on Computer Design: VLSI</title>
		<meeting>1988 IEEE International Conference on Computer Design: VLSI</meeting>
		<imprint>
			<date type="published" when="1988-10">Oct 1988</date>
			<biblScope unit="page" from="221" to="226" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<analytic>
		<title level="a" type="main">A sub-nanosecond 0.5 /spl mu/m 64 b adder design</title>
		<author>
			<persName><forename type="first">S</forename><surname>Naffziger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">1996 IEEE International Solid-State Circuits Conference</title>
		<imprint>
			<date type="published" when="1996-02">Feb 1996</date>
			<biblScope unit="page" from="362" to="363" />
		</imprint>
		<respStmt>
			<orgName>ISSCC</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<monogr>
		<title level="m" type="main">FPGA-Based System Design</title>
		<author>
			<persName><forename type="first">W</forename><surname>Wolf</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004">2004</date>
			<publisher>Prentice Hall PTR</publisher>
			<pubPlace>Upper Saddle River, NJ, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b92">
	<monogr>
		<title level="m" type="main">Fpga adders: Performance evaluation and optimal design</title>
		<author>
			<persName><forename type="first">S</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Yu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b93">
	<analytic>
		<title level="a" type="main">Highperformance carry chains for fpga&apos;s</title>
		<author>
			<persName><forename type="first">S</forename><surname>Hauck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Hosler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">W</forename><surname>Fry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Very Large Scale Integration (VLSI) Systems</title>
		<imprint>
			<date type="published" when="2000-04">April 2000</date>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="138" to="147" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
