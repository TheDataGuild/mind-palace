<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Neural Decoder for Topological Codes</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2017-05-09">May 9, 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Giacomo</forename><surname>Torlai</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Roger</forename><forename type="middle">G</forename><surname>Melko</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Department of Physics and Astronomy</orgName>
								<orgName type="institution">University of Waterloo</orgName>
								<address>
									<postCode>N2L 3G1</postCode>
									<region>Ontario</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Perimeter Institute for Theoretical Physics</orgName>
								<address>
									<postCode>N2L 2Y5</postCode>
									<settlement>Waterloo</settlement>
									<region>Ontario</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">A Neural Decoder for Topological Codes</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2017-05-09">May 9, 2017</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1103/PhysRevLett.119.030501</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-10-09T23:23+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present an algorithm for error correction in topological codes that exploits modern machine learning techniques. Our decoder is constructed from a stochastic neural network called a Boltzmann machine, of the type extensively used in deep learning. We provide a general prescription for the training of the network and a decoding strategy that is applicable to a wide variety of stabilizer codes with very little specialization. We demonstrate the neural decoder numerically on the well-known two dimensional toric code with phase-flip errors.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Introduction: Much of the success of modern machine learning stems from the flexibility of a given neural network architecture to be employed for a multitude of different tasks. This generalizability means that neural networks can have the ability to infer structure from vastly different data sets with only a change in optimal hyperparameters. For this purpose, the machine learning community has developed a set of standard tools, such as fully-connected feed forward networks <ref type="bibr" target="#b0">[1]</ref> and Boltzmann machines <ref type="bibr" target="#b1">[2]</ref>. Specializations of these underlie many of the more advanced algorithms, including convolutional networks <ref type="bibr" target="#b2">[3]</ref> and deep learning <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5]</ref>, encountered in real-world applications such as image or speech recognition <ref type="bibr" target="#b5">[6]</ref>.</p><p>These machine learning techniques may be harnessed for a multitude of complex tasks in science and engineering <ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b8">[9]</ref><ref type="bibr" target="#b9">[10]</ref><ref type="bibr" target="#b10">[11]</ref><ref type="bibr" target="#b11">[12]</ref><ref type="bibr" target="#b12">[13]</ref><ref type="bibr" target="#b13">[14]</ref><ref type="bibr" target="#b14">[15]</ref><ref type="bibr" target="#b15">[16]</ref><ref type="bibr" target="#b16">[17]</ref>. An important application lies in quantum computing. For a quantum logic operation to succeed, noise sources which lead to decoherence in a qubit must be mitigated. This can be done through some type of quantum error correction -a process where the logical state of a qubit is encoded redundantly so that errors can be corrected before they corrupt it <ref type="bibr" target="#b17">[18]</ref>. A leading candidate for this is the implementation of fault-tolerant hardware through surface codes, where a logical qubit is stored as a topological state of an array of physical qubits <ref type="bibr" target="#b18">[19]</ref>. Random errors in the states of the physical qubits can be corrected before they proliferate and destroy the logical state. The quantum error correction protocols that perform this correction are termed "decoders", and must be implemented by classical algorithms running on conventional computers <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b20">21]</ref>.</p><p>In this paper we demonstrate how one of the simplest stochastic neural networks for unsupervised learning, the restricted Boltzmann machine <ref type="bibr" target="#b21">[22]</ref>, can be used to construct a general error-correction protocol for stabilizer codes. Give a syndrome, defined by a measurement of the end points of an (unknown) chain of physical qubit errors, we use our Boltzmann machine to devise a protocol with the goal of correcting errors without corrupting the logical bit. Our decoder works for generic degenerate stabilizers codes that have a probabilistic relation between syndrome and errors, which does not have to be a priori known. Importantly, it is very simple to implement, requiring no specialization regarding code locality, dimension, or structure. We test our decoder numerically on a simple two-dimensional surface code with phase-flip errors.</p><p>The 2D Toric Code. Most topological codes can be described in terms of the stabilizer formalism <ref type="bibr" target="#b22">[23]</ref>. A stabilizer code is a particular class of error-correcting code characterized by a protected subspace C defined by a stabilizer group S. The simplest example is the 2D toric code, first introduced by Kitaev <ref type="bibr" target="#b23">[24]</ref>. Here, the quantum information is encoded into the homological degrees of freedom, with topological invariance given by the first homology group <ref type="bibr" target="#b24">[25]</ref>. The code features N qubits placed on the links of a L × L square lattice embedded on a torus. The stabilizers group is S = { Ẑp , Xv }, where the plaquette and vertex stabilizers are defined respectively as Ẑp = Given a reference state |ψ 0 ∈ C, let us consider the simple phase-flip channel described by a Pauli operator where σz is applied to each qubit with probability p err . This operator can be efficiently described by a mapping between the links and Z 2 , called an error chain e, whose boundary is called a syndrome S(e). In a experimental implementation, only the syndrome (and not the error chain) can be measured. Error correction (decoding) consists of applying a recovery operator whose chain r generates the same syndrome, S(e) = S(r). The recovery succeeds only if the combined operation is described by a cycle (i.e. a chain with no boundaries) e ⊕ r that belongs to the trivial homology class h 0 , describing contractible loops on the torus. On the other hand, if the cycle belongs to a non-trivial homology class (being noncontractible on the torus), the recovery operation directly manipulates the encoded logical information, leading to a logical failure (Fig <ref type="figure" target="#fig_0">1</ref>).</p><p>Several decoders have been proposed for the 2D toric code, based on different strategies <ref type="bibr" target="#b25">[26]</ref><ref type="bibr" target="#b26">[27]</ref><ref type="bibr" target="#b27">[28]</ref><ref type="bibr" target="#b28">[29]</ref><ref type="bibr" target="#b29">[30]</ref>. Maximum likelihood decoding consists of finding a recovery chain r with the most likely homology class <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b31">32]</ref>. A different recovery strategy, designed to reduce computational complexity, consists of generating the recovery chain r compatible with the syndrome simply by using the minimum number of errors. Such a procedure, called Minimum Weight Perfect Matching <ref type="bibr" target="#b32">[33]</ref> (MWPM), has the advantage that can be performed without the knowledge of the error probability p err . This algorithm is however sub-optimal (with lower threshold probability <ref type="bibr" target="#b24">[25]</ref>) since it does not take into account the high degeneracy of the error chains given a syndrome.</p><p>The Neural Decoder. Neural networks are commonly used to extract features from raw data in terms of probability distributions. In order to exploit this for error correction, we first build a dataset made of error chains and their syndromes D = {e, S}, and train a neural network to model the underlying probability distribution p data (e, S). Our goal is to then generate error chains to use for the recovery. We use a generative model called a Boltzmann machine, a powerful stochastic neural network widely used in the pre-training of the layers of deep neural networks <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b34">35]</ref>. The network architecture features three layers of stochastic binary neurons, the syndrome layer S ∈ {0, 1} N/2 , the error layer e ∈ {0, 1} N , and one hidden layer h ∈ {0, 1} n h (Fig. <ref type="figure" target="#fig_2">2</ref>). Symmetric edges connect both the syndrome and the error layer with the hidden layer. We point out the this network is equivalent to a traditional bilayer restricted Boltzmann machine, where we have here divided the visible layer into two separate layers for clarity. The weights on the edges connecting the network layers are given by the matrices U and W . Moreover, we also add external fields b, c and d coupled to the every neuron in each layer. The probability distribution that the probabilistic model associates to this graph structure is the Boltzmann distribution <ref type="bibr" target="#b35">[36]</ref> </p><formula xml:id="formula_0">p λ (e, S, h) = 1 Z λ e -E λ (e,S,h)<label>(1)</label></formula><p>where Z λ = Tr {h,S,e} e -E λ (e,S,h) is the partition function, λ = {U , W , b, c, d} is the set of parameters of the model, and the energy is</p><formula xml:id="formula_1">E λ (e, S, h) = - ik U ik h i S k - ij W ij h i e j + - j b j e j - i c i h i - k d k S k .<label>(2)</label></formula><p>The joint probability distribution over (e, S) is obtained after integrating out the hidden variables from the full distribution We now discuss the decoding algorithm, which proceeds assuming that we successfully learned the distribution p λ (e, S). Given an error chain e 0 with syndrome S 0 we wish to use the Boltzmann machine to generate an error chain compatible with S 0 to use for the recovery. To achieve this goal we separately train networks on different datasets obtained from different error regimes p err . Assuming we know the error regimes that generated e 0 , the recovery procedure consists of sampling a recovery chain from the distribution p λ (e | S 0 ) given by the network trained at the same probability p err of e 0 . Although the Boltzmann machine does not learn this distribution directly, by sampling the error and hidden layers while keeping the syndrome layer fixed to S 0 , since p λ (e, S 0 ) = p λ (e | S 0 )p(S 0 ), we are enforcing sampling from the desired conditional distribution. An advantage of this procedure over decoders that employ conventional Monte Carlo <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b28">29]</ref> on specific stabilizer codes is that specialized sampling algorithms tied to the stabilizer structure, or multi-canonical methods such as parallel tempering, are not required. Finally, note that the assumption of perfect learning is not critical, since the above sampling routine can be modified with an extra rejection step as discussed in Ref. <ref type="bibr" target="#b13">[14]</ref> to ensure sampling occurs from the proper physical distribution.</p><formula xml:id="formula_2">p λ (e, S) = h p λ (e, S, h) = 1 Z λ e -E λ (e,S)<label>(3)</label></formula><p>An error correction procedure can be defined as follows (Alg. 1): we first initialize the machine into a random state of the error and hidden layers (see Fig. <ref type="figure" target="#fig_2">2</ref>) and to S 0 for the syndrome layer. We then let the machine equilibrate by repeatedly performing block Gibbs sampling. After some amount of equilibration steps, we begin checking the syndrome of the error state e in the machine and, as soon as S(e) = S 0 we select it for the recovery operation. If such a condition is not met before a fixed amount of sampling steps, the recovery attempt is stopped and considered failed. This condition makes the precise computational requirements of the algorithm ill-defined, since the cut-off time can always be increased resulting in better performance for a higher computational cost. Results. We train neural networks in different error regimes by building several datasets D p = {e k , S k } M k=1 at elementary error probabilities p = {0.5, 0.6, . . . , 0.15} of the phase-flip channel. For a given error probability, the network hyper-parameters are individually optimized via a grid search (for details see the Supplementary Material). Once training is complete, we perform decoding following the procedure laid out in Alg. 1. We generate a test set T p = {e k } M k=1 and for each error chain e k ∈ T p , after a suitable equilibration time (usually N eq ∝ 10 2 sampling steps), we collect the first error chain e compatible with the original syndrome, S(e) = S(e k ). We use this error chain for the recovery, r (k) = e. Importantly, error recovery with r (k) chosen from the first compatible chain means that the cycle e k + r (k) is sampled from a distribution that includes all homology classes. By computing the Wilson loops on the cycles we can measure their homology class. This allows us to gauge the accuracy of the decoder in term of the logical failure probability, defined as P f ail = n f ail M where n f ail is the number of cycles with non-trivial homology. Because of the fully-connected architecture of the network, and the large complexity of the probability distribution arising from the high degeneracy of error chains given a syndrome, we found that the dataset size required to accurately capture the underlying statistics must be relatively large (|D p | ∝ 10 5 ). In Fig. <ref type="figure" target="#fig_5">3</ref> we plot the logical failure probability P f ail as a function of the elementary error probability for the neural decoding scheme. We note that at low p err , our logical failure probabilities follow the expected <ref type="bibr" target="#b36">[37]</ref> scaling form p L/2 err (not plotted). To compare our numerical results we also perform error correction using the recovery scheme given by MWPM <ref type="bibr" target="#b37">[38]</ref>. This algorithm creates a graph whose vertices corresponds to the syndrome and the edges connect each vertex with a weight equal to the Manhattan distance (the number of links connecting the vertices in the original square lattice). MWPM then finds an optimal matching of all the vertices pairwise using the minimum weight, which corresponds to the minimum number of edges in the lattice <ref type="bibr" target="#b38">[39]</ref>. Fig. <ref type="figure" target="#fig_5">3</ref> displays the comparison between a MWPM decoder (line) and our neural decoder (markers). As is evident, the neural decoder has an almost identical logical failure rate for error proba-bilities below the threshold (p err ≈ 10.9 <ref type="bibr" target="#b24">[25]</ref>), yet a significant higher probability above. Note that by training the Boltzmann machine on different datasets we have enforced in the neural decoder a dependence on the error probability. This is in contrast to MWPM which is performed without such knowledge. Another key difference is that the distributions learned by the Boltzmann machine contain the entropic contribution from the high degeneracy of error chains, which is directly encoded into the datasets. It will be instructive to explore this further, to determine whether the differences in Fig. <ref type="figure" target="#fig_5">3</ref> come from inefficiencies in the training, the different decoding model of the neural network, or both. Finite-size scaling on larger L will allow calculation of the threshold defined by the neural decoder.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 Neural Decoding Strategy</head><p>In the above algorithm, which amounts to a simple and practical implementation of the neural decoder, our choice to use the first compatible chain for error correction means that the resulting logical operation is sampled from a distribution that includes all homology classes. This is illustrated in Fig. <ref type="figure">4</ref>, where we plot the histogram of the homology classes for several different elementary error probabilities. Accordingly, our neural decoder can easily be modified to perform Maximum Likelihood (ML) optimal decoding. For a given syndrome, instead of obtaining only one error chain to use in decoding, one could sample many error chains and build up the histogram of homology classes with respect to any reference error state. Then, choosing the recovery chain from the largest histogram bin will implement, by definition, ML decoding. Although the computational cost of this procedure will clearly be expensive using the current fullyconnected restricted Boltzmann machine, it would be interesting to explore specializations of the neural network architecture in the future to see how its performance may compare to other ML decoding algorithms <ref type="bibr" target="#b30">[31]</ref> Conclusions. We have presented a decoder for topological codes using a simple algorithm implemented with a restricted Boltzmann machine, a common neural network used in many machine learning applications. Our neural decoder is easy to program using standard machine learning software libraries and training techniques, and relies on the efficient sampling of error chains distributed over all homology classes. Numerical results show that our decoder has a logical failure probability that is close to MWPM, but not identical, a consequence of our neural network being trained separately at different elementary error probabilities. This leads to the natural question of the relationship between the neural decoder and optimal decoding, which could be explored further by a variation of our algorithm that implements maximum likelihood decoding.</p><p>In its current implementation, the Boltzmann machine is restricted within a given layer of neurons, but fullyconnected between layers. This means that our decoder does not depend on the specific geometry used to im- plement the code, nor on the structure of the stabilizer group; it is trained simply using a raw data input vector, with no information on locality or dimension. Such a high degree of generalizability, which is one of the core advantages of this decoder, also represents a challenge for investigating bigger systems. For example, a bottleneck in our scheme to decode larger sizes is finding an error chain compatible with the syndrome within a reasonable cut-off time.</p><formula xml:id="formula_3">h 0 h 1 h 2 h 3 h 0 h 1 h 2 h 3</formula><p>In order to scale up our system sizes on the 2D toric code (as required e.g. to calculate the threshold), one could relax some of the general fully-connected structure of the network, and specialize it to accommodate the specific details of the code. Geometric specialization such as this has been explicitly demonstrated to improve the representational efficiency of neural networks in the case of the toric code <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b12">13]</ref>. This specialization should be explored in detail, before comparisons of computational efficiency can be made between our neural decoder, MWPM, and other decoding schemes. Note that, even with moderate specialization, the neural decoder as we have presented above can immediately be extended to other choices of error models <ref type="bibr" target="#b39">[40]</ref>, such as the more realistic case of imperfect syndrome measurement <ref type="bibr" target="#b40">[41]</ref>, or transferred to other topological stabilizer codes, such as color codes <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b42">43]</ref>. We also point out that the training of the networks are performed off-line and have to be carried out only once. As such, the high computational cost of the training need not be considered when evaluating the decoder computational efficiency for any of these examples.</p><p>Finally, it would be interesting to explore the improvements in performance obtained by implementing standard tricks in machine learning, such as convolutions, adaptive optimization algorithms, or the stacking of multiple Boltzmann machines into a network with deep structure. Given the rapid advancement of machine learning technology within the world's information industry, we expect that such tools will be the obvious choice for the real-world implementation of decoding schemes on future topologically fault-tolerant qubit hardware.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>ẐFIG. 1 .</head><label>1</label><figDesc>FIG. 1. Several operations on a 2D toric code. Logical operators Ẑ(1) L and Ẑ(1) L (orange) are non-trivial cycles on the real lattice. A physical error chain e (purple) and its syndrome S(e) (black squares). A recovery chain r (green), with the combined operator on the cycle e ⊕ r being a product of stabilizers Ẑα Ẑβ Ẑγ (recovery success). A recovery chain r (red) whose cycle has non-trivial homology and acts on the code state as Ẑ(1) L (logical failure).</figDesc><graphic coords="2,372.75,296.74,97.03,97.03" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>∈p σz and Xv = ∈v σx , with σz and σx acting respectively on the links contained in the plaque-tte p and the links connected to the vertex v. There are two encoded logical qubits, manipulated by logical operators Ẑ(1,2) L as σz acting on the non-contractible loops on the real lattice and logical X(1,2) L as the non-contractible loops on the dual lattice (Fig 1).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>FIG. 2 .</head><label>2</label><figDesc>FIG.2. The neural decoder architecture. The hidden layer h is fully-connected to the syndrome and error layers S and e with weights U and W respectively.</figDesc><graphic coords="3,411.01,167.84,106.27,106.27" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>where the effective energy E λ (e, S) can be computed exactly. Moreover, given the structure of the network, the conditional probabilities p λ (e | h), p λ (S | h) and p λ (h | e, S) are also known exactly. The training of the machine consists of tuning the parameters λ until the model probability p λ (e, S) becomes close to the target distribution p data (e, S) of the dataset. This translates into solving an optimization problem over the parameters λ by minimizing the distance between the two distribution, defined as the Kullbach-Leibler (KL) divergence, KL ∝ -(e,S)∈D log p λ (e, S). Details about the Boltzmann machine and its training algorithm are reported in the Supplementary Materials.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>1 : 5 : 6 :</head><label>156</label><figDesc>e0: physical error chain 2: S0 = S(e0) Syndrome Extraction 3: RBM = {e, S = S0, h} Network Initialization 4: while S(e) = S0 do Sampling Sample h ∼ p(h | e, S0) Sample e ∼ p(e | h) 7: end while 8: r = e Decoding</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>FIG. 3 .</head><label>3</label><figDesc>FIG. 3. Logical failure probability as a function of elementary error probability for MWPM (lines) and the neural decoder (markers) of size L = 4 (red) and L = 6 (green).</figDesc><graphic coords="4,386.16,74.10,124.28,124.28" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>p 15 FIG. 4 .</head><label>154</label><figDesc>FIG. 4. Histogram of the homology classes returned by our neural decoder for various elementary error probabilities perr. The green bars represent the trivial homology class h0 corresponding to contractible loops on the torus. The other three classes correspond respectively to the logical operations Ẑ(1) L , Ẑ(2) L and Ẑ(1) L Ẑ(2) L .</figDesc></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements. The authors thank J. Carrasquilla, D. Gottesman, M. Hastings, C. Herdmann, B. Kulchytskyy, M. Mariantoni and D. Poulin for enlightening discussions. This research was supported by NSERC, the CRC program, the Ontario Trillium Foundation, the Perimeter Institute for Theoretical Physics, and the National Science Foundation under Grant No. NSF PHY-1125915. Simulations were performed on resources provided by SHARCNET. Research at Perimeter Institute is supported through Industry Canada and by the Province of Ontario through the Ministry of Research &amp; Innovation.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">K</forename><surname>Hornik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Stinchcombe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>White</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">359</biblScope>
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Dep. Comp. Sc., University. of Toronto</title>
		<imprint>
			<biblScope unit="page">2</biblScope>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
	<note type="report_type">Technical Report UTML</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<title level="m">Proc. Advances in Neural Information Processing Systems</title>
		<meeting>Advances in Neural Information essing Systems</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page">1090</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trends in Cognitive Science</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">428</biblScope>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">521</biblScope>
			<biblScope unit="page">436</biblScope>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page">82</biblScope>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">G</forename><surname>Torlai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Melko</surname></persName>
		</author>
		<idno type="DOI">10.1103/PhysRevB.94.165134</idno>
	</analytic>
	<monogr>
		<title level="j">Physical Review B</title>
		<imprint>
			<biblScope unit="volume">94</biblScope>
			<biblScope unit="page">165134</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">J</forename><surname>Carrasquilla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">G</forename><surname>Melko</surname></persName>
		</author>
		<idno type="DOI">10.1038/nphys4035</idno>
	</analytic>
	<monogr>
		<title level="j">Nat Phys</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">431</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.1103/PhysRevB.94.195105</idno>
	</analytic>
	<monogr>
		<title level="j">Physical Review B</title>
		<imprint>
			<biblScope unit="volume">94</biblScope>
			<biblScope unit="page">195105</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">G</forename><surname>Carleo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Troyer</surname></persName>
		</author>
		<idno type="DOI">10.1126/science.aag2302</idno>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">355</biblScope>
			<biblScope unit="page">602</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">P</forename><surname>Broecker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Carrasquilla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">G</forename><surname>Melko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Trebst</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.07848</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">K</forename><surname>Ch'ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Carrasquilla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">G</forename><surname>Melko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Khatami</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.02552</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">D.-L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">D</forename><surname>Sarma</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.09060</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">L</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.1103/PhysRevB.95.035105</idno>
	</analytic>
	<monogr>
		<title level="j">Physical Review B</title>
		<imprint>
			<biblScope unit="volume">95</biblScope>
			<biblScope unit="page">35105</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">Y</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fu</surname></persName>
		</author>
		<idno type="DOI">10.1103/PhysRevB.95.041101</idno>
	</analytic>
	<monogr>
		<title level="j">Physical Review B</title>
		<imprint>
			<biblScope unit="volume">95</biblScope>
			<biblScope unit="page">41101</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Stoudenmire</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Schwab</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.05775</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">G</forename><surname>Torlai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Mazzola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Carrasquilla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Troyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">G</forename><surname>Melko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Carleo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.05334</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">D</forename><surname>Nigg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mueller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">A</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Schindler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hennrich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Monz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Martin-Delgado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Blatt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">345</biblScope>
			<biblScope unit="page">302</biblScope>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<author>
			<persName><forename type="first">H</forename><surname>Bombin</surname></persName>
		</author>
		<title level="m">Quantum Error Correction</title>
		<editor>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Lidar</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><forename type="middle">A</forename><surname>Brun</surname></persName>
		</editor>
		<meeting><address><addrLine>Cambridge</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page">19</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Fowler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mariantoni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Martinis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">N</forename><surname>Cleland</surname></persName>
		</author>
		<idno type="DOI">10.1103/PhysRevA.86.032324</idno>
	</analytic>
	<monogr>
		<title level="j">Phys. Rev. A</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="page">32324</biblScope>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">H</forename><surname>Bombin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Martin-Delgado</surname></persName>
		</author>
		<idno type="DOI">10.1088/1751-8113/42/9/095302/meta</idno>
	</analytic>
	<monogr>
		<title level="j">J. Phys. A:Math. Theor</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page">95302</biblScope>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-642-35289-8_32</idno>
		<title level="m">Neural Networks: Tricks of the Trade</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page">599</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">D</forename><surname>Gottesman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:quant-ph/9705052</idno>
		<imprint>
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Kitaev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals of Physics</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">2</biblScope>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">E</forename><surname>Dennis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kitaev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Landahl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Preskill</surname></persName>
		</author>
		<idno type="DOI">10.1063/1.1499754</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of Mathematical Physics</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="page">4452</biblScope>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">G</forename><surname>Duclos-Cianci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Poulin</surname></persName>
		</author>
		<idno type="DOI">10.1103/PhysRevLett.104.050504</idno>
	</analytic>
	<monogr>
		<title level="j">Phys. Rev. Lett</title>
		<imprint>
			<biblScope unit="volume">104</biblScope>
			<biblScope unit="page">50504</biblScope>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">G</forename><surname>Duclos-Cianci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Poulin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Quant. Inf. Comp</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page">721</biblScope>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Wootton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Loss</surname></persName>
		</author>
		<idno type="DOI">10.1103/PhysRevLett.109.160503</idno>
	</analytic>
	<monogr>
		<title level="j">Phys. Rev. Lett</title>
		<imprint>
			<biblScope unit="volume">109</biblScope>
			<biblScope unit="page">160503</biblScope>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">A</forename><surname>Hutter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Wootton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Loss</surname></persName>
		</author>
		<idno type="DOI">10.1103/PhysRevA.89.022326</idno>
	</analytic>
	<monogr>
		<title level="j">Phys. Rev. A</title>
		<imprint>
			<biblScope unit="volume">89</biblScope>
			<biblScope unit="page">22326</biblScope>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">A</forename><surname>Fowler</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1310.0863</idno>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">S</forename><surname>Bravyi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Suchara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vargo</surname></persName>
		</author>
		<idno type="DOI">10.1103/PhysRevA.90.032326</idno>
	</analytic>
	<monogr>
		<title level="j">Phys. Rev. A</title>
		<imprint>
			<biblScope unit="volume">90</biblScope>
			<biblScope unit="page">32326</biblScope>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">B</forename><surname>Heim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">M</forename><surname>Svore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">B</forename><surname>Hastings</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.06373</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">J</forename><surname>Edmonds</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Canadian Journal of Mathematics</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page">449</biblScope>
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Osindero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Teh</surname></persName>
		</author>
		<idno type="DOI">10.1162/neco.2006.18.7.1527#.VzSfdWamvEY</idno>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page">1527</biblScope>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Murray</surname></persName>
		</author>
		<title level="m">ICML&apos;08 Proceedings of the 25th international conference on machine learning</title>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page">872</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Igel</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-642-33275-3_2</idno>
		<title level="m">Progress in Pattern Recognition, Image Analysis, Computer Vision, and Applications</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">H E</forename><surname>Watson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">D</forename><surname>Barrett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">New Journal of Physics</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page">93045</biblScope>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">V</forename><surname>Kolmogorov</surname></persName>
		</author>
		<idno type="DOI">10.1007/s12532-009-0002-8</idno>
	</analytic>
	<monogr>
		<title level="j">Math. Prog. Comp</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">43</biblScope>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Fowler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Whiteside</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">C L</forename><surname>Hollenberg</surname></persName>
		</author>
		<idno type="DOI">10.1103/PhysRevLett.108.180501</idno>
	</analytic>
	<monogr>
		<title level="j">Phys. Rev. Lett</title>
		<imprint>
			<biblScope unit="volume">108</biblScope>
			<biblScope unit="page">180501</biblScope>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">E</forename><surname>Novais</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">R</forename><surname>Mucciolo</surname></persName>
		</author>
		<idno type="DOI">10.1103/PhysRevLett.110.010502</idno>
	</analytic>
	<monogr>
		<title level="j">Phys. Rev. Lett</title>
		<imprint>
			<biblScope unit="volume">110</biblScope>
			<biblScope unit="page">10502</biblScope>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Harrington</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Preskill</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals of Physics</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">31</biblScope>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">G</forename><surname>Katzgraber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Bombin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Martin-Delgado</surname></persName>
		</author>
		<idno type="DOI">10.1103/PhysRevLett.103.090501</idno>
	</analytic>
	<monogr>
		<title level="j">Phys. Rev. Lett</title>
		<imprint>
			<biblScope unit="volume">103</biblScope>
			<biblScope unit="page">90501</biblScope>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">J</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">H</forename><surname>Nickerson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">E</forename><surname>Browne</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Communications</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
